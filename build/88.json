{
  "id": "88",
  "title": "GPT-2 Text Generation",
  "difficulty": "hard",
  "category": "NLP",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description": "### Implement a Simplified GPT-2-like Text Generation Function\n\nYou are tasked with implementing a simplified GPT-2-like text generation function in Python. This function will incorporate the following components of a minimal GPT-2 architecture:\n\n- **Token Embeddings**: Map input tokens to dense vector representations.\n- **Positional Embeddings**: Add positional information to token embeddings.\n- **Multi-head Attention**: Attend to various parts of the sequence.\n- **Feed-Forward Network**: Process attention outputs through a dense layer.\n- **Layer Normalization**: Stabilize the training process.\n\nThe function must take in the following parameters:\n\n1. **Prompt**: The initial text to guide the generation process.\n2. **Number of Tokens to Generate**: Specify how many tokens to output.\n\nYour function should output the generated text.\n\nAdditionally, utilize the helper function `load_encoder_hparams_and_params` to retrieve:\n\n- A dummy encoder.\n- Model hyperparameters.\n- Model parameters.\n\nBuild your text generation logic around these components. This exercise is designed to help you understand the core concepts behind GPT-2's autoregressive text generation.",
  "learn_section": "# Understanding Transformer Architecture and Text Generation\n\nTransformers have revolutionized the field of Natural Language Processing (NLP) with their efficient and scalable architecture. This guide provides an in-depth look into the core components of transformers and how they facilitate advanced text generation.\n\n## 1. Introduction to Transformers\n\nTransformers are a groundbreaking neural network architecture that has significantly advanced NLP. Introduced in the seminal paper *\"Attention is All You Need\"* by Vaswani et al. (2017), transformers have outperformed traditional models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) in various NLP tasks.\n\n### Key Advantages of Transformers\n\n- **Parallel Processing**:  \n  Unlike RNNs, which process input sequences sequentially, transformers handle entire sequences simultaneously. This parallelism leads to substantial improvements in training speed and efficiency.\n\n- **Scalability**:  \n  Transformers can effectively scale to handle large datasets and complex tasks, making them ideal for applications like language translation, text generation, and summarization.\n\n- **Self-Attention Mechanism**:  \n  The core innovation of transformers is the self-attention mechanism, which allows the model to weigh the importance of different words in a sequence relative to each other. This capability enables the model to capture long-range dependencies and contextual relationships within the text.\n\n### Applications of Transformers\n\n- **Text Generation**: Creating coherent and contextually relevant text based on a given prompt.\n- **Machine Translation**: Translating text from one language to another with high accuracy.\n- **Text Summarization**: Condensing long documents into concise summaries while retaining key information.\n- **Question Answering**: Providing accurate answers to user queries based on contextual understanding.\n\n---\n\n## 2. Core Concepts\n\nTo fully grasp the transformer architecture, it's essential to understand its foundational components. Below are the core concepts that constitute the building blocks of transformers:\n\n### 2.1 GELU Activation Function\n\nThe Gaussian Error Linear Unit (GELU) is an advanced activation function that enhances the performance of deep neural networks.\n\n**Mathematical Expression**:  \n$$\n\\text{GELU}(x) = 0.5 \\cdot x \\cdot (1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)))\n$$\n\n**Purpose**:  \nGELU introduces non-linearity in the network while maintaining smooth gradient flow. Unlike the Rectified Linear Unit (ReLU) or Sigmoid functions, GELU provides a probabilistic approach to activation, allowing for better handling of uncertainty and improving model performance in deep architectures.\n\n**Benefits**:\n- **Smooth Activation**: Reduces the likelihood of \"dead neurons\" that can occur with ReLU.\n- **Improved Gradient Flow**: Facilitates more stable and efficient training by preventing gradient vanishing or exploding.\n\n### 2.2 Softmax for Attention\n\nSoftmax is a fundamental function used to convert raw attention scores into a probability distribution, ensuring that the weights sum to one.\n\n**Mathematical Expression**:  \n$$\n\\text{Softmax}(x_i) = \\frac{\\exp(x_i)}{\\sum_{j=1}^n \\exp(x_j)}\n$$\n\n**Purpose**:  \nIn the context of attention mechanisms, Softmax normalizes the attention scores, enabling the model to focus on relevant parts of the input sequence by assigning higher weights to more important tokens.\n\n**Example**:  \nIf the attention scores for a sentence are `[2, 1, 0.1]`, applying Softmax will convert these to probabilities like `[0.659, 0.242, 0.099]`, indicating the relative importance of each token.\n\n### 2.3 Layer Normalization\n\nLayer normalization stabilizes and accelerates the training process by standardizing the inputs across the features.\n\n**Mathematical Expression**:  \n$$\n\\text{LayerNorm}(x) = g \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + b\n$$\n\nWhere:\n- \\( \\mu \\): Mean of input \\( x \\) along the last axis.\n- \\( \\sigma^2 \\): Variance of \\( x \\).\n- \\( g, b \\): Learnable scaling and bias parameters.\n- \\( \\epsilon \\): A small constant to prevent division by zero.\n\n**Purpose**:  \nBy normalizing the inputs, layer normalization ensures that each layer receives inputs with a consistent distribution, which enhances training stability and convergence speed.\n\n---\n\n### 2.4 Multi-Head Attention\n\nMulti-head attention is an extension of the attention mechanism that allows the model to focus on different representation subspaces simultaneously.\n\n**Components**:\n- **Query (Q), Key (K), Value (V) Matrices**: Each attention head computes its own set of Q, K, and V matrices by projecting the input embeddings into different subspaces.\n- **Scaled Dot-Product Attention**:\n  $$\n  \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n  $$\n\n**Benefits**:\n- **Diversity of Attention**: Allows the model to focus on different parts of the input simultaneously.\n- **Enhanced Representation**: Captures richer features by aggregating multiple attention heads.\n\n---\n\n### 2.5 Feedforward Network (FFN)\n\nThe Feedforward Network is a simple yet powerful component applied to each position independently within the transformer.\n\n**Mathematical Expression**:  \n$$\n\\text{FFN}(x) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(x)))\n$$\n\n**Structure**:\n1. First Linear Layer: Projects the input to a higher-dimensional space.\n2. GELU Activation: Introduces non-linearity to the model.\n3. Second Linear Layer: Projects the data back to the original dimensionality.\n\n**Purpose**:  \nThe FFN enhances the model's capacity to learn intricate patterns.\n\n---\n\n### 2.6 Transformer Block\n\nA transformer block is the fundamental building unit of the transformer architecture, combining multi-head attention and the feedforward network with residual connections and layer normalization.\n\n**Structure**:\n- **Multi-Head Attention Layer**:  \n  $$ x_1 = \\text{LayerNorm}(x + \\text{MHA}(x)) $$\n- **Feedforward Network**:  \n  $$ x_2 = \\text{LayerNorm}(x_1 + \\text{FFN}(x_1)) $$\n\n**Advantages**:\n- **Deep Architecture Support**: Facilitates the construction of deep networks without significant performance degradation.\n- **Modularity**: Each transformer block can be stacked multiple times, allowing for scalable model depth.\n\n---\n\n### 2.7 GPT-2 Text Generation\n\nGPT-2 (Generative Pre-trained Transformer 2) leverages the transformer architecture for generating human-like text. Developed by OpenAI, GPT-2 has demonstrated remarkable capabilities in various NLP tasks.\n\n**Key Components**:\n- **Word and Positional Embeddings**: Captures semantic meaning and token position in a sequence.\n- **Causal Attention**: Ensures left-to-right text generation by masking future tokens.\n- **Stacked Transformer Blocks**: Refines input representations iteratively.\n\n**Text Generation Process**:\n1. Provide a prompt to initiate the process.\n2. Tokenize the input into embeddings.\n3. Process embeddings through transformer blocks.\n4. Generate the next token based on probabilities.\n5. Repeat steps 3-4 to produce coherent text.\n\n---\n\n### Conclusion\n\nTransformers have fundamentally transformed NLP by introducing efficient and scalable architectures capable of handling complex language tasks. Understanding their core components such as GELU activation, Softmax attention, layer normalization, multi-head attention, feedforward networks, and the transformer block provides a foundation for leveraging these models in various applications. GPT-2 exemplifies the transformative power of these architectures while highlighting ethical considerations for their use.",
  "starter_code": "def gen_text(prompt: str, n_tokens_to_generate: int = 40):\n\t####your code goes here####\n\tpass\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n\tclass DummyBPE:\n\t\tdef __init__(self):\n\t\t\tself.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n\t\tdef encode(self, text: str):\n\t\t\ttokens = text.strip().split()\n\t\t\treturn [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n\t\tdef decode(self, token_ids: list):\n\t\t\treversed_dict = {v: k for k, v in self.encoder_dict.items()}\n\t\t\treturn \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n\thparams = {\n\t\t\"n_ctx\": 1024,\n\t\t\"n_head\": 12\n\t}\n\n\tparams = {\n\t\t\"wte\": np.random.rand(3, 10),\n\t\t\"wpe\": np.random.rand(1024, 10),\n\t\t\"blocks\": [],\n\t\t\"ln_f\": {\n\t\t\t\"g\": np.ones(10),\n\t\t\t\"b\": np.zeros(10),\n\t\t}\n\t}\n\n\tencoder = DummyBPE()\n\treturn encoder, hparams, params",
  "solution": "import numpy as np\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\n\ndef linear(x, w, b):\n    return x @ w + b\n\ndef ffn(x, c_fc, c_proj):\n    return linear(gelu(linear(x, **c_fc)), **c_proj)\n\ndef attention(q, k, v, mask):\n    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\ndef mha(x, c_attn, c_proj, n_head):\n    x = linear(x, **c_attn)\n    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), np.split(x, 3, axis=-1)))\n    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n    x = linear(np.hstack(out_heads), **c_proj)\n    return x\n\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n    return x\n\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n    x = wte[inputs] + wpe[range(len(inputs))]\n    for block in blocks:\n        x = transformer_block(x, **block, n_head=n_head)\n    return layer_norm(x, **ln_f) @ wte.T\n\ndef generate(inputs, params, n_head, n_tokens_to_generate):\n    for _ in range(n_tokens_to_generate):\n        logits = gpt2(inputs, **params, n_head=n_head)\n        next_id = np.argmax(logits[-1])\n        inputs.append(int(next_id))\n    return inputs[len(inputs) - n_tokens_to_generate:]\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n    output_text = encoder.decode(output_ids)\n    return output_text",
  "example": {
    "input": "prompt=\"hello\", n_tokens_to_generate=5",
    "output": "world <UNK> <UNK> <UNK> <UNK>",
    "reasoning": "The function encodes the input \"hello\" into tokens using the dummy encoder, then runs a simplified GPT-2 forward pass to generate 5 tokens. Finally, it decodes the generated tokens back into text."
  },
  "test_cases": [
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(gen_text(\"hello\", n_tokens_to_generate=5))",
      "expected_output": "hello hello hello <UNK> <UNK>"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(gen_text(\"hello world\", n_tokens_to_generate=10))",
      "expected_output": "world world world world world world world world world world"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(gen_text(\"world\", n_tokens_to_generate=3))",
      "expected_output": "world world world"
    }
  ]
}