{
  "id": "106",
  "title": "Train Logistic Regression with Gradient Descent",
  "difficulty": "hard",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/turkunov",
      "name": "turkunov"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "Implement a gradient descent-based training algorithm for logistic regression. Your task is to compute model parameters using Binary Cross Entropy loss and return the optimized coefficients along with collected loss values over iterations(round to the 4th decimal).",
  "learn_section": "## Overview\nLogistic regression is a model used for a binary classification poblem.\n\n## Prerequisites for a regular logistic regression\nLogistic regression is based on the concept of \"logits of odds\". **Odds** is measure of how frequent we encounter success. It also allows us to shift our probabilities domain of $[0, 1]$ to $[0,\\infty]$ Consider a probability of scoring a goal $p=0.8$, then our $odds=\\frac{0.8}{0.2}=4$. This means that every $4$ matches we could be expecting a goal followed by a miss. So the higher the odds, the more consistent is our streak of goals. **Logit** is an inverse of the standard logistic function, i.e. sigmoid: $logit(p)=\\sigma^{-1}(p)=ln\\frac{p}{1-p}$. In our case $p$ is a probability, therefore we call $\\frac{p}{1-p}$ the \"odds\". The logit allows us to further expand our domain from $[0,\\infty]$ to $[-\\infty,\\infty]$.\n\nWith this domain expansion we can treat our problem as a linear regression and try to approximate our logit function: $X\\beta=logit(p)$. However what we really want for this approximation is to yield predictions for probabilities:\n$$\nX\\beta=ln\\frac{p}{1-p} \\\\\ne^{-X\\beta}=\\frac{1-p}{p} \\\\ \ne^{-X\\beta}+1 = \\frac{1}{p} \\\\\np = \\frac{1}{e^{-X\\beta}+1}\n$$\n\nWhat we practically just did is taking an inverse of a logit function w.r.t. our approximation and go back to sigmoid. This is also the backbone of the regular logistic regression, which is commonly defined as:\n$$\n\\pi=\\frac{e^{\\alpha+X\\beta}}{1+e^{\\alpha+X\\beta}}=\\frac{1}{1+e^{-(\\alpha+X\\beta)}}.\n$$\n\n## Loss in logistic regression\nThe loss function used for solving the logistic regression for $\\beta$ is derived from MLE (Maximum Likelihood Estimation). This method allows us to search for $\\beta$ that maximize our **likelihood function** $L(\\beta)$. This function tells us how likely it is that $X$ has come from the distribution generated by $\\beta$: $L(\\beta)=L(\\beta|X)=P(X|\\beta)=\\prod_{\\{x\\in X\\}}f^{univar}_X(x;\\beta)$, where $f$ is a PMF and $univar$ means univariate, i.e. applied to a single variable.\n\nIn the case of a regular logistic regression we expect our output to belong to a single Bernoulli-distributed random variable (hence the univariance), since our true label is either $y_i=0$ or $y_i=1$. The Bernoulli's PMF is defined as $P(Y=y)=p^y(1-p)^{(1-y)}$, where $y\\in\\{0, 1\\}$. Also let's denote $\\{x\\in X\\}$ simply as $X$ and refer to a single pair of vectors from the training set as $(x_i, y_i)$. Thus, our likelihood function would look like this:\n$$\n\\prod_X p\\left(x_i\\right)^{y_i} \\times\\left[1-p\\left(x_i\\right)\\right]^{1-y_i}\n$$\n\nThen we convert our function from likelihood to log-likelihood by taking $ln$ (or $log$) of it:\n$$\n\\sum_X y_i \\log \\left[p\\left(x_i\\right)\\right]+\\left(1-y_i\\right) \\log \\left[1-p\\left(x_i\\right)\\right]\n$$\n\nAnd then we replace $p(x_i)$ with the sigmoid from previously defined equality to get a final version of our **loss function**:\n$$\n\\sum_X y_i \\log \\left(\\frac{1}{1+e^{-x_i\\beta}}\\right)+\\left(1-y_i\\right)\\log \\left(1-\\frac{1}{1+e^{-x_i\\beta}}\\right)\n$$\n\n## Optimization objective\nRecall that originally we wanted to search for $\\beta$ that maximize the likelihood function. Since $log$ is a monotonic transformation, our maximization objective does not change and we can confindently say that now we can equally search for $\\beta$ that maximize our log-likelihood. Hence we can finally write our actual objective as:\n\n$$\nargmax_\\beta [\\sum_X y_i \\log\\sigma(x_i\\beta)+\\left(1-y_i\\right)\\log (1-\\sigma(x_i\\beta))] = \\\\\n= argmin_\\beta -[\\sum_X y_i \\log\\sigma(x_i\\beta)+\\left(1-y_i\\right)\\log (1-\\sigma(x_i\\beta))]\n$$\n\nwhere $\\sigma$ is the sigmoid. This function we're trying to minimize is also called **Binary Cross Entropy** loss function (BCE). To find the minimum we would need to take the gradient of this LLF (Log-Likelihood Function), or find a vector of derivatives with respect to every individual $\\beta_j$.\n\n### Step 1\nTo do that we're going to use a chain rule, that describes relational change in variables that our original function is made of. In our case the log-likeligood function depends on sigmoid $\\sigma$, $\\sigma$ depends on $X\\beta$ and $X\\beta$ finally depends on $\\beta_j$, hence:\n\n$$\n\\frac{\\partial LLF}{\\partial\\beta_j}=\\frac{\\partial LLF}{\\partial\\sigma}\\frac{\\partial\\sigma}{\\partial[X\\beta]}\\frac{\\partial[X\\beta]}{\\beta_j}\\\\\n=-\\sum_{i=1}^n\\left(y^{(i)} \\frac{1}{\\sigma\\left(x^{(i)}\\beta\\right)}-(1-y^{(i)} ) \\frac{1}{1-\\sigma\\left(x^{(i)}\\beta\\right)}\\right) \\frac{\\partial\\sigma}{\\partial[x^{(i)}\\beta]}\n$$\n\n### Step 2\nThen we use a derivative of the sigmoid function, that is $\\frac{\\partial\\sigma(x)}{\\partial x}=\\sigma(x)(1-\\sigma(x))$: \n$$\n-\\sum_{i=1}^n\\left(y^{(i)} \\frac{1}{\\sigma\\left(x^{(i)}\\beta\\right)}-(1-y^{(i)} ) \\frac{1}{1-\\sigma\\left(x^{(i)}\\beta\\right)}\\right)\\\\\n     \\sigma\\left(x^{(i)}\\beta\\right)\\left(1-\\sigma\\left(x^{(i)}\\beta\\right)\\right)^{(*)} \\frac{\\partial[x^{(i)}\\beta]}{\\partial\\beta_j} \\\\\n=-\\sum_{i=1}^n\\left(y^{(i)}\\left(1-\\sigma\\left(x^{(i)}\\beta\\right)\\right)-(1-y^{(i)} ) \\sigma\\left(x^{(i)}\\beta\\right)\\right) x_j^{(i)} \\\\\n=-\\sum_{i=1}^n\\left(y^{(i)}-\\sigma\\left(x^{(i)}\\beta\\right)\\right) x_j^{(i)} \\\\\n=\\sum_{i=1}^n\\left(\\sigma\\left(x^{(i)}\\beta\\right)-y^{(i)}\\right) x_j^{(i)}.\n$$\n\nThe result sum can be then rewritten in a more convenient gradient matrix form as:\n$$\nX^T(\\sigma(X\\beta)-Y)\n$$\n\nThen we can finally use gradient descent in order to iteratively update our parameters:\n$$\n\\beta_{t+1}=\\beta_t - \\eta [X^T(\\sigma(X\\beta_t)-Y)]\n$$",
  "starter_code": "import numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n\t\"\"\"\n\tGradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\t\"\"\"\n\t# Your code here\n\tpass",
  "solution": "import numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    y = y.reshape(-1, 1)\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    B = np.zeros((X.shape[1], 1))\n    losses = []\n\n    for _ in range(iterations):\n        y_pred = sigmoid(X @ B)\n        B -= learning_rate * X.T @ (y_pred - y)\n        loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n\n    return B.flatten().round(4).tolist(), losses",
  "example": {
    "input": "train_logreg(np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]), np.array([1, 0, 1, 0]), 0.01, 20)",
    "output": "([0.0037, 0.0246, 0.0202], [2.7726, 2.7373, 2.7024, 2.6678, 2.6335, 2.5995, 2.5659, 2.5327, 2.4997, 2.4671, 2.4348, 2.4029, 2.3712, 2.3399, 2.3089, 2.2783, 2.2480, 2.2180, 2.1882, 2.1588])",
    "reasoning": "The function iteratively updates the logistic regression parameters using gradient descent and collects loss values over iterations."
  },
  "test_cases": [
    {
      "test": "print(train_logreg(np.array([[0.7674, -0.2341, -0.2341, 1.5792], [-1.4123, 0.3142, -1.0128, -0.9080], [-0.4657, 0.5425, -0.4694, -0.4634], [-0.5622, -1.9132, 0.2419, -1.7249], [-1.4247, -0.2257, 1.4656, 0.0675], [1.8522, -0.2916, -0.6006, -0.6017], [0.3756, 0.1109, -0.5443, -1.1509], [0.1968, -1.9596, 0.2088, -1.3281], [1.5230, -0.1382, 0.4967, 0.6476], [-1.2208, -1.0577, -0.0134, 0.8225]]), np.array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]), 0.001, 10))",
      "expected_output": "([-0.0097, 0.0286, 0.015, 0.0135, 0.0316], [6.9315, 6.9075, 6.8837, 6.8601, 6.8367, 6.8134, 6.7904, 6.7675, 6.7448, 6.7223])"
    },
    {
      "test": "print(train_logreg(np.array([[ 0.76743473,  1.57921282, -0.46947439],[-0.23415337,  1.52302986, -0.23413696],[ 0.11092259, -0.54438272, -1.15099358],[-0.60063869,  0.37569802, -0.29169375],[-1.91328024,  0.24196227, -1.72491783],[-1.01283112, -0.56228753,  0.31424733],[-0.1382643 ,  0.49671415,  0.64768854],[-0.46341769,  0.54256004, -0.46572975],[-1.4123037 , -0.90802408,  1.46564877],[ 0.0675282 , -0.2257763 , -1.42474819]]), np.array([1, 1, 0, 0, 0, 0, 1, 1, 0, 0]), 0.1, 10))",
      "expected_output": "([-0.2509, 0.9325, 1.6218, 0.6336], [6.9315, 5.5073, 4.6382, 4.0609, 3.6503, 3.3432, 3.1045, 2.9134, 2.7567, 2.6258])"
    }
  ]
}