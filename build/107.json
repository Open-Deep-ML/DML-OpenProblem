{
  "id": "107",
  "title": "Implement Masked Self-Attention",
  "difficulty": "medium",
  "category": "Deep Learning",
  "video": "https://youtu.be/R_OISH-JWPA?si=DP5hKJHoe2uKdjON",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/nzomi",
      "name": "nzomi"
    }
  ],
  "description": "Implement masked self-attention, a variation of the attention mechanism used in sequence modeling tasks such as text generation. Your task is to compute masked self-attention using query (Q), key (K), value (V) matrices and an attention mask.",
  "learn_section": "## Understanding Masked Attention\n\nMasked attention is a variation of the attention mechanism used primarily in sequence modeling tasks, such as language modeling and text generation. The key idea behind masked attention is to control the flow of information by selectively masking certain elements in the input sequence. This ensures that the model attends only to valid positions when computing attention scores.\n\nMasked attention is particularly useful in autoregressive tasks where future information should not influence the current prediction. By masking out future tokens, the model is constrained to attend only to preceding tokens or the current token, preserving causality during training and inference.\n\n### Concepts\n\nThe attention mechanism enables the model to weigh the importance of different elements in the input sequence based on their relevance to a specific task. Masked attention modifies this process by incorporating a mask, which defines which elements the model is allowed to attend to. This ensures that the attention mechanism respects temporal or structural constraints, such as the directionality of time in sequence data.\n\nThe process of masked attention involves the following steps:\n\n1. **Computing Attention Scores:** The model calculates how much focus each element in the sequence should receive based on its relationship with other elements.\n2. **Applying the Mask:** A mask is applied to restrict attention to specific positions in the sequence. Elements outside the allowed range are effectively ignored.\n3. **Normalizing Scores:** The masked scores are transformed into probabilities using the softmax function.\n4. **Computing the Output:** The final output is computed as a weighted sum of the input values, with weights determined by the normalized attention scores.\n\n### Structure of Masked Attention\n\nThe attention mechanism can be described using Query (Q), Key (K), and Value (V) matrices. In masked attention, these matrices interact with an additional mask to determine the attention distribution.\n\n#### 1. Query, Key, and Value Matrices\n\n- **Query (Q):** Represents the current element for which the model is computing attention.\n- **Key (K):** Encodes information about all elements in the sequence.\n- **Value (V):** Contains the representations that will be aggregated into the output.\n\nAssume that the input sequence has a length of $\\text{seqLen}$ and the model dimension is $d_{\\text{model}}$. The dimensions of the Q, K, and V matrices are:\n\n- Query (Q): $(\\text{seqLen}, d_{\\text{model}})$\n- Key (K): $(\\text{seqLen}, d_{\\text{model}})$\n- Value (V): $(\\text{seqLen}, d_{\\text{model}})$\n\n#### 2. Computing Attention Scores\n\nThe raw attention scores are computed as the scaled dot product between the Query (Q) and Key (K) matrices:\n\n$$\n\\text{score} = \\frac{QK^T}{\\sqrt{d_k}}\n$$\n\nWhere $d_k$ is the dimensionality of the key space. The scaling factor $\\frac{1}{\\sqrt{d_k}}$ ensures that the dot product values do not grow excessively large, preventing instability in the softmax function.\n\n#### 3. Applying the Mask\n\nThe mask is used to control which elements the model is allowed to attend to. Typically, the mask is a binary matrix of dimensions $(\\text{seqLen}, \\text{seqLen})$, where:\n\n- A value of 0 indicates that attention is allowed.\n- A value of $-\\infty$ (or a very large negative value like $-1e9$) indicates that attention is prohibited.\n\nThe raw attention scores are modified by adding the mask:\n\n$$\n\\text{maskedScore} = \\text{score} + \\text{mask}\n$$\n\nThis ensures that prohibited positions receive attention scores that are effectively $-\\infty$, making their softmax probabilities zero.\n\n#### 4. Softmax Calculation\n\nThe softmax function is applied to the masked scores to compute attention weights. To ensure numerical stability, the maximum score in each row is subtracted before applying the softmax function:\n\n$$\n\\text{SoftmaxScore} = \\frac{\\exp(\\text{maskedScore} - \\text{maskedScore}_{\\text{max}})}{\\sum\\exp(\\text{maskedScore} - \\text{maskedScore}_{\\text{max}})}\n$$\n\n#### 5. Computing the Output\n\nThe final attention output is computed as a weighted sum of the Value (V) matrix, with weights determined by the attention scores:\n\n$$\n\\text{output} = \\text{SoftmaxScore} \\cdot V\n$$\n\n### Key Points\n\n- **Masking Future Tokens:** In autoregressive tasks, a triangular mask is used to prevent the model from attending to future positions. For a sequence of length $n$, the mask is an upper triangular matrix with 0s in the lower triangle and $-\\infty$ in the upper triangle.\n\n  Example:\n  $$\n  \\text{mask} = \\begin{bmatrix}\n  0 & -\\infty & -\\infty \\\\\n  0 & 0 & -\\infty \\\\\n  0 & 0 & 0\n  \\end{bmatrix}\n  $$\n\n- **Numerical Stability:** Subtracting the maximum score before applying softmax ensures numerical stability and prevents overflow or underflow errors.\n- **Flexibility:** The mask can be customized to handle other constraints, such as ignoring padding tokens in variable-length sequences.\n\nBy selectively controlling the flow of information through masking, masked attention ensures that the model respects temporal or structural constraints, enabling it to generate coherent and contextually accurate outputs in sequence modeling tasks.",
  "starter_code": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n\t\"\"\"\n\tCompute Query (Q), Key (K), and Value (V) matrices.\n\t\"\"\"\n\treturn np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v)\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n\t\"\"\"\n\tCompute masked self-attention.\n\t\"\"\"\n\t# Your code here\n\tpass",
  "solution": "import numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    d_k = Q.shape[1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    scores = scores + mask  # Apply mask\n    attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = attention_weights / np.sum(attention_weights, axis=1, keepdims=True)\n    return np.matmul(attention_weights, V)",
  "example": {
    "input": "masked_attention(Q, K, V, mask)",
    "output": "[[547. 490. 399. 495. 485. 439. 645. 393.]\n [547. 490. 399. 495. 485. 439. 645. 393.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]]",
    "reasoning": "The function computes self-attention by applying a mask to restrict information flow, ensuring causal dependencies are maintained."
  },
  "test_cases": [
    {
      "test": "np.random.seed(42)\nX = np.arange(48).reshape(6,8)\nX = np.random.permutation(X.flatten()).reshape(6, 8)\nmask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(8,8))\nW_k = np.random.randint(0,5,size=(8,8))\nW_v = np.random.randint(0,6,size=(8,8))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(masked_attention(Q, K, V, mask))",
      "expected_output": "[[547. 490. 399. 495. 485. 439. 645. 393.]\n [547. 490. 399. 495. 485. 439. 645. 393.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]\n [471. 472. 429. 538. 377. 450. 531. 362.]]"
    },
    {
      "test": "np.random.seed(42)\nX = np.arange(16).reshape(4,4)\nX = np.random.permutation(X.flatten()).reshape(4, 4)\nmask = np.triu(np.ones((4, 4))*(-np.inf), k=1)\nW_q = np.random.randint(0,4,size=(4,4))\nW_k = np.random.randint(0,5,size=(4,4))\nW_v = np.random.randint(0,6,size=(4,4))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(masked_attention(Q, K, V, mask))",
      "expected_output": "[[ 52.  63.  48.  71.]\n [103. 109.  46.  99.]\n [103. 109.  46.  99.]\n [103. 109.  46.  99.]]"
    }
  ]
}