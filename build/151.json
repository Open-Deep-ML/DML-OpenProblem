{
  "id": "151",
  "title": "StepLR Learning Rate Scheduler",
  "difficulty": "easy",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/komaksym",
      "name": "komaksym"
    }
  ],
  "description": "## Problem\n\nWrite a Python class StepLRScheduler to implement a learning rate scheduler based on the StepLR strategy. Your class should have an __init__ method implemented to initialize with an initial_lr (float), step_size (int), and gamma (float) parameter. It should also have a **get_lr(self, epoch)** method implemented that returns the current learning rate for a given epoch (int). The learning rate should be decreased by gamma every step_size epochs. Only use standard Python. ",
  "learn_section": "# **Learning Rate Schedulers: StepLR**\n\n## **1. Definition**\n\nA **learning rate scheduler** is a component used in machine learning, especially in neural network training, to adjust the learning rate during the training process. The **learning rate** is a hyperparameter that determines the step size at each iteration while moving towards a minimum of a loss function.\n\n**StepLR (Step Learning Rate)** is a common type of learning rate scheduler that multiplicatively decays the learning rate by a fixed factor at predefined intervals (epochs). It is simple yet effective in stabilizing training and improving model performance.\n\n## **2. Why Use Learning Rate Schedulers?**\n\n* **Faster Convergence:** A higher initial learning rate can help quickly move through the loss landscape.\n* **Improved Performance:** A smaller learning rate towards the end of training allows for finer adjustments and helps in converging to a better local minimum, avoiding oscillations around the minimum.\n* **Stability:** Reducing the learning rate prevents large updates that could lead to divergence or instability.\n\n## **3. StepLR Mechanism**\n\nThe learning rate is reduced by a factor $\\gamma$ (gamma) every $\\text{step\\_size}$ epochs.\n\nThe formula for the learning rate $LR_e$ at a given epoch $e$ is:\n\n$$LR_e = LR_{initial} \\times \\gamma^{\\lfloor e / \\text{step\\_size} \\rfloor}$$\n\nWhere:\n-   $LR_e$: The learning rate at epoch $e$.\n-   $LR_{initial}$: The initial learning rate.\n-   $\\gamma$ (gamma): The multiplicative factor by which the learning rate is reduced (usually between 0 and 1, e.g., 0.1, 0.5).\n-   $\\text{step\\_size}$: The interval (in epochs) after which the learning rate is decayed.\n-   $\\lfloor \\cdot \\rfloor$: The floor function, which rounds down to the nearest integer. This determines how many times the learning rate has been decayed.\n\n**Example:**\nIf $LR_{initial} = 0.1$, $\\text{step\\_size} = 5$, and $\\gamma = 0.5$:\n-   Epoch 0-4: $LR_e = LR_{initial} \\times 0.5^{\\lfloor 0/5 \\rfloor} = 0.1 \\times 0.5^0 = 0.1$\n-   Epoch 5-9: $LR_e = LR_{initial} \\times 0.5^{\\lfloor 5/5 \\rfloor} = 0.1 \\times 0.5^1 = 0.05$\n-   Epoch 10-14: $LR_e = LR_{initial} \\times 0.5^{\\lfloor 10/5 \\rfloor} = 0.1 \\times 0.5^2 = 0.025$\n\n## **4. Applications of Learning Rate Schedulers**\n\nLearning rate schedulers, including StepLR, are widely used in training various machine learning models, especially deep neural networks, across diverse applications such as:\n\n-   **Image Classification:** Training Convolutional Neural Networks (CNNs) for tasks like object recognition.\n-   **Natural Language Processing (NLP):** Training Recurrent Neural Networks (RNNs) and Transformers for tasks like machine translation, text generation, and sentiment analysis.\n-   **Speech Recognition:** Training models for converting spoken language to text.\n-   **Reinforcement Learning:** Optimizing policies in reinforcement learning agents.\n-   **Any optimization problem** where gradient descent or its variants are used.",
  "starter_code": "class StepLRScheduler:\n    def __init__(self, initial_lr, step_size, gamma):\n        # Initialize initial_lr, step_size, and gamma\n        pass\n\n    def get_lr(self, epoch):\n        # Calculate and return the learning rate for the given epoch\n        pass",
  "solution": "class StepLRScheduler:\n    def __init__(self, initial_lr, step_size, gamma):\n        \"\"\"\n        Initializes the StepLR scheduler.\n\n        Args:\n            initial_lr (float): The initial learning rate.\n            step_size (int): The period of learning rate decay.\n                             Learning rate is decayed every `step_size` epochs.\n            gamma (float): The multiplicative factor of learning rate decay.\n                           (e.g., 0.1 for reducing LR by 10x).\n        \"\"\"\n        self.initial_lr = initial_lr\n        self.step_size = step_size\n        self.gamma = gamma\n\n    def get_lr(self, epoch):\n        \"\"\"\n        Calculates and returns the current learning rate for a given epoch.\n\n        Args:\n            epoch (int): The current epoch number (0-indexed).\n\n        Returns:\n            float: The calculated learning rate for the current epoch.\n        \"\"\"\n        # Calculate the number of decays that have occurred.\n        # Integer division (//) in Python automatically performs the floor operation.\n        num_decays = epoch // self.step_size\n        \n        # Apply the decay factor 'gamma' raised to the power of 'num_decays'\n        # to the initial learning rate.\n        current_lr = self.initial_lr * (self.gamma ** num_decays)\n        \n        return current_lr",
  "example": {
    "input": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))\nprint(scheduler.get_lr(epoch=4))\nprint(scheduler.get_lr(epoch=5))\nprint(scheduler.get_lr(epoch=9))\nprint(scheduler.get_lr(epoch=10))",
    "output": "0.1\n0.1\n0.05\n0.05\n0.025",
    "reasoning": "The initial learning rate is 0.1. It stays 0.1 for epochs 0-4. At epoch 5, it decays by 0.5 to 0.05. It stays 0.05 for epochs 5-9. At epoch 10, it decays again to 0.025."
  },
  "test_cases": [
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=0))",
      "expected_output": "0.1"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=4))",
      "expected_output": "0.1"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=5))",
      "expected_output": "0.05"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=9))",
      "expected_output": "0.05"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.1, step_size=5, gamma=0.5)\nprint(scheduler.get_lr(epoch=10))",
      "expected_output": "0.025"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.01, step_size=10, gamma=0.1)\nprint(scheduler.get_lr(epoch=0))\nprint(scheduler.get_lr(epoch=9))\nprint(scheduler.get_lr(epoch=10))\nprint(scheduler.get_lr(epoch=19))\nprint(scheduler.get_lr(epoch=20))",
      "expected_output": "0.01\n0.01\n0.001\n0.001\n0.0001"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=1.0, step_size=1, gamma=0.9)\nprint(scheduler.get_lr(epoch=0))\nprint(scheduler.get_lr(epoch=1))\nprint(scheduler.get_lr(epoch=2))\nprint(scheduler.get_lr(epoch=3))",
      "expected_output": "1.0\n0.9\n0.81\n0.7290000000000001"
    },
    {
      "test": "scheduler = StepLRScheduler(initial_lr=0.0001, step_size=100, gamma=0.8)\nprint(scheduler.get_lr(epoch=99))\nprint(scheduler.get_lr(epoch=100))\nprint(scheduler.get_lr(epoch=199))\nprint(scheduler.get_lr(epoch=200))",
      "expected_output": "0.0001\n8e-05\n8e-05\n6.4e-05"
    }
  ]
}