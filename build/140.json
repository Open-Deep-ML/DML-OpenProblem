{
  "id": "140",
  "title": "Bernoulli Naive Bayes Classifier",
  "difficulty": "medium",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Coder1010ayush",
      "name": "Coder1010ayush"
    }
  ],
  "description": "Write a Python class to implement the Bernoulli Naive Bayes classifier for binary (0/1) feature data. Your class should have two methods: `forward(self, X, y)` to train on the input data (X: 2D NumPy array of binary features, y: 1D NumPy array of class labels) and `predict(self, X)` to output predicted labels for a 2D test matrix X. Use Laplace smoothing (parameter: smoothing=1.0). Return predictions as a NumPy array. Only use NumPy. Predictions must be binary (0 or 1) and you must handle cases where the training data contains only one class. All log/likelihood calculations should use log probabilities for numerical stability.",
  "learn_section": "# **Naive Bayes Classifier**\n\n## **1. Definition**\n\nNaive Bayes is a **probabilistic machine learning algorithm** used for **classification tasks**. It is based on **Bayes' Theorem**, which describes the probability of an event based on prior knowledge of related events.\n\nThe algorithm assumes that:\n- **Features are conditionally independent** given the class label (the \"naive\" assumption).\n- It calculates the posterior probability for each class and assigns the class with the **highest posterior** to the sample.\n\n---\n\n## **2. Bayes' Theorem**\n\nBayes' Theorem is given by:\n\n$$\nP(C | X) = \\frac{P(X | C) \\times P(C)}{P(X)}\n$$\n\nWhere:\n- $P(C | X)$ **Posterior** probability: the probability of class $C $ given the feature vector $X$\n- $P(X | C)$ → **Likelihood**: the probability of the data $X$ given the class\n- $P(C)$ → **Prior** probability: the initial probability of class $C$ before observing any data\n- $ P(X)$ → **Evidence**: the total probability of the data across all classes (acts as a normalizing constant)\n\nSince $P(X)$ is the same for all classes during comparison, it can be ignored, simplifying the formula to:\n\n$$\nP(C | X) \\propto P(X | C) \\times P(C)\n$$\n---\n\n### 3 **Bernoulli Naive Bayes**\n- Used for **binary data** (features take only 0 or 1 values).\n- The likelihood is given by:\n\n$$\nP(X | C) = \\prod_{i=1}^{n} P(x_i | C)^{x_i} \\cdot (1 - P(x_i | C))^{1 - x_i}\n$$\n\n---\n\n## **4. Applications of Naive Bayes**\n\n- **Text Classification:** Spam detection, sentiment analysis, and news categorization.\n- **Document Categorization:** Sorting documents by topic.\n- **Fraud Detection:** Identifying fraudulent transactions or behaviors.\n- **Recommender Systems:** Classifying users into preference groups.\n\n--- ",
  "starter_code": "import numpy as np\n\nclass NaiveBayes():\n    def __init__(self, smoothing=1.0):\n        # Initialize smoothing\n        pass\n\n    def forward(self, X, y):\n        # Fit model to binary features X and labels y\n        pass\n\n    def predict(self, X):\n        # Predict class labels for test set X\n        pass",
  "solution": "import numpy as np\n\nclass NaiveBayes():\n    def __init__(self, smoothing=1.0):\n        self.smoothing = smoothing\n        self.classes = None\n        self.priors = None\n        self.likelihoods = None\n\n    def forward(self, X, y):\n        self.classes, class_counts = np.unique(y, return_counts=True)\n        self.priors = {cls: np.log(class_counts[i] / len(y)) for i, cls in enumerate(self.classes)}\n        self.likelihoods = {}\n        for cls in self.classes:\n            X_cls = X[y == cls]\n            prob = (np.sum(X_cls, axis=0) + self.smoothing) / (X_cls.shape[0] + 2 * self.smoothing)\n            self.likelihoods[cls] = (np.log(prob), np.log(1 - prob))\n\n    def _compute_posterior(self, sample):\n        posteriors = {}\n        for cls in self.classes:\n            posterior = self.priors[cls]\n            prob_1, prob_0 = self.likelihoods[cls]\n            likelihood = np.sum(sample * prob_1 + (1 - sample) * prob_0)\n            posterior += likelihood\n            posteriors[cls] = posterior\n        return max(posteriors, key=posteriors.get)\n\n    def predict(self, X):\n        return np.array([self._compute_posterior(sample) for sample in X])",
  "example": {
    "input": "X = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]]); y = np.array([1, 1, 0, 0, 1])\nmodel = NaiveBayes(smoothing=1.0)\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))",
    "output": "[1]",
    "reasoning": "The model learns class priors and feature probabilities with Laplace smoothing. For [1, 0, 1], the posterior for class 1 is higher, so the model predicts 1."
  },
  "test_cases": [
    {
      "test": "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1], [0, 1, 0], [1, 1, 1]])\ny = np.array([1, 1, 0, 0, 1])\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 0, 1]])))",
      "expected_output": "[1]"
    },
    {
      "test": "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[0], [1], [0], [1]])\ny = np.array([0, 1, 0, 1])\nmodel.forward(X, y)\nprint(model.predict(np.array([[0], [1]])))",
      "expected_output": "[0 1]"
    },
    {
      "test": "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.array([[0, 0], [1, 0], [0, 1]])\ny = np.array([0, 1, 0])\nmodel.forward(X, y)\nprint(model.predict(np.array([[1, 1]])))",
      "expected_output": "[0]"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nmodel = NaiveBayes(smoothing=1.0)\nX = np.random.randint(0, 2, (100, 5))\ny = np.random.choice([0, 1], size=100)\nmodel.forward(X, y)\nX_test = np.random.randint(0, 2, (10, 5))\npred = model.predict(X_test)\nprint(pred.shape)",
      "expected_output": "(10,)"
    },
    {
      "test": "import numpy as np\nmodel = NaiveBayes(smoothing=1.0)\nX = np.random.randint(0, 2, (10, 3))\ny = np.zeros(10)\nmodel.forward(X, y)\nX_test = np.random.randint(0, 2, (3, 3))\nprint(model.predict(X_test))",
      "expected_output": "[0, 0, 0]"
    }
  ]
}