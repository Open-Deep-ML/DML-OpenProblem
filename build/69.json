{
  "id": "69",
  "title": "Calculate R-squared for Regression Analysis",
  "difficulty": "easy",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/rittik9",
      "name": "rittik9"
    }
  ],
  "description": "\n## Task: Compute the R-squared Value in Regression Analysis\n\n- R-squared, also known as the coefficient of determination, is a measure that indicates how well the independent variables explain the variability of the dependent variable in a regression model. \n\n- **Your Task**: \n    To implement the function `r_squared(y_true, y_pred)` that calculates the R-squared value, given arrays of true values `y_true` and predicted values `y_pred`.",
  "learn_section": "\n# Understanding R-squared (R²) in Regression Analysis\n\nR-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It provides insight into how well the model fits the data.\n\n### Mathematical Definition\n\nThe R-squared value is calculated using the following formula:  \n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n$$\nWhere:  \n\n1) $ \\text{SSR} $ (Sum of Squared Residuals): The sum of the squares of the differences between the actual values and the predicted values.  \n2) $ \\text{SST} $ (Total Sum of Squares): The sum of the squares of the differences between the actual values and the mean of the actual values.\n\n### Equations for SSR and SST\n\nTo calculate SSR and SST, we use the following formulas:  \n\n1) SSR:  \n$$\n\\text{SSR} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$  \n\n2) SST:  \n$$\n\\text{SST} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$  \n\nWhere:  \n\n1) $ y_i $: Actual value  \n2) $ \\hat{y}_i $: Predicted value  \n3) $ \\bar{y} $: Mean of the actual values  \n\n### Significance of R-squared\n\nR-squared is a key metric for evaluating how well a regression model performs. A higher R-squared value indicates a better fit for the model, meaning it can explain more variability in the data. However, it's important to note:  \n\n- A high R-squared does not always imply that the model is good; it can sometimes be misleading if overfitting occurs.  \n- It should be used in conjunction with other metrics for comprehensive model evaluation.\n\n### Implementing R-squared Calculation\n\nIn this problem, you will implement a function to calculate R-squared given arrays of true and predicted values from a regression task. The results should be rounded to three decimal places.  \n\nIn the solution, the implemented $ r\\_squared() $ function calculates R-squared by first determining SSR and SST, then applying them to compute $ R^2 $. It handles edge cases such as perfect predictions and situations where all true values are identical.\n\n### Reference\n\nYou can refer to this resource for more information:  \n[Coefficient of Determination](https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html)",
  "starter_code": "\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n\t# Write your code here\n\tpass",
  "solution": "\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (R²) coefficient of determination.\n    \n    Args:\n        y_true (numpy.ndarray): Array of true values\n        y_pred (numpy.ndarray): Array of predicted values\n    \n    Returns:\n        float: R-squared value rounded to 3 decimal places\n    \"\"\"\n    if np.array_equal(y_true, y_pred):\n        return 1.0\n\n    # Calculate mean of true values\n    y_mean = np.mean(y_true)\n\n    # Calculate Sum of Squared Residuals (SSR)\n    ssr = np.sum((y_true - y_pred) ** 2)\n\n    # Calculate Total Sum of Squares (SST)\n    sst = np.sum((y_true - y_mean) ** 2)\n\n    try:\n        # Calculate R-squared\n        r2 = 1 - (ssr / sst)\n        if np.isinf(r2):\n            return 0.0\n        return round(r2, 3)\n    except ZeroDivisionError:\n        return 0.0",
  "example": {
    "input": "import numpy as np\n\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])\nprint(r_squared(y_true, y_pred))",
    "output": "0.989",
    "reasoning": "The R-squared value is calculated to be 0.989, indicating that the regression model explains 98.9% of the variance in the dependent variable."
  },
  "test_cases": [
    {
      "test": "\nimport numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 5])\nprint(r_squared(y_true, y_pred))\n",
      "expected_output": "1.0"
    },
    {
      "test": "\nimport numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])\nprint(r_squared(y_true, y_pred))\n",
      "expected_output": "0.989"
    },
    {
      "test": "\nimport numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([2, 1, 4, 3, 5])\nprint(r_squared(y_true, y_pred))\n",
      "expected_output": "0.6"
    },
    {
      "test": "\nimport numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([3, 3, 3, 3, 3])\nprint(r_squared(y_true, y_pred))\n",
      "expected_output": "0.0"
    },
    {
      "test": "\nimport numpy as np\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([5, 4, 3, 2, 1])\nprint(r_squared(y_true, y_pred))\n",
      "expected_output": "-3.0"
    }
  ]
}