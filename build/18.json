{
  "id": "18",
  "title": "Implement K-Fold Cross-Validation",
  "difficulty": "medium",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/peppermin-t",
      "name": "Yinjia Chen"
    },
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "tinygrad_difficulty": "medium",
  "pytorch_difficulty": "medium",
  "description": "Implement a function to generate train and test splits for K-Fold Cross-Validation. Your task is to divide the dataset into k folds and return a list of train-test indices for each fold.",
  "learn_section": "## Understanding K-Fold Cross-Validation\n\nK-Fold Cross-Validation is a resampling technique used to evaluate machine learning models by partitioning the dataset into multiple folds.\n\n### How it Works\n1. The dataset is split into **k** equal (or almost equal) parts called folds.\n2. Each fold is used **once** as a test set, while the remaining **k-1** folds form the training set.\n3. The process is repeated **k times**, ensuring each fold serves as a test set exactly once.\n\n### Why Use K-Fold Cross-Validation?\n- It provides a more **robust** estimate of model performance than a single train-test split.\n- Reduces bias introduced by a single training/testing split.\n- Allows evaluation across multiple data distributions.\n\n### Implementation Steps\n1. Shuffle the data if required.\n2. Split the dataset into **k** equal (or nearly equal) folds.\n3. Iterate over each fold, using it as the test set while using the remaining data as the training set.\n4. Return train-test indices for each iteration.\n\nBy implementing this function, you will learn how to **split a dataset for cross-validation**, a crucial step in model evaluation.",
  "starter_code": "import numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True):\n    \"\"\"\n    Implement k-fold cross-validation by returning train-test indices.\n    \"\"\"\n    # Your code here\n    pass",
  "solution": "import numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True):\n    \"\"\"\n    Return train and test indices for k-fold cross-validation.\n    \"\"\"\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        if random_seed is not None:\n        np.random.shuffle(indices)\n    \n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        folds.append(indices[current:current + fold_size])\n        current += fold_size\n\n    result = []\n    for i in range(k):\n        test_idx = folds[i]\n        train_idx = np.concatenate(folds[:i] + folds[i+1:])\n        result.append((train_idx.tolist(), test_idx.tolist()))\n    \n    return result",
  "example": {
    "input": "k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False)",
    "output": "[([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]",
    "reasoning": "The function splits the dataset into 5 folds without shuffling and returns train-test splits for each iteration."
  },
  "test_cases": [
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False))",
      "expected_output": "[([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True))",
      "expected_output": "[([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False))",
      "expected_output": "[([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]"
    },
    {
      "test": "import numpy as np\nnp.random.seed(42)\nprint(k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False))",
      "expected_output": "[([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]"
    }
  ],
  "tinygrad_starter_code": "def k_fold_cross_validation_tg(X, y, k=5, shuffle=True) -> list[tuple[list[int], list[int]]]:\n    \"\"\"\n    Return train/test index splits for k-fold cross-validation using pure Python or tinygrad.\n    X: list or Tensor of shape (n_samples, ...)\n    y: list or Tensor of shape (n_samples, ...)\n    k: number of folds\n    shuffle: whether to shuffle indices before splitting\n    Returns list of (train_idx, test_idx) pairs, each as Python lists of ints.\n    \"\"\"\n    # Your implementation here\n    pass",
  "tinygrad_solution": "import numpy as np\n\ndef k_fold_cross_validation_tg(X, y, k=5, shuffle=True) -> list[tuple[list[int], list[int]]]:\n    \"\"\"\n    Return train/test index splits for k-fold cross-validation using NumPy backend.\n    X: list or NumPy array or Tensor of shape (n_samples, ...)\n    y: list or NumPy array or Tensor of shape (n_samples, ...)\n    k: number of folds\n    shuffle: whether to shuffle indices before splitting\n    Returns list of (train_idx, test_idx) pairs, each as Python lists of ints.\n    \"\"\"\n    X_np = np.array(X)\n    n_samples = X_np.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    base = n_samples // k\n    extras = n_samples % k\n    fold_sizes = [base + (1 if i < extras else 0) for i in range(k)]\n    folds = []\n    start = 0\n    for fs in fold_sizes:\n        folds.append(indices[start:start+fs].tolist())\n        start += fs\n    result = []\n    for i in range(k):\n        test_idx = folds[i]\n        train_idx = [idx for j, f in enumerate(folds) if j != i for idx in f]\n        result.append((train_idx, test_idx))\n    return result",
  "tinygrad_test_cases": [
    {
      "test": "res = k_fold_cross_validation_tg(list(range(6)), list(range(6)), k=3, shuffle=False)\nprint(res)",
      "expected_output": "[([2, 3, 4, 5], [0, 1]), ([0, 1, 4, 5], [2, 3]), ([0, 1, 2, 3], [4, 5])]"
    },
    {
      "test": "res = k_fold_cross_validation_tg(list(range(8)), list(range(8)), k=4, shuffle=False)\nprint(res)",
      "expected_output": "[([2, 3, 4, 5, 6, 7], [0, 1]), ([0, 1, 4, 5, 6, 7], [2, 3]), ([0, 1, 2, 3, 6, 7], [4, 5]), ([0, 1, 2, 3, 4, 5], [6, 7])]"
    }
  ],
  "pytorch_starter_code": "import torch\n\ndef k_fold_cross_validation(X, y, k=5, shuffle=True) -> list[tuple[list[int], list[int]]]:\n    \"\"\"\n    Return train/test index splits for k-fold cross-validation using PyTorch.\n    X: Tensor or convertible of shape (n_samples, ...)\n    y: Tensor or convertible of shape (n_samples, ...)\n    k: number of folds\n    shuffle: whether to shuffle indices before splitting\n    Returns list of (train_idx, test_idx) pairs, each as Python lists of ints.\n    \"\"\"\n    X_t = torch.as_tensor(X)\n    n_samples = X_t.size(0)\n    indices = torch.arange(n_samples)\n    if shuffle:\n        indices = indices[torch.randperm(n_samples)]\n    # compute fold sizes\n    base = n_samples // k\n    extras = n_samples % k\n    fold_sizes = [base + (1 if i < extras else 0) for i in range(k)]\n    # split into folds\n    folds = []\n    start = 0\n    for fs in fold_sizes:\n        folds.append(indices[start:start+fs].tolist())\n        start += fs\n    # build train/test pairs\n    result = []\n    for i in range(k):\n        test_idx = folds[i]\n        train_idx = [idx for j, f in enumerate(folds) if j != i for idx in f]\n        result.append((train_idx, test_idx))\n    return result",
  "pytorch_solution": "import torch\n\ndef k_fold_cross_validation(X, y, k=5, shuffle=True) -> list[tuple[list[int], list[int]]]:\n    \"\"\"\n    Return train/test index splits for k-fold cross-validation using PyTorch.\n    X: Tensor or convertible of shape (n_samples, ...)\n    y: Tensor or convertible of shape (n_samples, ...)\n    k: number of folds\n    shuffle: whether to shuffle indices before splitting\n    Returns list of (train_idx, test_idx) pairs, each as Python lists of ints.\n    \"\"\"\n    X_t = torch.as_tensor(X)\n    n_samples = X_t.size(0)\n    indices = torch.arange(n_samples)\n    if shuffle:\n        indices = indices[torch.randperm(n_samples)]\n    base = n_samples // k\n    extras = n_samples % k\n    fold_sizes = [base + (1 if i < extras else 0) for i in range(k)]\n    folds = []\n    start = 0\n    for fs in fold_sizes:\n        folds.append(indices[start:start+fs].tolist())\n        start += fs\n    result = []\n    for i in range(k):\n        test_idx = folds[i]\n        train_idx = []\n        for j, f in enumerate(folds):\n            if j != i:\n                train_idx.extend(f)\n        result.append((train_idx, test_idx))\n    return result",
  "pytorch_test_cases": [
    {
      "test": "res = k_fold_cross_validation(list(range(6)), list(range(6)), k=3, shuffle=False)\nprint(res)",
      "expected_output": "[(2, 3, 4, 5), [0, 1]], [0, 1, 4, 5, [2, 3]], [0, 1, 2, 3, [4, 5]]"
    },
    {
      "test": "res = k_fold_cross_validation(list(range(8)), list(range(8)), k=4, shuffle=False)\nprint(res)",
      "expected_output": "[([2,3,4,5,6,7], [0,1]), ([0,1,4,5,6,7], [2,3]), ([0,1,2,3,6,7], [4,5]), ([0,1,2,3,4,5], [6,7])]"
    }
  ]
}