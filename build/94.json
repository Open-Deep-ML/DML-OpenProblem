{
  "id": "94",
  "title": "Implement Multi-Head Attention",
  "difficulty": "hard",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/nzomi",
      "name": "nzomi"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "Implement the multi-head attention mechanism, a critical component of transformer models. Given Query (Q), Key (K), and Value (V) matrices, compute the attention outputs for multiple heads and concatenate the results.",
  "learn_section": "## Understanding Multi-Head Attention\n\nMulti-head attention is a fundamental mechanism in transformer models, allowing the model to focus on different parts of the input sequence simultaneously. This enables the model to capture a wider variety of relationships and dependencies, which is crucial for handling complex data, such as natural language. By using multiple attention heads, the model learns to attend to various aspects of the input at different levels of abstraction, enhancing its ability to capture complex relationships.\n\n### Concepts\n\nThe attention mechanism allows the model to weigh the importance of different input elements based on their relevance to a specific task. In tasks like machine translation, for example, attention helps the model focus on relevant words in a sentence to understand the overall meaning. Multi-head attention extends this concept by using multiple attention heads, each learning different representations of the input data, which improves the model's ability to capture richer relationships and dependencies.\n\nThe process of multi-head attention involves several key steps:\n\n1. **Computing Attention Scores:** This involves calculating how much focus each element in the input should receive based on its relationship with other elements.\n2. **Applying Softmax:** The attention scores are transformed into probabilities using the softmax function, which normalizes the scores so that they sum to one.\n3. **Aggregating Results:** The final output is computed by taking a weighted sum of the input values, where the weights are determined by the attention scores.\n\n### Structure of Multi-Head Attention\n\nThe attention mechanism can be described with Query (Q), Key (K), and Value (V) matrices. The process of multi-head attention works by repeating the standard attention mechanism multiple times in parallel, with different sets of learned weight matrices for each attention head.\n\n#### 1. Splitting Q, K, and V\n\nAssume that the input Query (Q), Key (K), and Value (V) matrices have dimensions $(\\text{seqLen}, d_{model})$, where $d_{\\text{model}}$ is the model dimension. In multi-head attention, these matrices are divided into n smaller matrices, each corresponding to a different attention head. Each smaller matrix has dimensions $(\\text{seqLen}, d_k)$, where $d_k = \\frac{d_{\\text{model}}}{n}$ is the dimensionality of each head.\n\nFor each attention $\\text{head}_i$, we get its subset of Query $\\text{Q}_i$, Key $\\text{K}_i$, and Value $\\text{V}_i$. These subsets are computed independently for each head.\n\n#### 2. Computing Attention for Each Head\n\nEach head independently computes its attention output. The calculation is similar to the single-head attention mechanism:\n\n$$\n\\text{score}_i = \\frac{Q_i K_i^T}{\\sqrt{d_k}}\n$$\n\nWhere $$d_k$$ is the dimensionality of the key space for each head. The scaling factor $$\\frac{1}{\\sqrt{d_k}}$$ ensures the dot product doesn't grow too large, preventing instability in the softmax function.\n\nThe softmax function is applied to the scores to normalize them, transforming them into attention weights for each head:\n\n$$\n\\text{SoftmaxScore}_i = \\text{softmax}(\\text{score}_i)\n$$\n\n#### 3. Softmax Calculation and Numerical Stability\n\nWhen computing the softmax function, especially in the context of attention mechanisms, there's a risk of numerical overflow or underflow, which can occur when the attention scores become very large or very small. This issue arises because the exponential function $$\\exp$$ grows very quickly, and when dealing with large numbers, it can result in values that are too large for the computer to handle, leading to overflow errors.\n\nTo prevent this, we apply a common technique: subtracting the maximum score from each attention score before applying the exponential function. This helps to ensure that the largest value in the attention scores becomes zero, reducing the likelihood of overflow. Here's how it's done:\n\n$$\n\\text{SoftmaxScore} = \\frac{\\exp(\\text{score} - \\text{score}_{\\text{max}})}{\\sum\\exp(\\text{score} - \\text{score}_{\\text{max}})}\n$$\n\nWhere $$\\text{score}_{i,\\text{max}}$$ is the maximum value of the attention scores for the \\(i\\)-th head. Subtracting the maximum score from each individual score ensures that the largest value becomes 0, which prevents the exponentials from becoming too large.\n\nThis subtraction does not affect the final result of the softmax calculation because the softmax is a relative function it's the ratios of the exponentials that matter. Therefore, this adjustment ensures numerical stability while maintaining the correctness of the computation.\n\nTo summarize, when computing softmax in multi-head attention:\n\n- Subtract the maximum score from each attention score before applying the exponential function.\n- This technique prevents overflow by ensuring that the largest value becomes 0, which keeps the exponential values within a manageable range.\n- The relative relationships between the scores remain unchanged, so the softmax output remains correct.\n\nBy applying this numerical stability trick, the softmax function becomes more robust and prevents computational issues that could arise during training or inference, especially when dealing with large models or sequences.\n\nFinally, the attention output for each $$\\text{head}_i$$ is computed as:\n\n$$\n\\text{head}_i = \\text{SoftmaxScore}_i \\cdot V_i\n$$\n\n#### 4. Concatenation and Linear Transformation\n\nAfter computing the attention output for each head, the outputs are concatenated along the feature dimension. This results in a matrix of dimensions $$(\\text{seqLen}, d_{\\text{model}})$$, where the concatenated attention outputs are passed through a final linear transformation to obtain the final multi-head attention output.\n\n$$\n\\text{MultiHeadOutput} = \\text{concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_n)\n$$\n\nThe concatenated result is then linearly transformed using a weight matrix $W_{\\text{o}}$ to obtain the final output. However, in our case, obtaining the multi-head attention output without this final transformation is sufficient:\n\n$$\n\\text{MultiHeadOutput} = W_o \\cdot \\text{MultiHeadOutput}\n$$\n\n### Key Points\n\n- Each attention head processes the input independently using its own set of learned weights. This allows each head to focus on different relationships in the data.\n- Each head calculates its attention scores based on its corresponding Query, Key, and Value matrices, producing different attention outputs.\n- The outputs of all attention heads are concatenated to form a unified representation. This concatenated result is then linearly transformed to generate the final output.\n\nMulti-head attention allows the model to attend to different aspects of the input sequence in parallel, making it more capable of learning complex and diverse relationships. This parallelization of attention heads enhances the model's ability to understand the data from multiple angles simultaneously, contributing to improved performance in tasks like machine translation, text generation, and more.",
  "starter_code": "import numpy as np\n\ndef compute_qkv(X, W_q, W_k, W_v):\n\tpass\n\ndef self_attention(Q, K, V):\n\tpass\n\ndef multi_head_attention(Q, K, V, n_heads):\n\tpass",
  "solution": "import numpy as np\nfrom typing import Tuple, List\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n    \n    Args:\n    X: numpy array of shape (seq_len, d_model), input sequence\n    W_q, W_k, W_v: numpy arrays of shape (d_model, d_model), weight matrices for Q, K, and V\n    \n    Returns:\n    Q, K, V: numpy arrays of shape (seq_len, d_model)\n    \"\"\"\n    Q = np.dot(X, W_q)  # Compute the Query matrix Q\n    K = np.dot(X, W_k)  # Compute the Key matrix K\n    V = np.dot(X, W_v)  # Compute the Value matrix V\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute self-attention for a single head.\n    \n    Args:\n    Q: numpy array of shape (seq_len, d_k), Query matrix\n    K: numpy array of shape (seq_len, d_k), Key matrix\n    V: numpy array of shape (seq_len, d_k), Value matrix\n    \n    Returns:\n    attention_output: numpy array of shape (seq_len, d_k), output of the self-attention mechanism\n    \"\"\"\n    d_k = Q.shape[1]  # Get the dimension of the keys\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)  # Compute scaled dot-product attention scores\n    score_max = np.max(scores, axis=1, keepdims=True)  # Find the maximum score for numerical stability\n    attention_weights = np.exp(scores - score_max) / np.sum(np.exp(scores - score_max), axis=1, keepdims=True)  # Compute softmax to get attention weights\n    attention_output = np.matmul(attention_weights, V)  # Compute the final attention output\n    return attention_output\n\ndef multi_head_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Compute multi-head attention.\n    \n    Args:\n    Q, K, V: numpy arrays of shape (seq_len, d_model), Query, Key, and Value matrices\n    n_heads: int, number of attention heads\n    \n    Returns:\n    attention_output: numpy array of shape (seq_len, d_model), final attention output\n    \"\"\"\n    d_model = Q.shape[1]  # Get the model dimension\n    assert d_model % n_heads == 0  # Ensure d_model is divisible by n_heads\n    d_k = d_model // n_heads  # Dimension for each head\n\n    # Reshape Q, K, V to separate heads\n    Q_reshaped = Q.reshape(Q.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n    K_reshaped = K.reshape(K.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n    V_reshaped = V.reshape(V.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n\n    # Compute attention scores for each head\n    attentions = []  # Store attention outputs for each head\n\n    for i in range(n_heads):\n        attn = self_attention(Q_reshaped[i], K_reshaped[i], V_reshaped[i])  # Compute attention for the i-th head\n        attentions.append(attn)  # Collect attention output\n\n    # Concatenate all head outputs\n    attention_output = np.concatenate(attentions, axis=-1)  # Concatenate along the last axis (columns)\n    return attention_output  # Return the final attention output",
  "example": {
    "input": "Q = np.array([[1, 0], [0, 1]]), K = np.array([[1, 0], [0, 1]]), V = np.array([[1, 0], [0, 1]]), n_heads = 2",
    "output": "[[1., 0.], [0., 1.]]",
    "reasoning": "Multi-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output."
  },
  "test_cases": [
    {
      "test": "m, n = 4, 4\nn_heads = 2\nnp.random.seed(42)\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\nW_q = np.random.randint(0,4,size=(n,n))\nW_k = np.random.randint(0,5,size=(n,n))\nW_v = np.random.randint(0,6,size=(n,n))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(multi_head_attention(Q, K, V, n_heads))",
      "expected_output": "np.array([[103, 109, 46, 99],\n                                [103, 109, 46, 99],\n                                [103, 109, 46, 99],\n                                [103, 109, 46, 99]])"
    },
    {
      "test": "m, n = 6, 8\nn_heads = 4\nnp.random.seed(42)\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\nW_q = np.random.randint(0,4,size=(n,n))\nW_k = np.random.randint(0,5,size=(n,n))\nW_v = np.random.randint(0,6,size=(n,n))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(multi_head_attention(Q, K, V, n_heads))",
      "expected_output": "[[500, 463, 399, 495, 377, 450, 531, 362],\n                                [500, 463, 399, 495, 377, 450, 531, 362],\n                                [500, 463, 399, 495, 377, 450, 531, 362],\n                                [500, 463, 399, 495, 377, 450, 531, 362],\n                                [500, 463, 399, 495, 377, 450, 531, 362],\n                                [500, 463, 399, 495, 377, 450, 531, 362]]"
    },
    {
      "test": "m, n = 6, 8\nn_heads = 2\nnp.random.seed(42)\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\nW_q = np.random.randint(0,4,size=(n,n))\nW_k = np.random.randint(0,5,size=(n,n))\nW_v = np.random.randint(0,6,size=(n,n))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n# test multi-head attention\nactual_output = multi_head_attention(Q, K, V, n_heads)\nprint(actual_output)",
      "expected_output": "[[547, 490, 399, 495, 377, 450, 531, 362],\n                                [547, 490, 399, 495, 377, 450, 531, 362],\n                                [547, 490, 399, 495, 377, 450, 531, 362],\n                                [547, 490, 399, 495, 377, 450, 531, 362],\n                                [547, 490, 399, 495, 377, 450, 531, 362],\n                                [547, 490, 399, 495, 377, 450, 531, 362]]"
    }
  ]
}