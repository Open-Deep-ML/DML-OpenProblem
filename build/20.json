{
  "id": "20",
  "title": "Decision Tree Learning",
  "difficulty": "hard",
  "category": "Machine Learning",
  "video": null,
  "likes": "0",
  "dislikes": "0",
  "contributor": null,
  "tinygrad_difficulty": "hard",
  "pytorch_difficulty": "hard",
  "description": "Write a Python function that implements the decision tree learning algorithm for classification. The function should use recursive binary splitting based on entropy and information gain to build a decision tree. It should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as input, and return a nested dictionary representing the decision tree.",
  "learn_section": "\n## Decision Tree Learning Algorithm\n\nThe decision tree learning algorithm is a method used for classification that predicts the value of a target variable based on several input variables. Each internal node of the tree corresponds to an input variable, and each leaf node corresponds to a class label.\n\n### Algorithm Overview\nThe recursive binary splitting starts by selecting the attribute that best separates the examples according to the entropy and information gain, calculated as follows:\n\n### Entropy\n$$\nH(X) = -\\sum p(x) \\log_2 p(x)\n$$\n\n### Information Gain\n$$\nIG(D, A) = H(D) - \\sum \\frac{|D_v|}{|D|} H(D_v)\n$$\n\n### Explanation of Terms\n- **Entropy \\( H(X) \\)**: Measures the impurity or disorder of the set.\n- **Information Gain \\( IG(D, A) \\)**: Represents the reduction in entropy after splitting the dataset \\( D \\) on attribute \\( A \\).\n- **\\( D_v \\)**: The subset of \\( D \\) for which attribute \\( A \\) has value \\( v \\).\n\n### Process\n1. **Select Attribute**: Choose the attribute with the highest information gain.\n2. **Split Dataset**: Divide the dataset based on the values of the selected attribute.\n3. **Recursion**: Repeat the process for each subset until:\n   - All data is perfectly classified, or\n   - No remaining attributes can be used to make a split.\n\nThis recursive process continues until the decision tree can no longer be split further or all examples have been classified.",
  "starter_code": "def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\t# Your code here\n\treturn decision_tree",
  "solution": "\nimport math\nfrom collections import Counter\n\ndef calculate_entropy(labels):\n    label_counts = Counter(labels)\n    total_count = len(labels)\n    entropy = -sum((count / total_count) * math.log2(count / total_count) for count in label_counts.values())\n    return entropy\n\ndef calculate_information_gain(examples, attr, target_attr):\n    total_entropy = calculate_entropy([example[target_attr] for example in examples])\n    values = set(example[attr] for example in examples)\n    attr_entropy = 0\n    for value in values:\n        value_subset = [example[target_attr] for example in examples if example[attr] == value]\n        value_entropy = calculate_entropy(value_subset)\n        attr_entropy += (len(value_subset) / len(examples)) * value_entropy\n    return total_entropy - attr_entropy\n\ndef majority_class(examples, target_attr):\n    return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n\ndef learn_decision_tree(examples, attributes, target_attr):\n    if not examples:\n        return 'No examples'\n    if all(example[target_attr] == examples[0][target_attr] for example in examples):\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_class(examples, target_attr)\n    \n    gains = {attr: calculate_information_gain(examples, attr, target_attr) for attr in attributes}\n    best_attr = max(gains, key=gains.get)\n    tree = {best_attr: {}}\n    \n    for value in set(example[best_attr] for example in examples):\n        subset = [example for example in examples if example[best_attr] == value]\n        new_attributes = attributes.copy()\n        new_attributes.remove(best_attr)\n        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    \n    return tree",
  "example": {
    "input": "examples = [\n                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n                    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n                    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n                ],\n                attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']",
    "output": "{\n            'Outlook': {\n                'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n                'Overcast': 'Yes',\n                'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}\n            }\n        }",
    "reasoning": "Using the given examples, the decision tree algorithm determines that 'Outlook' is the best attribute to split the data initially. When 'Outlook' is 'Overcast', the outcome is always 'Yes', so it becomes a leaf node. In cases of 'Sunny' and 'Rain', it further splits based on 'Humidity' and 'Wind', respectively. The resulting tree structure is able to classify the training examples with the attributes 'Outlook', 'Temperature', 'Humidity', and 'Wind'."
  },
  "test_cases": [
    {
      "test": "print(learn_decision_tree([\n    {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'},\n    {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))",
      "expected_output": "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"
    }
  ],
  "tinygrad_starter_code": "import math\nfrom collections import Counter\nfrom typing import List, Dict, Any, Union\n\n\ndef learn_decision_tree_tg(\n    examples: List[Dict[str, Any]],\n    attributes: List[str],\n    target_attr: str\n) -> Union[Dict[str, Any], Any]:\n    \"\"\"\n    Learn a decision tree using ID3 with tinygrad for entropy/gain.\n    Returns a nested dict tree or a class label.\n    \"\"\"\n    # Your implementation here\n    pass",
  "tinygrad_solution": "import math\nfrom collections import Counter\nfrom tinygrad.tensor import Tensor\nfrom typing import List, Dict, Any, Union\n\n\ndef calculate_entropy_tg(labels) -> float:\n    arr = labels.tolist() if isinstance(labels, Tensor) else labels\n    total = len(arr)\n    cnt = Counter(arr)\n    return -sum((c/total)*math.log2(c/total) for c in cnt.values())\n\n\ndef calculate_information_gain_tg(\n    examples: List[Dict[str, Any]],\n    attr: str,\n    target_attr: str\n) -> float:\n    total = [ex[target_attr] for ex in examples]\n    total_ent = calculate_entropy_tg(total)\n    n = len(examples)\n    rem = 0.0\n    for v in set(ex[attr] for ex in examples):\n        subset = [ex[target_attr] for ex in examples if ex[attr] == v]\n        rem += (len(subset)/n) * calculate_entropy_tg(subset)\n    return total_ent - rem\n\n\ndef majority_class_tg(\n    examples: List[Dict[str, Any]],\n    target_attr: str\n) -> Any:\n    return Counter(ex[target_attr] for ex in examples).most_common(1)[0][0]\n\n\ndef learn_decision_tree_tg(\n    examples: List[Dict[str, Any]],\n    attributes: List[str],\n    target_attr: str\n) -> Union[Dict[str, Any], Any]:\n    if not examples:\n        return 'No examples'\n    first_label = examples[0][target_attr]\n    if all(ex[target_attr] == first_label for ex in examples):\n        return first_label\n    if not attributes:\n        return majority_class_tg(examples, target_attr)\n    gains = {a: calculate_information_gain_tg(examples, a, target_attr) for a in attributes}\n    best = max(gains, key=gains.get)\n    tree: Dict[str, Any] = {best: {}}\n    for v in set(ex[best] for ex in examples):\n        subset = [ex for ex in examples if ex[best] == v]\n        rem_attrs = [a for a in attributes if a != best]\n        tree[best][v] = learn_decision_tree_tg(subset, rem_attrs, target_attr)\n    return tree",
  "tinygrad_test_cases": [
    {
      "test": "print(learn_decision_tree_tg([\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'No'},\n    {'Outlook': 'Overcast','Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny',   'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast','Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))",
      "expected_output": "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"
    }
  ],
  "pytorch_starter_code": "import torch\nimport math\nfrom collections import Counter\nfrom typing import List, Dict, Any, Union\n\n\ndef calculate_entropy(labels: List[Any]) -> float:\n    \"\"\"\n    Compute the Shannon entropy of the list of labels.\n    labels: list of any hashable items.\n    Returns a Python float.\n    \"\"\"\n    # Your implementation here\n    pass\n\n\ndef calculate_information_gain(\n    examples: List[Dict[str, Any]],\n    attr: str,\n    target_attr: str\n) -> float:\n    \"\"\"\n    Compute information gain for splitting `examples` on `attr` w.r.t. `target_attr`.\n    Returns a Python float.\n    \"\"\"\n    # Your implementation here\n    pass\n\n\ndef majority_class(\n    examples: List[Dict[str, Any]],\n    target_attr: str\n) -> Any:\n    \"\"\"\n    Return the most common value of `target_attr` in `examples`.\n    \"\"\"\n    # Your implementation here\n    pass\n\n\ndef learn_decision_tree(\n    examples: List[Dict[str, Any]],\n    attributes: List[str],\n    target_attr: str\n) -> Union[Dict[str, Any], Any]:\n    \"\"\"\n    Learn a decision tree using the ID3 algorithm.\n    Returns either a nested dict representing the tree or a class label at the leaves.\n    \"\"\"\n    # Your implementation here\n    pass",
  "pytorch_solution": "import torch\nimport math\nfrom collections import Counter\nfrom typing import List, Dict, Any, Union\n\n\ndef calculate_entropy(labels: List[Any]) -> float:\n    counts = Counter(labels)\n    total = sum(counts.values())\n    entropy = 0.0\n    for count in counts.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    return entropy\n\n\ndef calculate_information_gain(\n    examples: List[Dict[str, Any]],\n    attr: str,\n    target_attr: str\n) -> float:\n    total_labels = [ex[target_attr] for ex in examples]\n    total_ent = calculate_entropy(total_labels)\n    n = len(examples)\n    rem = 0.0\n    for v in set(ex[attr] for ex in examples):\n        subset_labels = [ex[target_attr] for ex in examples if ex[attr] == v]\n        rem += (len(subset_labels)/n) * calculate_entropy(subset_labels)\n    return total_ent - rem\n\n\ndef majority_class(\n    examples: List[Dict[str, Any]],\n    target_attr: str\n) -> Any:\n    return Counter(ex[target_attr] for ex in examples).most_common(1)[0][0]\n\n\ndef learn_decision_tree(\n    examples: List[Dict[str, Any]],\n    attributes: List[str],\n    target_attr: str\n) -> Union[Dict[str, Any], Any]:\n    if not examples:\n        return 'No examples'\n    first_label = examples[0][target_attr]\n    if all(ex[target_attr] == first_label for ex in examples):\n        return first_label\n    if not attributes:\n        return majority_class(examples, target_attr)\n    gains = {a: calculate_information_gain(examples, a, target_attr) for a in attributes}\n    best = max(gains, key=gains.get)\n    tree: Dict[str, Any] = {best: {}}\n    for v in set(ex[best] for ex in examples):\n        subset = [ex for ex in examples if ex[best] == v]\n        rem_attrs = [a for a in attributes if a != best]\n        tree[best][v] = learn_decision_tree(subset, rem_attrs, target_attr)\n    return tree",
  "pytorch_test_cases": [
    {
      "test": "print(learn_decision_tree([\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'No'},\n    {'Outlook': 'Overcast','Wind': 'Strong', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Sunny',   'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny',   'Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Overcast','Wind': 'Weak',   'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain',    'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Rain',    'Wind': 'Weak',   'PlayTennis': 'Yes'}\n], ['Outlook', 'Wind'], 'PlayTennis'))",
      "expected_output": "{'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}"
    }
  ]
}