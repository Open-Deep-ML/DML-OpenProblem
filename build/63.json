{
  "id": "63",
  "title": "Implement the Conjugate Gradient Method for Solving Linear Systems",
  "difficulty": "hard",
  "category": "Linear Algebra",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/paddywardle",
      "name": "paddywardle"
    }
  ],
  "description": "## Task: Implement the Conjugate Gradient Method for Solving Linear Systems\n\nYour task is to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. Given a matrix `A` and a vector `b`, the algorithm will solve for `x` in the system \\( Ax = b \\).\n\nWrite a function `conjugate_gradient(A, b, n, x0=None, tol=1e-8)` that performs the Conjugate Gradient method as follows:\n\n- `A`: A symmetric, positive-definite matrix representing the linear system.\n- `b`: The vector on the right side of the equation.\n- `n`: Maximum number of iterations.\n- `x0`: Initial guess for the solution vector.\n- `tol`: Tolerance for stopping criteria.\n\nThe function should return the solution vector `x`.\n\n    ",
  "learn_section": "\n## Understanding The Conjugate Gradient Method\n\nThe Conjugate Gradient (CG) method is an iterative algorithm used to solve large systems of linear equations, particularly those that are symmetric and positive-definite.\n\n### Concepts\n\nThe CG gradient method is often applied to the quadratic form of a linear system, $Ax = b$:\n\n$$\nf(x) = \\frac{1}{2} x^T A x - b^T x\n$$\n\nThe quadratic form is used due to its differential reducing to the following for a symmetric $A$. Therefore, $x$ satisfies $Ax = b$ at the optimum:\n\n$$\nf'(x) = Ax - b\n$$\n\nThe conjugate gradient method uses search directions that are conjugate to all the previous search directions. This is satisfied when search directions are $A$-orthogonal, i.e.\n\n$$\np_i^T A p_j = 0 \\quad \\text{for } i \\neq j\n$$\n\nThis results in a more efficient algorithm, as it ensures that the algorithm gathers all information in a search direction at once and then doesn't need to search in that direction again. This is opposed to steepest descent, where the algorithm steps a bit in one direction and then may search in that direction again later.\n\n### Algorithm Steps\n\n1) **Initialization**:\n   - $ x_0 $: Initial guess for the variable vector.\n   - $ r_0 = b - A x_0 $: Initial residual vector.\n   - $ p_0 = r_0 $: Initial search direction.\n\n2) **Iteration $k$**:\n   - $ \\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k} $: Step size.\n   - $ x_{k+1} = x_k + \\alpha_k p_k $: Update solution.\n   - $ r_{k+1} = r_k - \\alpha_k A p_k $: Update residual.\n   - Check convergence: $ \\| r_{k+1} \\| < \\text{tolerance} $.\n   - $ \\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k} $: New direction scaling. This ensures search directions are $A$-orthogonal.\n   - $ p_{k+1} = r_{k+1} + \\beta_k p_k $: Update search direction.\n\n3) **Termination**:\n   - Stop when $ \\| r_{k+1} \\| < \\text{tolerance} $ or after a set number of iterations.\n\n### Example Calculation\n\nLet's solve the system of equations:\n\n$$\n4x_1 + x_2 = 6 \\quad x_1 + 3x_2 = 6\n$$\n\n1) **Initialize**: $ x_0 = [0, 0]^T $, $ r_0 = b - A x_0 = [6, 6]^T $, and $ p_0 = r_0 = [6, 6]^T $.\n\n2) **First iteration**:\n   - Compute $ \\alpha_0 $:\n\n   $$\n   \\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0} = \\frac{72}{324} = 0.2222\n   $$\n\n   - Update solution $ x_1 $:\n\n   $$\n   x_1 = x_0 + \\alpha_0 p_0 = [0, 0]^T + 0.2222 \\cdot [6, 6]^T = [1.3333, 1.3333]^T\n   $$\n\n   - Update residual $ r_1 $:\n\n   $$\n   r_1 = r_0 - \\alpha_0 A p_0 = [6, 6]^T - 0.2222 \\cdot \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix} \\cdot [6, 6]^T = [6.67, 5.33]^T\n   $$\n\n   - Compute $ \\beta_0 $:\n\n   $$\n   \\beta_0 = \\frac{r_1^T r_1}{r_0^T r_0} = \\frac{6.67^2 + 5.33^2}{6^2 + 6^2} \\approx 0.99\n   $$\n\n   - Update search direction $ p_1 $:\n\n   $$\n   p_1 = r_1 + \\beta_0 p_0 = [6.67, 5.33]^T + 0.99 \\cdot [6, 6]^T = [12.60, 11.26]^T\n   $$\n\n3) **Second iteration**:\n   - Compute $ \\alpha_1 $, $ x_2 $, $ r_2 $, and repeat until convergence.\n\n### Applications\n\nThe conjugate gradient method is often used because it's more efficient than other iterative solvers, such as steepest descent, and direct solvers, such as Gaussian Elimination. Iterative linear solvers are commonly used in:\n\n- Optimization\n- Machine Learning\n- Computational Fluid Dynamics",
  "starter_code": "import numpy as np\n\ndef conjugate_gradient(A, b, n, x0=None, tol=1e-8):\n\t\"\"\"\n\tSolve the system Ax = b using the Conjugate Gradient method.\n\n\t:param A: Symmetric positive-definite matrix\n\t:param b: Right-hand side vector\n\t:param n: Maximum number of iterations\n\t:param x0: Initial guess for solution (default is zero vector)\n\t:param tol: Convergence tolerance\n\t:return: Solution vector x\n\t\"\"\"\n\t# calculate initial residual vector\n\tx = np.zeros_like(b)",
  "solution": "import numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8) -> np.array:\n\n    # calculate initial residual vector\n    x = np.zeros_like(b)\n    r = residual(A, b, x) # residual vector\n    rPlus1 = r\n    p = r # search direction vector\n\n    for i in range(n):\n\n        # line search step value - this minimizes the error along the current search direction\n        alp = alpha(A, r, p)\n\n        # new x and r based on current p (the search direction vector)\n        x = x + alp * p\n        rPlus1 = r - alp * (A@p)\n\n        # calculate beta - this ensures that all vectors are A-orthogonal to each other\n        bet = beta(r, rPlus1)\n\n        # update x and r\n        # using a othogonal search direction ensures we get all the information we need in more direction and then don't have to search in that direction again\n        p = rPlus1 + bet * p\n\n        # update residual vector\n        r = rPlus1\n\n        # break if less than tolerance\n        if np.linalg.norm(residual(A,b,x)) < tol:\n            break\n\n    return x\n\ndef residual(A: np.array, b: np.array, x: np.array) -> np.array:\n    # calculate linear system residuals\n    return b - A @ x\n\ndef alpha(A: np.array, r: np.array, p: np.array) -> float:\n\n    # calculate step size\n    alpha_num = np.dot(r, r)\n    alpha_den = np.dot(p @ A, p)\n\n    return alpha_num/alpha_den\n\ndef beta(r: np.array, r_plus1: np.array) -> float:\n\n    # calculate direction scaling\n    beta_num = np.dot(r_plus1, r_plus1)\n    beta_den = np.dot(r, r)\n\n    return beta_num/beta_den",
  "example": {
    "input": "A = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5\n\nprint(conjugate_gradient(A, b, n))",
    "output": "[0.09090909, 0.63636364]",
    "reasoning": "The Conjugate Gradient method is applied to the linear system Ax = b with the given matrix A and vector b. The algorithm iteratively refines the solution to converge to the exact solution."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5\nprint(conjugate_gradient(A, b, n))",
      "expected_output": "[0.09090909, 0.63636364]"
    },
    {
      "test": "import numpy as np\n\nA = np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]])\nb = np.array([7, 8, 5])\nn = 1\nprint(conjugate_gradient(A, b, n))",
      "expected_output": "[1.2627451, 1.44313725, 0.90196078]"
    },
    {
      "test": "import numpy as np\n\nA = np.array([[6, 2, 1, 1, 0],\n              [2, 5, 2, 1, 1],\n              [1, 2, 6, 1, 2],\n              [1, 1, 1, 7, 1],\n              [0, 1, 2, 1, 8]])\nb = np.array([1, 2, 3, 4, 5])\nn = 100\nprint(conjugate_gradient(A, b, n))",
      "expected_output": "[0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]"
    }
  ]
}