{
  "id": "60",
  "title": "Implement TF-IDF (Term Frequency-Inverse Document Frequency)",
  "difficulty": "medium",
  "category": "NLP",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Hui-cd",
      "name": "Hui"
    }
  ],
  "description": "## Task: Implement TF-IDF (Term Frequency-Inverse Document Frequency)\n\nYour task is to implement a function that computes the TF-IDF scores for a query against a given corpus of documents.\n\n### Function Signature\n\nWrite a function `compute_tf_idf(corpus, query)` that takes the following inputs:\n\n- `corpus`: A list of documents, where each document is a list of words.\n- `query`: A list of words for which you want to compute the TF-IDF scores.\n\n### Output\n\nThe function should return a list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\n\n### Important Considerations\n\n1. **Handling Division by Zero:**  \n   When implementing the Inverse Document Frequency (IDF) calculation, you must account for cases where a term does not appear in any document (`df = 0`). This can lead to division by zero in the standard IDF formula. Add smoothing (e.g., adding 1 to both numerator and denominator) to avoid such errors.\n\n2. **Empty Corpus:**  \n   Ensure your implementation gracefully handles the case of an empty corpus. If no documents are provided, your function should either raise an appropriate error or return an empty result. This will ensure the program remains robust and predictable.\n\n3. **Edge Cases:**  \n   - Query terms not present in the corpus.  \n   - Documents with no words.  \n   - Extremely large or small values for term frequencies or document frequencies.\n\nBy addressing these considerations, your implementation will be robust and handle real-world scenarios effectively.",
  "learn_section": "# Understanding TF-IDF (Term Frequency-Inverse Document Frequency)\n\nTF-IDF is a numerical statistic that reflects how important a word is in a document relative to a collection (or corpus). It is widely used in information retrieval, text mining, and natural language processing tasks.\n\n## Mathematical Formulation\n\nTF-IDF is the product of two key statistics: **Term Frequency (TF)** and **Inverse Document Frequency (IDF)**.\n\n### 1. Term Frequency (TF)\n\nThe term frequency is defined as:\n\n$TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}$\n\n- $t$: A specific term (word).\n- $d$: A specific document in the corpus.\n\n### 2. Inverse Document Frequency (IDF)\n\nTo account for how common or rare a term is across all documents in the corpus, we calculate:\n\n$IDF(t) = \\log\\Bigl(\\frac{N + 1}{\\text{df}(t) + 1}\\Bigr) + 1$\n\nWhere:\n\n- $N$: Total number of documents in the corpus.\n- $\\text{df}(t)$: Number of documents containing the term $t$.\n- Adding $+1$ inside the fraction prevents division by zero if a term never appears.\n- Adding $+1$ outside the log ensures IDF remains nonzero.\n\n### 3. TF-IDF\n\nCombining TF and IDF:\n\n$TFIDF(t, d) = TF(t, d) \\times IDF(t)$\n\n## Implementation Steps\n\n1. **Compute TF**  \n   For each document, count how often each term appears and divide by the document’s total word count.\n\n2. **Compute IDF**  \n   For each term, calculate its document frequency across all documents and apply the IDF formula.\n\n3. **Calculate TF-IDF**  \n   For every term in every document, multiply the term’s TF by its IDF.\n\n4. **Normalization (Optional)**  \n   Normalize TF-IDF vectors (e.g., using $L2$ norm) if comparing documents in a vector space model.\n\n## Example Calculation\n\nSuppose we have a small corpus of 3 documents:\n\n- **Doc1**: \"The cat sat on the mat\"  \n- **Doc2**: \"The dog chased the cat\"  \n- **Doc3**: \"The bird flew over the mat\"\n\nWe want to calculate the TF-IDF for the word **\"cat\"** in **Doc1**.\n\n### Step 1: Compute $TF(\"cat\", \\text{Doc1})$\n\n$TF(\"cat\", \\text{Doc1}) = \\frac{1}{6} \\approx 0.1667$\n\n- \"cat\" appears once.\n- Total words in Doc1 (counting each occurrence of “the”) = 6.\n\n### Step 2: Compute $IDF(\"cat\")$\n\n- \"cat\" appears in 2 out of 3 documents, so $\\text{df}(\"cat\") = 2$.\n- $N = 3$.\n\nUsing the formula with smoothing and an added constant:\n\n$IDF(\"cat\") = \\log\\Bigl(\\frac{N + 1}{\\text{df}(\"cat\") + 1}\\Bigr) + 1 = \\log\\Bigl(\\frac{3 + 1}{2 + 1}\\Bigr) + 1 = \\log\\Bigl(\\frac{4}{3}\\Bigr) + 1 \\approx 0.2877 + 1 = 1.2877$\n\n### Step 3: Calculate $TFIDF(\"cat\", \\text{Doc1})$\n\n$TFIDF(\"cat\", \\text{Doc1}) = TF(\"cat\", \\text{Doc1}) \\times IDF(\"cat\") = 0.1667 \\times 1.2877 \\approx 0.2147$\n\n## Applications of TF-IDF\n\n1. **Information Retrieval**  \n   TF-IDF is often used in search engines to rank how relevant a document is to a given query.\n2. **Text Mining**  \n   Helps identify key terms and topics in large volumes of text.\n3. **Document Classification**  \n   Useful for weighting important words in classification tasks.\n4. **Search Engines**  \n   Refines document ranking by emphasizing distinctive terms.\n5. **Recommendation Systems**  \n   Evaluates text-based similarity (e.g., for content-based filtering).\n\nTF-IDF remains a foundational technique in natural language processing, widely used for feature extraction and analysis across numerous text-based applications.",
  "starter_code": "import numpy as np\n\ndef compute_tf_idf(corpus, query):\n\t\"\"\"\n\tCompute TF-IDF scores for a query against a corpus of documents.\n    \n\t:param corpus: List of documents, where each document is a list of words\n\t:param query: List of words in the query\n\t:return: List of lists containing TF-IDF scores for the query words in each document\n\t\"\"\"\n\tpass",
  "solution": "import numpy as np\n\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a corpus of documents using only NumPy.\n    The output TF-IDF scores retain five decimal places.\n    \"\"\"\n    vocab = sorted(set(word for document in corpus for word in document).union(query))\n    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n\n    tf = np.zeros((len(corpus), len(vocab)))\n\n    for doc_idx, document in enumerate(corpus):\n        for word in document:\n            word_idx = word_to_index[word]\n            tf[doc_idx, word_idx] += 1\n        tf[doc_idx, :] /= len(document)\n\n    df = np.count_nonzero(tf > 0, axis=0)\n\n    num_docs = len(corpus)\n    idf = np.log((num_docs + 1) / (df + 1)) + 1\n\n    tf_idf = tf * idf\n\n    query_indices = [word_to_index[word] for word in query]\n    tf_idf_scores = tf_idf[:, query_indices]\n\n    tf_idf_scores = np.round(tf_idf_scores, 5)\n\n    return tf_idf_scores.tolist()",
  "example": {
    "input": "corpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\"]\n\nprint(compute_tf_idf(corpus, query))",
    "output": "[[0.21461], [0.25754], [0.0]]",
    "reasoning": "The TF-IDF scores for the word \"cat\" in each document are computed and rounded to five decimal places."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\"]\n\nprint(compute_tf_idf(corpus, query))",
      "expected_output": "[[0.21461], [0.25754], [0.0]]"
    },
    {
      "test": "import numpy as np\n\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\", \"mat\"]\n\nprint(compute_tf_idf(corpus, query))",
      "expected_output": "[[0.21461, 0.21461], [0.25754, 0.0], [0.0, 0.21461]]"
    },
    {
      "test": "import numpy as np\n\ncorpus = [\n    [\"this\", \"is\", \"a\", \"sample\"],\n    [\"this\", \"is\", \"another\", \"example\"],\n    [\"yet\", \"another\", \"sample\", \"document\"],\n    [\"one\", \"more\", \"document\", \"for\", \"testing\"]\n]\nquery = [\"sample\", \"document\", \"test\"]\n\nprint(compute_tf_idf(corpus, query))",
      "expected_output": "[[0.37771, 0.0, 0.0], [0.0, 0.0, 0.0], [0.37771, 0.37771, 0.0], [0.0, 0.30217, 0.0]]"
    }
  ]
}