{
  "id": "142",
  "title": "Gridworld Policy Evaluation",
  "difficulty": "medium",
  "category": "Reinforcement Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/arpitsinghgautam",
      "name": "Arpit Singh Gautam"
    }
  ],
  "description": "Implement policy evaluation for a 5x5 gridworld. Given a policy (mapping each state to action probabilities), compute the state-value function $V(s)$ for each cell using the Bellman expectation equation. The agent can move up, down, left, or right, receiving a constant reward of -1 for each move. Terminal states (the four corners) are fixed at 0. Iterate until the largest change in $V$ is less than a given threshold. Only use Python built-ins and no external RL libraries.",
  "learn_section": "# Gridworld Policy Evaluation\n\nIn reinforcement learning, **policy evaluation** is the process of computing the state-value function for a given policy. For a gridworld environment, this involves iteratively updating the value of each state based on the expected return following the policy.\n\n## Key Concepts\n\n- **State-Value Function (V):**  \n  The expected return when starting from a state and following a given policy.\n\n- **Policy:**  \n  A mapping from states to probabilities of selecting each available action.\n\n- **Bellman Expectation Equation:**  \n  For each state $s$:\n  $$\n  V(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V(s')]\n  $$\n  where:\n  - $ \\pi(a|s) $ is the probability of taking action $ a $ in state $ s $,\n  - $ P(s'|s,a) $ is the probability of transitioning to state $ s' $,\n  - $ R(s,a,s') $ is the reward for that transition,\n  - $ \\gamma $ is the discount factor.\n\n## Algorithm Overview\n\n1. **Initialization:**  \n   Start with an initial guess (commonly zeros) for the state-value function $ V(s) $.\n\n2. **Iterative Update:**  \n   For each non-terminal state, update the state value using the Bellman expectation equation. Continue updating until the maximum change in value (delta) is less than a given threshold.\n\n3. **Terminal States:**  \n   For this example, the four corners of the grid are considered terminal, so their values remain unchanged.\n\nThis evaluation method is essential for understanding how \"good\" each state is under a specific policy, and it forms the basis for more advanced reinforcement learning algorithms.",
  "starter_code": "def gridworld_policy_evaluation(policy: dict, gamma: float, threshold: float) -> list[list[float]]:\n    \"\"\"\n    Evaluate state-value function for a policy on a 5x5 gridworld.\n    \n    Args:\n        policy: dict mapping (row, col) to action probability dicts\n        gamma: discount factor\n        threshold: convergence threshold\n    Returns:\n        5x5 list of floats\n    \"\"\"\n    # Your code here\n    pass",
  "solution": "def gridworld_policy_evaluation(policy: dict, gamma: float, threshold: float) -> list[list[float]]:\n    grid_size = 5\n    V = [[0.0 for _ in range(grid_size)] for _ in range(grid_size)]\n    actions = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n    reward = -1\n    while True:\n        delta = 0.0\n        new_V = [row[:] for row in V]\n        for i in range(grid_size):\n            for j in range(grid_size):\n                if (i, j) in [(0, 0), (0, grid_size-1), (grid_size-1, 0), (grid_size-1, grid_size-1)]:\n                    continue\n                v = 0.0\n                for action, prob in policy[(i, j)].items():\n                    di, dj = actions[action]\n                    ni = i + di if 0 <= i + di < grid_size else i\n                    nj = j + dj if 0 <= j + dj < grid_size else j\n                    v += prob * (reward + gamma * V[ni][nj])\n                new_V[i][j] = v\n                delta = max(delta, abs(V[i][j] - new_V[i][j]))\n        V = new_V\n        if delta < threshold:\n            break\n    return V",
  "example": {
    "input": "policy = {(i, j): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25} for i in range(5) for j in range(5)}\ngamma = 0.9\nthreshold = 0.001\nV = gridworld_policy_evaluation(policy, gamma, threshold)\nprint(round(V[2][2], 4))",
    "output": "-7.0902",
    "reasoning": "The policy is uniform (equal chance of each move). The agent receives -1 per step. After iterative updates, the center state value converges to about -7.09, and corners remain at 0."
  },
  "test_cases": [
    {
      "test": "grid_size = 5\ngamma = 0.9\nthreshold = 0.001\npolicy = {(i, j): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25} for i in range(grid_size) for j in range(grid_size)}\nV = gridworld_policy_evaluation(policy, gamma, threshold)\nprint([round(V[2][2], 4), V[0][0], V[0][4], V[4][0], V[4][4]])",
      "expected_output": "[-7.0902, 0.0, 0.0, 0.0, 0.0]"
    },
    {
      "test": "grid_size = 5\ngamma = 0.9\nthreshold = 0.001\npolicy = {(i, j): {'up': 0.1, 'down': 0.4, 'left': 0.1, 'right': 0.4} for i in range(grid_size) for j in range(grid_size)}\nV = gridworld_policy_evaluation(policy, gamma, threshold)\nprint(round(V[1][3], 4) < 0)",
      "expected_output": "True"
    }
  ]
}