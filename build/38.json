{
  "id": "38",
  "title": "Implement AdaBoost Fit Method",
  "difficulty": "hard",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/eriklindernoren/ML-From-Scratch",
      "name": "Erik Linder-NorÃ©n"
    },
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description": "Write a Python function `adaboost_fit` that implements the fit method for an AdaBoost classifier. The function should take in a 2D numpy array `X` of shape `(n_samples, n_features)` representing the dataset, a 1D numpy array `y` of shape `(n_samples,)` representing the labels, and an integer `n_clf` representing the number of classifiers. The function should initialize sample weights, find the best thresholds for each feature, calculate the error, update weights, and return a list of classifiers with their parameters.",
  "learn_section": "\n## Understanding AdaBoost\n\nAdaBoost, short for Adaptive Boosting, is an ensemble learning method that combines multiple weak classifiers to create a strong classifier. The basic idea is to fit a sequence of weak learners on weighted versions of the data.\n\n### Implementing the Fit Method for an AdaBoost Classifier\n\n1. **Initialize Weights**  \n   Start by initializing the sample weights uniformly:\n   $$\n   w_i = \\frac{1}{N}, \\text{ where } N \\text{ is the number of samples}\n   $$\n\n2. **Iterate Through Classifiers**  \n   For each classifier, determine the best threshold for each feature to minimize the error.\n\n3. **Calculate Error and Flip Polarity**  \n   If the error is greater than 0.5, flip the polarity:\n   $$\n   \\text{error} = \\sum_{i=1}^N w_i [y_i \\neq h(x_i)]\n   $$\n   $$\n   \\text{if error} > 0.5: \\text{error} = 1 - \\text{error}, \\text{ and flip the polarity}\n   $$\n\n4. **Calculate Alpha**  \n   Compute the weight (alpha) of the classifier based on its error rate:\n   $$\n   \\alpha = \\frac{1}{2} \\ln \\left( \\frac{1 - \\text{error}}{\\text{error} + 1e-10} \\right)\n   $$\n\n5. **Update Weights**  \n   Adjust the sample weights based on the classifier's performance and normalize them:\n   $$\n   w_i = w_i \\exp(-\\alpha y_i h(x_i))\n   $$\n   $$\n   w_i = \\frac{w_i}{\\sum_{j=1}^N w_j}\n   $$\n\n6. **Save Classifier**  \n   Store the classifier with its parameters.\n\n### Key Insight\nThis method helps in focusing more on the misclassified samples in subsequent rounds, thereby improving the overall performance.",
  "starter_code": "import numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n\tn_samples, n_features = np.shape(X)\n\tw = np.full(n_samples, (1 / n_samples))\n\tclfs = []\n\n\t# Your code here\n\n\treturn clfs\n    ",
  "solution": "import math\nimport numpy as np\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = np.shape(X)\n    w = np.full(n_samples, (1 / n_samples))\n    clfs = []\n    \n    for _ in range(n_clf):\n        clf = {}\n        min_error = float('inf')\n        \n        for feature_i in range(n_features):\n            feature_values = np.expand_dims(X[:, feature_i], axis=1)\n            unique_values = np.unique(feature_values)\n            \n            for threshold in unique_values:\n                p = 1\n                prediction = np.ones(np.shape(y))\n                prediction[X[:, feature_i] < threshold] = -1\n                error = sum(w[y != prediction])\n                \n                if error > 0.5:\n                    error = 1 - error\n                    p = -1\n                \n                if error < min_error:\n                    clf['polarity'] = p\n                    clf['threshold'] = threshold\n                    clf['feature_index'] = feature_i\n                    min_error = error\n        \n        clf['alpha'] = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = np.ones(np.shape(y))\n        negative_idx = (X[:, clf['feature_index']] < clf['threshold'])\n        if clf['polarity'] == -1:\n            negative_idx = np.logical_not(negative_idx)\n        predictions[negative_idx] = -1\n        w *= np.exp(-clf['alpha'] * y * predictions)\n        w /= np.sum(w)\n        clfs.append(clf)\n\n    return clfs",
  "example": {
    "input": "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([1, 1, -1, -1])\n    n_clf = 3\n\n    clfs = adaboost_fit(X, y, n_clf)\n    print(clfs)",
    "output": "(example format, actual values may vary):\n    # [{'polarity': 1, 'threshold': 2, 'feature_index': 0, 'alpha': 0.5},\n    #  {'polarity': -1, 'threshold': 3, 'feature_index': 1, 'alpha': 0.3},\n    #  {'polarity': 1, 'threshold': 4, 'feature_index': 0, 'alpha': 0.2}]",
    "reasoning": "The function fits an AdaBoost classifier on the dataset X with the given labels y and number of classifiers n_clf. It returns a list of classifiers with their parameters, including the polarity, threshold, feature index, and alpha values"
  },
  "test_cases": [
    {
      "test": "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 1, -1, -1])\nn_clf = 3\nclfs = adaboost_fit(X, y, n_clf)\nprint(clfs)",
      "expected_output": "[{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.512925464970229}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.512925464970229}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.512925464970229}]"
    },
    {
      "test": "X = np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]])\ny = np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1])\nn_clf = 2\nclfs = adaboost_fit(X, y, n_clf)\nprint(clfs)",
      "expected_output": "[{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931471803099453}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493061439673882}]"
    }
  ]
}