{
  "id": "16",
  "title": "Feature Scaling Implementation",
  "difficulty": "easy",
  "category": "Machine Learning",
  "video": "https://youtu.be/RrEbO-lbg84?si=hk2qk-26eUyXh0Ee",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "tinygrad_difficulty": "easy",
  "pytorch_difficulty": "easy",
  "description": "Write a Python function that performs feature scaling on a dataset using both standardization and min-max normalization. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. It should return two 2D NumPy arrays: one scaled by standardization and one by min-max normalization. Make sure all results are rounded to the nearest 4th decimal.",
  "learn_section": "\n## Feature Scaling Techniques\n\nFeature scaling is crucial in many machine learning algorithms that are sensitive to the magnitude of features. This includes algorithms that use distance measures, like k-nearest neighbors, and gradient descent-based algorithms, like linear regression.\n\n### Standardization\nStandardization (or Z-score normalization) is the process where features are rescaled so that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one:\n$$\nz = \\frac{(x - \\mu)}{\\sigma}\n$$\nwhere \\( x \\) is the original feature, \\( \\mu \\) is the mean of that feature, and \\( \\sigma \\) is the standard deviation.\n\n### Min-Max Normalization\nMin-max normalization rescales the feature to a fixed range, typically 0 to 1, or it can be shifted to any range \\([a, b]\\) by transforming the data using the formula:\n$$\nx' = \\frac{(x - \\text{min}(x))}{(\\text{max}(x) - \\text{min}(x))} \\times (\\text{max} - \\text{min}) + \\text{min}\n$$\nwhere \\( x \\) is the original value, \\( \\text{min}(x) \\) is the minimum value for that feature, \\( \\text{max}(x) \\) is the maximum value, and \\( \\text{min} \\) and \\( \\text{max} \\) are the new minimum and maximum values for the scaled data.\n\n### Key Points\n- **Equal Contribution**: Implementing these scaling techniques ensures that features contribute equally to the development of the model.\n- **Improved Convergence**: Feature scaling can significantly improve the convergence speed of learning algorithms.\n\nThis structured explanation outlines the importance of feature scaling and describes two commonly used techniques with their mathematical formulas.",
  "starter_code": "def feature_scaling(data: np.ndarray) -> (np.ndarray, np.ndarray):\n\t# Your code here\n\treturn standardized_data, normalized_data",
  "solution": "\nimport numpy as np\n\ndef feature_scaling(data):\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    \n    # Min-Max Normalization\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    \n    return np.round(standardized_data,4).tolist(), np.round(normalized_data,4).tolist()",
  "example": {
    "input": "data = np.array([[1, 2], [3, 4], [5, 6]])",
    "output": "([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])",
    "reasoning": "Standardization rescales the feature to have a mean of 0 and a standard deviation of 1.\n        Min-max normalization rescales the feature to a range of [0, 1], where the minimum feature value\n        maps to 0 and the maximum to 1."
  },
  "test_cases": [
    {
      "test": "print(feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])))",
      "expected_output": "([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"
    }
  ],
  "tinygrad_starter_code": "from tinygrad.tensor import Tensor\n\ndef feature_scaling_tg(data) -> tuple[Tensor, Tensor]:\n    \"\"\"\n    Standardize and Min-Max normalize input data using tinygrad.\n    Input: Tensor or convertible of shape (m,n).\n    Returns (standardized_data, normalized_data), both rounded to 4 decimals.\n    \"\"\"\n    data_t = Tensor(data).float()\n    # Your implementation here\n    pass",
  "tinygrad_solution": "import numpy as np\nfrom tinygrad.tensor import Tensor\n\ndef feature_scaling_tg(data) -> tuple[Tensor, Tensor]:\n    \"\"\"\n    Standardize and Min-Max normalize input data using tinygrad.\n    Input: Tensor or convertible of shape (m,n).\n    Returns (standardized_data, normalized_data), both rounded to 4 decimals.\n    \"\"\"\n    data_t = Tensor(data).float()\n    data_np = data_t.numpy()\n    mean = np.mean(data_np, axis=0)\n    std = np.std(data_np, axis=0)\n    standardized_np = (data_np - mean) / std\n    min_val = np.min(data_np, axis=0)\n    max_val = np.max(data_np, axis=0)\n    normalized_np = (data_np - min_val) / (max_val - min_val)\n    return Tensor(np.round(standardized_np, 4)), Tensor(np.round(normalized_np, 4))",
  "tinygrad_test_cases": [
    {
      "test": "from tinygrad.tensor import Tensor\nstd, norm = feature_scaling_tg([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\nprint(std.numpy().tolist(), norm.numpy().tolist())",
      "expected_output": "[[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]] [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]]"
    },
    {
      "test": "from tinygrad.tensor import Tensor\nstd, norm = feature_scaling_tg(Tensor([[10.0,0.0],[20.0,5.0]]))\nprint(std.numpy().tolist(), norm.numpy().tolist())",
      "expected_output": "[[-1.0, -1.0], [1.0, 1.0]] [[0.0, 0.0], [1.0, 1.0]]"
    }
  ],
  "pytorch_starter_code": "import torch\n\ndef feature_scaling(data) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Standardize and Min-Max normalize input data using PyTorch.\n    Input: Tensor or convertible of shape (m,n).\n    Returns (standardized_data, normalized_data), both rounded to 4 decimals.\n    \"\"\"\n    data_t = torch.as_tensor(data, dtype=torch.float)\n    # Your implementation here\n    pass",
  "pytorch_solution": "import torch\n\ndef feature_scaling(data) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Standardize and Min-Max normalize input data using PyTorch.\n    Input: Tensor or convertible of shape (m,n).\n    Returns (standardized_data, normalized_data), both rounded to 4 decimals.\n    \"\"\"\n    data_t = torch.as_tensor(data, dtype=torch.float)\n    mean = data_t.mean(dim=0)\n    std = data_t.std(dim=0, unbiased=False)\n    standardized = (data_t - mean) / std\n    min_val = data_t.min(dim=0).values\n    max_val = data_t.max(dim=0).values\n    normalized = (data_t - min_val) / (max_val - min_val)\n    standardized = torch.round(standardized * 10000) / 10000\n    normalized = torch.round(normalized * 10000) / 10000\n    return standardized, normalized",
  "pytorch_test_cases": [
    {
      "test": "import torch\nstd, norm = feature_scaling([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\nprint(std.numpy().tolist(), norm.numpy().tolist())",
      "expected_output": "[[-1.2246999740600586, -1.2246999740600586], [0.0, 0.0], [1.2246999740600586, 1.2246999740600586]] [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]]"
    },
    {
      "test": "import torch\nstd, norm = feature_scaling(torch.tensor([[10.0,0.0],[20.0,5.0]]))\nprint(std.numpy().tolist(), norm.numpy().tolist())",
      "expected_output": "[[-1.0, -1.0], [1.0, 1.0]] [[0.0, 0.0], [1.0, 1.0]]"
    }
  ]
}