{
  "id": "99",
  "title": "Implement the Softplus Activation Function",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "marimo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-99",
  "description": "Implement the Softplus activation function, a smooth approximation of the ReLU function. Your task is to compute the Softplus value for a given input, handling edge cases to prevent numerical overflow or underflow.",
  "learn_section": "### Understanding the Softplus Activation Function\n\nThe Softplus activation function is a smooth approximation of the ReLU function. It's used in neural networks where a smoother transition around zero is desired. Unlike ReLU which has a sharp transition at x=0, Softplus provides a more gradual change.\n\n### Mathematical Definition\n\nThe Softplus function is mathematically defined as:\n\n$$\nSoftplus(x) = \\log(1 + e^x)\n$$\n\nWhere:\n- $x$ is the input to the function\n- $e$ is Euler's number (approximately 2.71828)\n- $\\log$ is the natural logarithm\n\n### Characteristics\n\n1. **Output Range**: \n   - The output is always positive: $(0, \\infty)$\n   - Unlike ReLU, Softplus never outputs exactly zero\n\n2. **Smoothness**:\n   - Softplus is continuously differentiable\n   - The transition around x=0 is smooth, unlike ReLU's sharp \"elbow\"\n\n3. **Relationship to ReLU**:\n   - Softplus can be seen as a smooth approximation of ReLU\n   - As x becomes very negative, Softplus approaches 0\n   - As x becomes very positive, Softplus approaches x\n\n4. **Derivative**:\n   - The derivative of Softplus is the logistic sigmoid function:\n   $$\n   \\frac{d}{dx}Softplus(x) = \\frac{1}{1 + e^{-x}}\n   $$\n\n### Use Cases\n- When smooth gradients are important for optimization\n- In neural networks where a continuous approximation of ReLU is needed\n- Situations where strictly positive outputs are required with smooth transitions",
  "starter_code": "def softplus(x: float) -> float:\n\t\"\"\"\n\tCompute the softplus activation function.\n\n\tArgs:\n\t\tx: Input value\n\n\tReturns:\n\t\tThe softplus value: log(1 + e^x)\n\t\"\"\"\n\t# Your code here\n\tpass\n\t return round(val,4)",
  "solution": "import math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # To prevent overflow for large positive values\n    if x > 100:\n        return x\n    # To prevent underflow for large negative values\n    if x < -100:\n        return 0.0\n\n    return round (math.log(1.0 + math.exp(x)),4)",
  "example": {
    "input": "softplus(2)",
    "output": "2.1269",
    "reasoning": "For x = 2, the Softplus activation is calculated as $\\log(1 + e^x)$."
  },
  "test_cases": [
    {
      "test": "print(softplus(0))",
      "expected_output": "0.6931"
    },
    {
      "test": "print(softplus(100))",
      "expected_output": "100.0"
    },
    {
      "test": "print(softplus(-100))",
      "expected_output": "0.0"
    },
    {
      "test": "print(softplus(2))",
      "expected_output": "2.1269"
    },
    {
      "test": "print(softplus(-2))",
      "expected_output": "0.1269"
    }
  ]
}