{
  "id": "22",
  "title": "Sigmoid Activation Function Understanding",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "https://youtu.be/DL_PVRD-NOg",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    },
    {
      "profile_link": "https://github.com/Selbl",
      "name": "Selbl"
    }
  ],
  "tinygrad_difficulty": "easy",
  "pytorch_difficulty": "easy",
  "marimo_link": "https://open-deep-ml.github.io/deepml-notebooks/22/",
  "description": "Write a Python function that computes the output of the sigmoid activation function given an input value z. The function should return the output rounded to four decimal places.",
  "learn_section": "\n## Understanding the Sigmoid Activation Function\n\nThe sigmoid activation function is crucial in neural networks, especially for binary classification tasks. It maps any real-valued number into the interval \\( (0, 1) \\), making it useful for modeling probability as an output.\n\n### Mathematical Definition\nThe sigmoid function is mathematically defined as:\n$$\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\nwhere \\( z \\) is the input to the function.\n\n### Characteristics\n- **Output Range**: The output is always between 0 and 1.\n- **Shape**: The function has an \"S\" shaped curve.\n- **Gradient**: The gradient is highest near \\( z = 0 \\) and decreases as \\( z \\) moves away from 0 in either direction.\n\nThe sigmoid function is particularly useful for turning logits (raw prediction values) into probabilities in binary classification models.",
  "starter_code": "import math\n\ndef sigmoid(z: float) -> float:\n\t#Your code here\n\treturn result",
  "solution": "\nimport math\ndef sigmoid(z: float) -> float:\n   result = 1 / (1 + math.exp(-z))\n   return round(result, 4)",
  "example": {
    "input": "z = 0",
    "output": "0.5",
    "reasoning": "The sigmoid function is defined as Ïƒ(z) = 1 / (1 + exp(-z)). For z = 0, exp(-0) = 1, hence the output is 1 / (1 + 1) = 0.5."
  },
  "test_cases": [
    {
      "test": "print(sigmoid(0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(sigmoid(1))",
      "expected_output": "0.7311"
    },
    {
      "test": "print(sigmoid(-1))",
      "expected_output": "0.2689"
    }
  ],
  "tinygrad_starter_code": "from tinygrad.tensor import Tensor\n\ndef sigmoid_tg(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function using tinygrad.\n    Input:\n      - z: float or tinygrad Tensor scalar\n    Returns:\n      - sigmoid(z) as Python float rounded to 4 decimals.\n    \"\"\"\n    # Your implementation here\n    pass",
  "tinygrad_solution": "from tinygrad.tensor import Tensor\n\ndef sigmoid_tg(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function using tinygrad.\n    Input:\n      - z: float or tinygrad Tensor scalar\n    Returns:\n      - sigmoid(z) as Python float rounded to 4 decimals.\n    \"\"\"\n    t = Tensor(z)\n    res = (Tensor(1.0) / (Tensor(1.0) + (-t).exp())).numpy().tolist()\n    return round(res, 4)",
  "tinygrad_test_cases": [
    {
      "test": "print(sigmoid_tg(0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(sigmoid_tg(1))",
      "expected_output": "0.7311"
    }
  ],
  "pytorch_starter_code": "import torch\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function.\n    Input:\n      - z: float or torch scalar tensor\n    Returns:\n      - sigmoid(z) as Python float rounded to 4 decimals.\n    \"\"\"\n    # Your implementation here\n    pass",
  "pytorch_solution": "import torch\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Compute the sigmoid activation function.\n    Input:\n      - z: float or torch scalar tensor\n    Returns:\n      - sigmoid(z) as Python float rounded to 4 decimals.\n    \"\"\"\n    z_t = torch.as_tensor(z, dtype=torch.float)\n    res = torch.sigmoid(z_t).item()\n    return round(res, 4)",
  "pytorch_test_cases": [
    {
      "test": "print(sigmoid(0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(sigmoid(1))",
      "expected_output": "0.7311"
    }
  ]
}