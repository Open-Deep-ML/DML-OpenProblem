{
  "id": "75",
  "title": "Generate a Confusion Matrix for Binary Classification",
  "difficulty": "easy",
  "category": "Machine Learning",
  "video": "https://youtu.be/0n7dGmp0xUE",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Selbl",
      "name": "Selbl"
    },
    {
      "profile_link": "https://www.youtube.com/@StoatScript/videos",
      "name": "StoatScript"
    }
  ],
  "marimo_link": "https://open-deep-ml.github.io/DML-OpenProblem/problem-75",
  "description": "\n## Task: Generate a Confusion Matrix\n\nYour task is to implement the function `confusion_matrix(data)` that generates a confusion matrix for a binary classification problem. The confusion matrix provides a summary of the prediction results on a classification problem, allowing you to visualize how many data points were correctly or incorrectly labeled.\n\n### Input:\n- A list of lists, where each inner list represents a pair \n- `[y_true, y_pred]` for one observation. `y_true` is the actual label, and `y_pred` is the predicted label.\n\n### Output:\n- A $2 \\times 2$ confusion matrix represented as a list of lists.",
  "learn_section": "\n## Generate Confusion Matrix\n\nThe confusion matrix is a very useful tool to get a better understanding of the performance of a classification model. In it, you can visualize how many data points were labeled according to their correct categories.\n\nFor a binary classification problem of a dataset with $ n $ observations, the confusion matrix is a $ 2 \\times 2 $ matrix with the following structure:\n\n$$\nM = \\begin{pmatrix} \nTP & FN \\\\\nFP & TN\n\\end{pmatrix}\n$$\n\nWhere:\n\n- **TP**: True positives, the number of observations from the positive label that were correctly labeled as positive.\n- **FN**: False negatives, the number of observations from the positive label that were incorrectly labeled as negative.\n- **FP**: False positives, the number of observations from the negative label that were incorrectly labeled as positive.\n- **TN**: True negatives, the number of observations from the negative label that were correctly labeled as negative.\n\nA confusion matrix is a great starting point for computing more advanced metrics such as precision and recall that capture the model's performance.",
  "starter_code": "\nfrom collections import Counter\n\ndef confusion_matrix(data):\n\t# Implement the function here\n\tpass",
  "solution": "\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Count all occurrences\n    counts = Counter(tuple(pair) for pair in data)\n    # Get metrics\n    TP, FN, FP, TN = counts[(1, 1)], counts[(1, 0)], counts[(0, 1)], counts[(0, 0)]\n    # Define matrix and return\n    confusion_matrix = [[TP, FN], [FP, TN]]\n    return confusion_matrix",
  "example": {
    "input": "data = [[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]\nprint(confusion_matrix(data))",
    "output": "[[1, 1], [2, 1]]",
    "reasoning": "The confusion matrix shows the counts of true positives, false negatives, false positives, and true negatives."
  },
  "test_cases": [
    {
      "test": "\ndata = [[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]\nprint(confusion_matrix(data))\n",
      "expected_output": "[[1, 1], [2, 1]]"
    },
    {
      "test": "\ndata = [[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]\nprint(confusion_matrix(data))\n",
      "expected_output": "[[5, 5], [4, 3]]"
    },
    {
      "test": "\ndata = [[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]\nprint(confusion_matrix(data))\n",
      "expected_output": "[[0, 3], [9, 4]]"
    }
  ]
}