{
  "id": "97",
  "title": "Implement the ELU Activation Function",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "marimo_link": "https://open-deep-ml.github.io/deepml-notebooks/97/",
  "description": "Implement the ELU (Exponential Linear Unit) activation function, which helps mitigate the limitations of ReLU by providing negative outputs for negative inputs. The function should compute the ELU activation value for a given input.",
  "learn_section": "## Understanding the ELU Activation Function\n\nThe ELU (Exponential Linear Unit) activation function is an advanced activation function that addresses some limitations of ReLU by providing negative values for negative inputs, which can help prevent the \"dying ReLU\" problem and speed up learning.\n\n### Mathematical Definition\n\nThe ELU function is mathematically defined as:\n\n$$\nELU(x) = \\begin{cases} \nx & \\text{if } x > 0 \\\\\n\\alpha(e^x - 1) & \\text{otherwise}\n\\end{cases}\n$$\n\nWhere:\n- $x$ is the input to the function\n- $\\alpha$ is a hyperparameter (typically set to 1.0) that controls the value to which an ELU saturates for negative inputs\n- $e$ is the base of natural logarithms (Euler's number)\n\n### Characteristics\n\n- **Output Range:** The output is in the range $[-\\alpha, \\infty)$. For positive inputs, it behaves like the identity function, while for negative inputs, it has a smooth exponential curve that approaches -alpha.\n- **Smoothness:** Unlike ReLU, ELU is smooth everywhere, including at x = 0, which can lead to faster learning.\n- **Gradient:** The gradient is 1 for positive values and $\\alpha e^x$ for negative values, providing non-zero gradients for negative inputs.\n\n### Advantages\n\n1. Reduces the vanishing gradient problem\n2. Can produce negative outputs, allowing the function to push mean unit activations closer to zero\n3. Smoother gradient flow compared to ReLU\n4. Better handling of noise in the data due to the bounded negative part\n\n### Visual Comparison with ReLU\n\nWhile ReLU simply outputs zero for all negative inputs, ELU provides a smooth negative response:\n\n- For x > 0: Both ReLU and ELU output x\n- For x <= 0: \n  - ReLU outputs 0\n  - ELU outputs $\\alpha(e^x - 1)$, which smoothly approaches -alpha\n\nELU is particularly useful in deep neural networks where you want to maintain some of the benefits of ReLU while addressing its limitations regarding negative inputs.",
  "starter_code": "def elu(x: float, alpha: float = 1.0) -> float:\n\t\"\"\"\n\tCompute the ELU activation function.\n\n\tArgs:\n\t\tx (float): Input value\n\t\talpha (float): ELU parameter for negative values (default: 1.0)\n\n\tReturns:\n\t\tfloat: ELU activation value\n\t\"\"\"\n\t# Your code here\n\tpass\n\treturn round(val,4)",
  "solution": "import math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    return round(x if x > 0 else alpha * (math.exp(x) - 1),4)",
  "example": {
    "input": "elu(-1)",
    "output": "-0.6321",
    "reasoning": "For x = -1 and alpha = 1.0, the ELU activation is computed as $\\alpha(e^x - 1)$."
  },
  "test_cases": [
    {
      "test": "print(elu(0))",
      "expected_output": "0.0"
    },
    {
      "test": "print(elu(1))",
      "expected_output": "1.0"
    },
    {
      "test": "print(elu(-1))",
      "expected_output": "-0.6321"
    },
    {
      "test": "print(elu(-1, alpha=2.0))",
      "expected_output": "-1.2642"
    },
    {
      "test": "print(elu(5))",
      "expected_output": "5.0"
    },
    {
      "test": "print(elu(-5))",
      "expected_output": "-0.9933"
    }
  ]
}