{
  "id": "134",
  "title": "Compute Multi-class Cross-Entropy Loss",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/emharsha1812",
      "name": "Harshwardhan Fartale"
    }
  ],
  "description": "Implement a function that computes the average cross-entropy loss for a batch of predictions in a multi-class classification task. Your function should take in a batch of predicted probabilities and one-hot encoded true labels, then return the average cross-entropy loss. Ensure that you handle numerical stability by clipping probabilities by epsilon.",
  "learn_section": "## Multi-class Cross-Entropy Loss Implementation\n\nCross-entropy loss, also known as log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. For multi-class classification tasks, we use the categorical cross-entropy loss.\n\n### Mathematical Background\n\nFor a single sample with C classes, the categorical cross-entropy loss is defined as:\n\n$L = -\\sum_{c=1}^{C} y_c \\log(p_c)$\n\nwhere:\n\n- $y_c$ is a binary indicator (0 or 1) if class label c is the correct classification for the sample\n- $p_c$ is the predicted probability that the sample belongs to class c\n- $C$ is the number of classes\n\n### Implementation Requirements\n\nYour task is to implement a function that computes the average cross-entropy loss across multiple samples:\n\n$L_{batch} = -\\frac{1}{N}\\sum_{n=1}^{N}\\sum_{c=1}^{C} y_{n,c} \\log(p_{n,c})$\n\nwhere N is the number of samples in the batch.\n\n### Important Considerations\n\n- Handle numerical stability by adding a small epsilon to avoid log(0)\n- Ensure predicted probabilities sum to 1 for each sample\n- Return average loss across all samples\n- Handle invalid inputs appropriately\n\nThe function should take predicted probabilities and true labels as input and return the average cross-entropy loss.",
  "starter_code": "import numpy as np\n\ndef compute_cross_entropy_loss(predicted_probs: np.ndarray, true_labels: np.ndarray, epsilon = 1e-15) -> float:\n    # Your code here\n    pass",
  "solution": "import numpy as np\n\ndef compute_cross_entropy_loss(predicted_probs: np.ndarray, true_labels: np.ndarray,epsilon = 1e-15) -> float:\n\n    predicted_probs = np.clip(predicted_probs, epsilon, 1 - epsilon)\n\n    #Write your code here\n    log_probs = np.log(predicted_probs)\n    loss = -np.sum(true_labels * log_probs, axis=1)\n    return float(np.mean(loss))",
  "example": {
    "input": "predicted_probs = [[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]]\ntrue_labels = [[1, 0, 0], [0, 1, 0]]",
    "output": "0.4338",
    "reasoning": "The predicted probabilities for the correct classes are 0.7 and 0.6. The cross-entropy is computed as -mean(log(0.7), log(0.6)), resulting in approximately 0.4463."
  },
  "test_cases": [
    {
      "test": "import numpy as np\npred = np.array([[1, 0, 0], [0, 1, 0]])\ntrue = np.array([[1, 0, 0], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np\npred = np.array([[0.1, 0.8, 0.1], [0.8, 0.1, 0.1]])\ntrue = np.array([[0, 0, 1], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))",
      "expected_output": "2.3026"
    },
    {
      "test": "import numpy as np\npred = np.array([[0.7, 0.2, 0.1], [0.3, 0.6, 0.1]])\ntrue = np.array([[1, 0, 0], [0, 1, 0]])\nprint(round(compute_cross_entropy_loss(pred, true), 4))",
      "expected_output": "0.4338"
    }
  ]
}