{
  "id": "152",
  "title": "Implementing ROUGE Score",
  "difficulty": "medium",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/kartik-git",
      "name": "kartik-git"
    }
  ],
  "description": "## Problem\n\nImplement the ROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation) score to evaluate the quality of a generated summary by comparing it to a reference summary. ROUGE-1 focuses on unigram (single word) overlaps between the candidate and reference texts.\n\nYour task is to write a function that computes the ROUGE-1 recall, precision, and F1 score based on the number of overlapping unigrams.",
  "learn_section": "# ROUGE-1 Score Learning Guide\n\n## Solution Explanation\n\nROUGE-1 (Recall-Oriented Understudy for Gisting Evaluation) is a fundamental metric for evaluating the quality of automatically generated summaries by comparing them to reference summaries. The \"1\" in ROUGE-1 refers to unigrams (single words), making it the most basic but widely used variant of ROUGE metrics.\n\n### Intuition\n\nImagine you're a teacher grading a student's book summary. You have a reference summary (the \"gold standard\") and want to measure how well the student's summary captures the key information. ROUGE-1 essentially counts how many important words from the reference summary appear in the student's summary.\n\nThe core idea is simple: **if a generated summary contains many of the same words as a high-quality reference summary, it's likely capturing similar content and therefore of good quality.**\n\n### Mathematical Foundation\n\nROUGE-1 is built on three fundamental components:\n\n**1. Precision (P)**\n$$P = \\frac{\\text{Number of overlapping unigrams}}{\\text{Total unigrams in generated summary}}$$\n\n**2. Recall (R)**\n$$R = \\frac{\\text{Number of overlapping unigrams}}{\\text{Total unigrams in reference summary}}$$\n\n**3. F1-Score (F)**\n$$F = \\frac{2 \\times P \\times R}{P + R}$$\n\nWhere an \"overlapping unigram\" is a word that appears in both the generated summary and the reference summary.\n\n### Step-by-Step Calculation Process\n\nLet's work through a concrete example:\n\n**Reference Summary:** \"The quick brown fox jumps over the lazy dog\"\n**Generated Summary:** \"A quick fox jumps over a lazy cat\"\n\n**Step 1: Tokenization**\n- Reference tokens: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n- Generated tokens: [\"A\", \"quick\", \"fox\", \"jumps\", \"over\", \"a\", \"lazy\", \"cat\"]\n\n**Step 2: Identify Overlapping Unigrams**\nOverlapping words (case-insensitive): [\"quick\", \"fox\", \"jumps\", \"over\", \"lazy\"]\n- Count of overlapping unigrams: 5\n\n**Step 3: Calculate Precision**\n$$P = \\frac{5}{8} = 0.625$$\n*Interpretation: 62.5% of words in the generated summary appear in the reference*\n\n**Step 4: Calculate Recall**\n$$R = \\frac{5}{9} = 0.556$$\n*Interpretation: 55.6% of words in the reference summary are captured in the generated summary*\n\n**Step 5: Calculate F1-Score**\n$$F = \\frac{2 \\times 0.625 \\times 0.556}{0.625 + 0.556} = \\frac{0.695}{1.181} = 0.588$$\n\n### Understanding the Components\n\n**Precision answers:** \"Of all the words in my generated summary, how many are actually relevant (appear in the reference)?\"\n- High precision means the generated summary doesn't contain many irrelevant words\n- Low precision suggests the summary is verbose or off-topic\n\n**Recall answers:** \"Of all the important words in the reference, how many did my generated summary capture?\"\n- High recall means the generated summary covers most key information\n- Low recall suggests the summary misses important content\n\n**F1-Score provides:** A balanced measure that penalizes both missing important information (low recall) and including irrelevant information (low precision)\n\n### Advanced Considerations\n\n**Preprocessing Steps:**\n1. **Case normalization:** Convert all text to lowercase\n2. **Tokenization:** Split text into individual words\n3. **Stop word handling:** Optionally remove common words like \"the\", \"and\", \"is\"\n4. **Stemming/Lemmatization:** Optionally reduce words to their root forms\n\n**Mathematical Variants:**\n- **ROUGE-1 Precision:** $P = \\frac{\\sum_{i} \\text{Count}_{\\text{match}}(unigram_i)}{\\sum_{i} \\text{Count}(unigram_i)}$\n- **ROUGE-1 Recall:** $R = \\frac{\\sum_{i} \\text{Count}_{\\text{match}}(unigram_i)}{\\sum_{i} \\text{Count}_{\\text{ref}}(unigram_i)}$\n\nWhere $\\text{Count}_{\\text{match}}(unigram_i)$ is the minimum of the counts of $unigram_i$ in the generated and reference summaries.\n\n### Practical Implementation Insights\n\n**Handling Multiple References:**\nWhen multiple reference summaries exist, ROUGE-1 can be calculated against each reference separately, then the maximum score is typically taken:\n\n$$\\text{ROUGE-1} = \\max_{j} \\text{ROUGE-1}(\\text{generated}, \\text{reference}_j)$$\n\n**Limitations to Consider:**\n- **Word order independence:** ROUGE-1 ignores sentence structure and word order\n- **Semantic blindness:** Synonyms and paraphrases aren't recognized\n- **Length bias:** Longer summaries may achieve higher recall simply by including more words\n\n### Real-World Applications\n\nROUGE-1 is extensively used in:\n- **Automatic summarization evaluation** (news articles, scientific papers)\n- **Machine translation quality assessment** (as a secondary metric)\n- **Question answering systems** (evaluating answer quality)\n- **Chatbot response evaluation** (measuring relevance to expected responses)",
  "starter_code": "# Implement your function below.\n\ndef rouge_1_score(reference: str, candidate: str) -> dict:\n    \"\"\"\n    Compute ROUGE-1 score between reference and candidate texts.\n    \n    Returns a dictionary with precision, recall, and f1.\n    \"\"\"\n    # Your code here\n    pass",
  "solution": "from collections import Counter\n\ndef rouge_1_score(reference: str, candidate: str) -> dict:\n    \"\"\"\n    Compute ROUGE-1 score between reference and candidate texts.\n    \n    Returns a dictionary with precision, recall, and f1.\n    \"\"\"\n    ref_tokens = reference.lower().split()\n    cand_tokens = candidate.lower().split()\n\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n\n    # Count overlapping unigrams\n    overlap = sum(min(ref_counter[w], cand_counter[w]) for w in cand_counter)\n\n    precision = overlap / len(cand_tokens) if cand_tokens else 0.0\n    recall = overlap / len(ref_tokens) if ref_tokens else 0.0\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0\n\n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}",
  "example": {
    "input": "rouge_1_score('the cat sat on the mat', 'the cat is on the mat')",
    "output": "{'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1': 0.8333333333333334}",
    "reasoning": "The reference text 'the cat sat on the mat' has 6 tokens, and the candidate text 'the cat is on the mat' has 6 tokens. The overlapping words are: 'the' (appears 2 times in reference, 2 times in candidate, so min(2,2)=2 overlap), 'cat' (1,1 → 1 overlap), 'on' (1,1 → 1 overlap), and 'mat' (1,1 → 1 overlap). Total overlap = 2+1+1+1 = 5. Precision = 5/6 ≈ 0.833 (5 overlapping words out of 6 candidate words). Recall = 5/6 ≈ 0.833 (5 overlapping words out of 6 reference words). F1 = 2×(0.833×0.833)/(0.833+0.833) = 0.833 since precision equals recall."
  },
  "test_cases": [
    {
      "test": "print(rouge_1_score('the cat sat on the mat', 'the cat is on the mat'))",
      "expected_output": "{'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1': 0.8333333333333334}"
    },
    {
      "test": "print(rouge_1_score('hello there', 'hello there'))",
      "expected_output": "{'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
    }
  ]
}