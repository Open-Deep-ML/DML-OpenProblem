{
  "id": "54",
  "title": "Implementing a Simple RNN",
  "difficulty": "medium",
  "category": "Deep Learning",
  "video": null,
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "Write a Python function that implements a simple Recurrent Neural Network (RNN) cell. The function should process a sequence of input vectors and produce the final hidden state. Use the tanh activation function for the hidden state updates. The function should take as inputs the sequence of input vectors, the initial hidden state, the weight matrices for input-to-hidden and hidden-to-hidden connections, and the bias vector. The function should return the final hidden state after processing the entire sequence, rounded to four decimal places.",
  "learn_section": "\n## Understanding Recurrent Neural Networks (RNNs)\n\nRecurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data by maintaining a hidden state that captures information from previous inputs.\n\n### Mathematical Formulation\n\nFor each time step $t$, the RNN updates its hidden state $h_t$ using the current input $x_t$ and the previous hidden state $h_{t-1}$:\n\n$$\nh_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n$$\n\nWhere:\n1. $W_x$ is the weight matrix for the input-to-hidden connections.\n2. $W_h$ is the weight matrix for the hidden-to-hidden connections.\n3. $b$ is the bias vector.\n4. $\\tanh$ is the hyperbolic tangent activation function applied element-wise.\n\n### Implementation Steps\n\n1. **Initialization**: Start with the initial hidden state $h_0$.\n\n2. **Sequence Processing**: For each input $x_t$ in the sequence:\n\n   $$\n   h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n   $$\n\n3. **Final Output**: After processing all inputs, the final hidden state $h_T$ (where $T$ is the length of the sequence) contains information from the entire sequence.\n\n### Example Calculation\n\nGiven:\n1. Inputs: $x_1 = 1.0$, $x_2 = 2.0$, $x_3 = 3.0$\n2. Initial hidden state: $h_0 = 0.0$\n3. Weights:\n   - $W_x = 0.5$\n   - $W_h = 0.8$\n4. Bias: $b = 0.0$\n\n**Compute**:\n\n1. First time step ($t = 1$):\n\n   $$\n   h_1 = \\tanh(0.5 \\times 1.0 + 0.8 \\times 0.0 + 0.0) = \\tanh(0.5) \\approx 0.4621\n   $$\n\n2. Second time step ($t = 2$):\n\n   $$\n   h_2 = \\tanh(0.5 \\times 2.0 + 0.8 \\times 0.4621 + 0.0) = \\tanh(1.0 + 0.3697) = \\tanh(1.3697) \\approx 0.8781\n   $$\n\n3. Third time step ($t = 3$):\n\n   $$\n   h_3 = \\tanh(0.5 \\times 3.0 + 0.8 \\times 0.8781 + 0.0) = \\tanh(1.5 + 0.7025) = \\tanh(2.2025) \\approx 0.9750\n   $$\n\nThe final hidden state $h_3$ is approximately 0.9750.\n\n### Applications\n\nRNNs are widely used in natural language processing, time-series prediction, and any task involving sequential data.",
  "starter_code": "import numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n\t# Your code here\n\treturn final_hidden_state",
  "solution": "\nimport numpy as np\n\ndef rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b):\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    final_hidden_state = np.round(h, 4)\n    return final_hidden_state.tolist()",
  "example": {
    "input": "input_sequence = [[1.0], [2.0], [3.0]]\n    initial_hidden_state = [0.0]\n    Wx = [[0.5]]  # Input to hidden weights\n    Wh = [[0.8]]  # Hidden to hidden weights\n    b = [0.0]     # Bias",
    "output": "final_hidden_state = [0.9993]",
    "reasoning": "The RNN processes each input in the sequence, updating the hidden state at each step using the tanh activation function."
  },
  "test_cases": [
    {
      "test": "print(rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]))",
      "expected_output": "[0.9759]"
    },
    {
      "test": "print(rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]))",
      "expected_output": "[0.118]"
    },
    {
      "test": "print(rnn_forward(\n    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],\n    [0.0, 0.0],\n    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],\n    [[0.7, 0.8], [0.9, 1.0]],\n    [0.1, 0.2]\n))",
      "expected_output": "[0.7474, 0.9302]"
    }
  ]
}