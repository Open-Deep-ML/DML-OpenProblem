{
  "id": "115",
  "title": "Implement Batch Normalization for BCHW Input",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/nzomi",
      "name": "nzomi"
    }
  ],
  "description": "Implement a function that performs Batch Normalization on a 4D NumPy array representing a batch of feature maps in the BCHW format (batch, channels, height, width). The function should normalize the input across the batch and spatial dimensions for each channel, then apply scale (gamma) and shift (beta) parameters. Use the provided epsilon value to ensure numerical stability.",
  "learn_section": "## Understanding Batch Normalization\n\nBatch Normalization (BN) is a widely used technique that helps to accelerate the training of deep neural networks and improve model performance. By normalizing the inputs to each layer so that they have a mean of zero and a variance of one, BN stabilizes the learning process, speeds up convergence, and introduces regularization, which can reduce the need for other forms of regularization like dropout.\n\n### Concepts\n\nBatch Normalization operates on the principle of reducing **internal covariate shift**, which occurs when the distribution of inputs to a layer changes during training as the model weights get updated. This can slow down training and make hyperparameter tuning more challenging. By normalizing the inputs, BN reduces this problem, allowing the model to train faster and more reliably.\n\nThe process of Batch Normalization consists of the following steps:\n\n1. **Compute the Mean and Variance:** For each mini-batch, compute the mean and variance of the activations for each feature (dimension).\n2. **Normalize the Inputs:** Normalize the activations using the computed mean and variance.\n3. **Apply Scale and Shift:** After normalization, apply a learned scale (gamma) and shift (beta) to restore the model's ability to represent the data's original distribution.\n4. **Training and Inference:** During training, the mean and variance are computed from the current mini-batch. During inference, a running average of the statistics from the training phase is used.\n\n### Structure of Batch Normalization for BCHW Input\n\nFor an input tensor with the shape **BCHW**, where:\n- **B**: batch size,\n- **C**: number of channels,\n- **H**: height,\n- **W**: width,\nthe Batch Normalization process operates on specific dimensions based on the task's requirement.\n\n#### 1. Mean and Variance Calculation\n\n- In **Batch Normalization**, we typically normalize the activations **across the batch** and **over the spatial dimensions (height and width)** for each **channel**. This means we calculate the mean and variance **per channel** (C) for the **batch and spatial dimensions** (H, W).\n\nFor each channel $c$, we compute the **mean** $\\mu_c$ and **variance** $\\sigma_c^2$ over the mini-batch and spatial dimensions:\n\n$$\n\\mu_c = \\frac{1}{B \\cdot H \\cdot W} \\sum_{i=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{i,c,h,w}\n$$\n\n$$\n\\sigma_c^2 = \\frac{1}{B \\cdot H \\cdot W} \\sum_{i=1}^{B} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{i,c,h,w} - \\mu_c)^2\n$$\n\nWhere:\n- $x_{i,c,h,w}$ is the input activation at batch index $i$, channel $c$, height $h$, and width $w$.\n- $B$ is the batch size.\n- $H$ and $W$ are the spatial dimensions (height and width).\n- $C$ is the number of channels.\n\nThe mean and variance are computed **over all spatial positions (H, W)** and **across all samples in the batch (B)** for each **channel (C)**.\n\n#### 2. Normalization\n\nOnce the mean $\\mu_c$ and variance $\\sigma_c^2$ have been computed for each channel, the next step is to **normalize** the input. The normalization is done by subtracting the mean and dividing by the standard deviation (plus a small constant $\\epsilon$ for numerical stability):\n\n$$\n\\hat{x}_{i,c,h,w} = \\frac{x_{i,c,h,w} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}\n$$\n\nWhere:\n- $\\hat{x}_{i,c,h,w}$ is the normalized activation for the input at batch index $i$, channel $c$, height $h$, and width $w$.\n- $\\epsilon$ is a small constant to avoid division by zero (for numerical stability).\n\n#### 3. Scale and Shift\n\nAfter normalization, the next step is to apply a **scale** ($\\gamma_c$) and **shift** ($\\beta_c$) to the normalized activations for each channel. These learned parameters allow the model to adjust the output distribution of each feature, preserving the flexibility of the original activations.\n\n$$\ny_{i,c,h,w} = \\gamma_c \\hat{x}_{i,c,h,w} + \\beta_c\n$$\n\nWhere:\n- $\\gamma_c$ is the scaling factor for channel $c$.\n- $\\beta_c$ is the shifting factor for channel $c$.\n\n#### 4. Training and Inference\n\n- **During Training**: The mean and variance are computed for each mini-batch and used for normalization across the batch and spatial dimensions for each channel.\n- **During Inference**: The model uses a running average of the statistics (mean and variance) that were computed during training to ensure consistent behavior when deployed.\n\n### Key Points\n\n- **Normalization Across Batch and Spatial Dimensions**: In Batch Normalization for **BCHW** input, the normalization is done **across the batch (B) and spatial dimensions (H, W)** for each **channel (C)**. This ensures that each feature channel has zero mean and unit variance, making the training process more stable.\n\n- **Channel-wise Normalization**: Batch Normalization normalizes the activations independently for each **channel (C)** because different channels in convolutional layers often have different distributions and should be treated separately.\n\n- **Numerical Stability**: The small constant $\\epsilon$ is added to the variance to avoid numerical instability when dividing by the square root of variance, especially when the variance is very small.\n\n- **Improved Gradient Flow**: By reducing internal covariate shift, Batch Normalization allows the gradients to flow more easily during backpropagation, helping the model train faster and converge more reliably.\n\n- **Regularization Effect**: Batch Normalization introduces noise into the training process because it relies on the statistics of a mini-batch. This noise acts as a form of regularization, which can prevent overfitting and improve generalization.\n\n### Why Normalize Over Batch and Spatial Dimensions?\n\n- **Across Batch**: Normalizing across the batch helps to stabilize the input distribution across all samples in a mini-batch. This allows the model to avoid the problem of large fluctuations in the input distribution as weights are updated.\n\n- **Across Spatial Dimensions**: In convolutional networks, the spatial dimensions (height and width) are highly correlated, and normalizing over these dimensions ensures that the activations are distributed consistently throughout the spatial field, helping to maintain a stable learning process.\n\n- **Channel-wise Normalization**: Each channel can have its own distribution of values, and normalization per channel ensures that each feature map is scaled and shifted independently, allowing the model to learn representations that are not overly sensitive to specific channels' scaling.\n\nBy normalizing across the batch and spatial dimensions and applying a per-channel transformation, Batch Normalization helps reduce internal covariate shift and speeds up training, leading to faster convergence and better overall model performance.",
  "starter_code": "import numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n\t# Your code here\n\tpass",
  "solution": "import numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    # Compute mean and variance across the batch and spatial dimensions\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)  # Mean over (B, H, W)\n    variance = np.var(X, axis=(0, 2, 3), keepdims=True)  # Variance over (B, H, W)\n    # Normalize\n    X_norm = (X - mean) / np.sqrt(variance + epsilon)\n    # Scale and shift\n    norm_X = gamma * X_norm + beta\n    return norm_X",
  "example": {
    "input": "B, C, H, W = 2, 2, 2, 2; np.random.seed(42); X = np.random.randn(B, C, H, W); gamma = np.ones(C).reshape(1, C, 1, 1); beta = np.zeros(C).reshape(1, C, 1, 1)",
    "output": "[[[[ 0.42859934, -0.51776438], [ 0.65360963,  1.95820707]], [[ 0.02353721,  0.02355215], [ 1.67355207,  0.93490043]]], [[[-1.01139563,  0.49692747], [-1.00236882, -1.00581468]], [[ 0.45676349, -1.50433085], [-1.33293647, -0.27503802]]]]",
    "reasoning": "The input X is a 2x2x2x2 array. For each channel, compute the mean and variance across the batch (B), height (H), and width (W) dimensions. Normalize X using (X - mean) / sqrt(variance + epsilon), then scale by gamma and shift by beta. The output matches the expected normalized values."
  },
  "test_cases": [
    {
      "test": "B, C, H, W = 2, 2, 2, 2\nnp.random.seed(42)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1)\nbeta = np.zeros(C).reshape(1, C, 1, 1)\nactual_output = batch_normalization(X, gamma, beta)\nexpected_output = np.array([[[[ 0.42859934, -0.51776438], [ 0.65360963,  1.95820707]], [[ 0.02353721,  0.02355215], [ 1.67355207,  0.93490043]]], [[[-1.01139563,  0.49692747], [-1.00236882, -1.00581468]], [[ 0.45676349, -1.50433085], [-1.33293647, -0.27503802]]]])\nprint(actual_output)",
      "expected_output": "[[[[ 0.42859934, -0.51776438], [ 0.65360963,1.95820707]],[[ 0.02353721,  0.02355215], [ 1.67355207,  0.93490043]]], [[[-1.01139563,  0.49692747], [-1.00236882, -1.00581468]], [[ 0.45676349, -1.50433085], [-1.33293647, -0.27503802]]]]"
    },
    {
      "test": "B, C, H, W = 2, 2, 2, 2\nnp.random.seed(101)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1)\nbeta = np.zeros(C).reshape(1, C, 1, 1)\nactual_output = batch_normalization(X, gamma, beta)\nexpected_output = np.array([[[[ 1.81773164,  0.16104096], [ 0.38406453,  0.06197112]], [[ 1.00432932, -0.37139956], [-1.12098938,  0.94031919]]], [[[-1.94800122,  0.25029395], [ 0.08188579, -0.80898678]], [[ 0.34878049, -0.99452891], [-1.24171594,  1.43520478]]]])\nprint(actual_output)",
      "expected_output": "[[[[ 1.81773164,0.16104096], [ 0.38406453,0.06197112]],[[ 1.00432932, -0.37139956], [-1.12098938,  0.94031919]]], [[[-1.94800122,  0.25029395], [ 0.08188579, -0.80898678]], [[ 0.34878049, -0.99452891], [-1.24171594,  1.43520478]]]]"
    },
    {
      "test": "B, C, H, W = 2, 2, 2, 2\nnp.random.seed(101)\nX = np.random.randn(B, C, H, W)\ngamma = np.ones(C).reshape(1, C, 1, 1) * 0.5\nbeta = np.ones(C).reshape(1, C, 1, 1)\nactual_output = batch_normalization(X, gamma, beta)\nexpected_output = np.array([[[[1.90886582, 1.08052048], [1.19203227, 1.03098556]], [[1.50216466, 0.81430022], [0.43950531, 1.4701596 ]]], [[[0.02599939, 1.12514697], [1.04094289, 0.59550661]], [[1.17439025, 0.50273554], [0.37914203, 1.71760239]]]])\nprint(actual_output)",
      "expected_output": "[[[[1.90886582, 1.08052048], [1.19203227, 1.03098556]], [[1.50216466, 0.81430022], [0.43950531, 1.4701596 ]]], [[[0.02599939, 1.12514697], [1.04094289, 0.59550661]], [[1.17439025, 0.50273554], [0.37914203, 1.71760239]]]]"
    }
  ]
}