{
  "id": "130",
  "title": "Implement a Simple CNN Training Function with Backpropagation",
  "difficulty": "hard",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "moe"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "Create a function that trains a basic Convolutional Neural Network (CNN) using backpropagation. The network should include one convolutional layer with ReLU activation, followed by flattening and a dense layer with softmax output, and a cross entropy loss. You need to handle the forward pass, compute the loss gradients, and update the weights and biases using stochastic gradient descent. Ensure the function processes input data as grayscale images and one-hot encoded labels, and returns the trained weights and biases for the convolutional and dense layers.",
  "learn_section": "## Understanding a Simple Convolutional Neural Network with Backpropagation\n\nA **Convolutional Neural Network** (CNN) learns two things at once:  \n1. **What to look for** - small filters (kernels) that detect edges, textures, etc.  \n2. **How to combine those detections** - a dense layer that converts them into class probabilities.\n\nBelow is the full training loop broken into intuitive steps that can be implemented directly in NumPy.\n\n---\n\n### 1. Forward Pass\n\n**Convolution**  \nThe convolution layer slides a small filter over the input and produces feature maps:\n\n$$\nZ^c[p, q, k] = \\sum_{i, j} X[p+i, q+j] \\cdot W^c[i, j, k] + b^c[k]\n$$\n\nThis results in a tensor of shape $(H - k + 1, W - k + 1, F)$, where $H$ and $W$ are the input height and width, $k$ is the kernel size, and $F$ is the number of filters.\n\n**ReLU Activation**\n\n$$\nA^c = \\max(0, Z^c)\n$$\n\nThis introduces non-linearity by zeroing out negative values.\n\n**Flattening**\n\nThe feature maps are reshaped into a vector:\n\n$$\nA^f = \\text{flatten}(A^c)\n$$\n\n**Dense Layer**\n\n$$\nZ^d = A^f \\cdot W^d + b^d\n$$\n\nEach entry in $A^f$ contributes to every output class via weight matrix $W^d$ and bias $b^d$.\n\n**Softmax Activation**\n\n$$\n\\hat{y}_c = \\frac{e^{Z^d_c}}{\\sum_j e^{Z^d_j}}\n$$\n\nThis converts raw scores into probabilities for classification.\n\n---\n\n### 2. Loss Function â€“ Cross Entropy\n\nFor one-hot encoded label $y$ and prediction $\\hat{y}$:\n\n$$\n\\mathcal{L}(\\hat{y}, y) = -\\sum_c y_c \\log(\\hat{y}_c)\n$$\n\nThis penalizes incorrect predictions based on confidence.\n\n---\n\n### 3. Backward Pass\n\n**Gradient of Softmax + Cross Entropy**\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial Z^d} = \\hat{y} - y\n$$\n\n**Dense Layer Gradients**\n\n$\\frac{\\partial \\mathcal{L}}{\\partial W^d} = (A^f)^T \\cdot \\frac{\\partial \\mathcal{L}}{\\partial Z^d}$,\nand the gradient with respect to biases is\n$\\frac{\\partial \\mathcal{L}}{\\partial b^d} = \\frac{\\partial \\mathcal{L}}{\\partial Z^d}$.\n\nReshape the upstream gradient to the shape of $A^c$ for backpropagation through ReLU.\n\n**ReLU Gradient**\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial Z^c} = \\frac{\\partial \\mathcal{L}}{\\partial A^c} \\cdot \\mathbf{1}(Z^c > 0)\n$$\n\n**Convolution Filter Gradients**\n\nFor each filter $k$:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W^c_{i,j,k}} = \\sum_{p,q} \\frac{\\partial \\mathcal{L}}{\\partial Z^c_{p,q,k}} \\cdot X_{p+i, q+j}\n$$\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b^c_k} = \\sum_{p,q} \\frac{\\partial \\mathcal{L}}{\\partial Z^c_{p,q,k}}\n$$\n\n---\n\n### 4. Updating Parameters (SGD)\n\nWith learning rate $\\eta$:\n\n$$\nW \\leftarrow W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n$$\n\n$$\nb \\leftarrow b - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n$$\n\nRepeat this process for each sample (stochastic gradient descent) and for multiple epochs.\n\n---\n\n### 5. Example Walkthrough\n\nSuppose $X$ is a grayscale image:\n\n$$\nX = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n$$\n\nAnd the kernel is:\n\n$$\nK = \\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix}\n$$\n\nPerform convolution at the top-left:\n\n$$\n(1 \\cdot 1 + 2 \\cdot 0 + 4 \\cdot 0 + 5 \\cdot (-1)) = 1 - 5 = -4\n$$\n\nAfter ReLU: max(0, -4) = 0  \nFlatten the result -> Dense layer -> Softmax output -> Compute loss\n\nBackpropagate the error to adjust weights, and repeat to learn better filters and classifications over time.",
  "starter_code": "def train_simple_cnn_with_backprop(X, y, epochs, learning_rate, kernel_size=3, num_filters=1):\n    # Your code here\n    pass",
  "solution": "import numpy as np\n\ndef train_simple_cnn_with_backprop(X, y, epochs, learning_rate, kernel_size=3, num_filters=1):\n    '''\n    Trains a simple CNN with one convolutional layer, ReLU activation, flattening, and a dense layer with softmax output using backpropagation.\n\n    Assumes X has shape (n_samples, height, width) for grayscale images and y is one-hot encoded with shape (n_samples, num_classes).\n\n    Parameters:\n    X : np.ndarray, input data\n    y : np.ndarray, one-hot encoded labels\n    epochs : int, number of training epochs\n    learning_rate : float, learning rate for weight updates\n    kernel_size : int, size of the square convolutional kernel\n    num_filters : int, number of filters in the convolutional layer\n\n    Returns:\n    W_conv, b_conv, W_dense, b_dense : Trained weights and biases for the convolutional and dense layers\n    '''\n    n_samples, height, width = X.shape\n    num_classes = y.shape[1]\n\n    # Initialize weights and biases\n    W_conv = np.random.randn(kernel_size, kernel_size, num_filters) * 0.01\n    b_conv = np.zeros(num_filters)\n    output_height = height - kernel_size + 1\n    output_width = width - kernel_size + 1\n    flattened_size = output_height * output_width * num_filters\n    W_dense = np.random.randn(flattened_size, num_classes) * 0.01\n    b_dense = np.zeros(num_classes)\n\n    for epoch in range(epochs):\n        for i in range(n_samples):  # Stochastic Gradient Descent with batch size 1\n            # Forward pass\n            # Convolutional layer\n            Z_conv = np.zeros((output_height, output_width, num_filters))\n            for k in range(num_filters):\n                for p in range(output_height):\n                    for q in range(output_width):\n                        Z_conv[p, q, k] = np.sum(X[i, p:p+kernel_size, q:q+kernel_size] * W_conv[:, :, k]) + b_conv[k]\n            A_conv = np.maximum(Z_conv, 0)  # ReLU activation\n            A_flat = A_conv.flatten()  # Flatten the output\n\n            # Dense layer\n            Z_dense = np.dot(A_flat, W_dense) + b_dense\n            exp_Z_dense = np.exp(Z_dense - np.max(Z_dense))  # Numerical stability for softmax\n            A_dense = exp_Z_dense / np.sum(exp_Z_dense)\n\n            # Backpropagation\n            # Loss gradient for cross-entropy with softmax\n            dZ_dense = A_dense - y[i]\n\n            # Dense layer gradients\n            dW_dense = np.outer(A_flat, dZ_dense)\n            db_dense = dZ_dense\n            dA_flat = np.dot(dZ_dense, W_dense.T)\n\n            # Reshape and backprop through ReLU\n            dA_conv = dA_flat.reshape(A_conv.shape)\n            dZ_conv = dA_conv * (A_conv > 0).astype(float)\n\n            # Convolutional layer gradients\n            dW_conv = np.zeros_like(W_conv)\n            db_conv = np.zeros(num_filters)\n            for k in range(num_filters):\n                db_conv[k] = np.sum(dZ_conv[:, :, k])\n                for ii in range(kernel_size):\n                    for jj in range(kernel_size):\n                   \n                        dW_conv[ii, jj, k] = np.sum(dZ_conv[:, :, k] * X[i, ii:ii+output_height, jj:jj+output_width])\n\n            \n# Update weights and biases\n            W_conv -= learning_rate * dW_conv\n            b_conv -= learning_rate * db_conv\n            W_dense -= learning_rate * dW_dense\n            b_dense -= learning_rate * db_dense\n\n    return W_conv, b_conv, W_dense, b_dense",
  "example": {
    "input": "import numpy as np; np.random.seed(42); X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]); y = np.array([[1, 0]]); print(train_simple_cnn_with_backprop(X, y, 1, 0.01, 3, 1))",
    "output": "(array([[[ 0.00501739],        [-0.00128214],        [ 0.00662764]],       [[ 0.01543131],        [-0.00209028],        [-0.00203986]],       [[ 0.01614389],        [ 0.00807636],        [-0.00424248]]]), array([5.02517066e-05]), array([[ 0.00635715, -0.00556573]]), array([ 0.00499531, -0.00499531]))",
    "reasoning": "The solution processes the input X through a forward pass, where it applies a convolutional layer with ReLU activation, flattens the output, and passes it through a dense layer with softmax to compute predictions and loss based on the one-hot encoded label y. In the backward pass, it calculates gradients using backpropagation for the weights and biases, then updates them using stochastic gradient descent with the specified learning rate, and returns the updated weights after one epoch."
  },
  "test_cases": [
    {
      "test": "import numpy as np; np.random.seed(42); X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]); y = np.array([[1, 0]]); print(train_simple_cnn_with_backprop(X, y, 1, 0.01, 3, 1))",
      "expected_output": "(array([[[ 0.00501739],        [-0.00128214],        [ 0.00662764]],       [[ 0.01543131],        [-0.00209028],        [-0.00203986]],       [[ 0.01614389],        [ 0.00807636],        [-0.00424248]]]), array([5.02517066e-05]), array([[ 0.00635715, -0.00556573]]), array([ 0.00499531, -0.00499531]))"
    },
    {
      "test": "import numpy as np; np.random.seed(42); X = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[9, 8, 7], [6, 5, 4], [3, 2, 1]]]); y = np.array([[0, 1], [1, 0]]); print(train_simple_cnn_with_backprop(X, y, 2, 0.01, 3, 1))",
      "expected_output": "(array([[[ 0.00561561],        [-0.00091884],        [ 0.00675603]],       [[ 0.01532478],        [-0.00243171],        [-0.00261621]],       [[ 0.01533262],        [ 0.00703018],        [-0.00552357]]]), array([-1.80360153e-05]), array([[ 0.00561137, -0.00481995]]), array([ 3.23124658e-05, -3.23124658e-05]))"
    },
    {
      "test": "import numpy as np; np.random.seed(42); X = np.array([[[2, 3, 4], [5, 6, 7], [8, 9, 10]]]); y = np.array([[0.5, 0.5]]); print(train_simple_cnn_with_backprop(X, y, 1, 0.005, 3, 1))",
      "expected_output": "(array([[[ 0.00496708],        [-0.00138273],        [ 0.00647677]],       [[ 0.01523016],        [-0.00234171],        [-0.00234157]],       [[ 0.0157919 ],        [ 0.00767409],        [-0.00469503]]]), array([-2.85717013e-08]), array([[ 0.00542496, -0.00463354]]), array([-2.84019221e-06,  2.84019221e-06]))"
    }
  ]
}