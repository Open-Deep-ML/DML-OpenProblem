{
  "id": "59",
  "title": "Implement Long Short-Term Memory (LSTM) Network",
  "difficulty": "medium",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Hui-cd",
      "name": "Hui"
    }
  ],
  "description": "## Task: Implement Long Short-Term Memory (LSTM) Network\n\nYour task is to implement an LSTM network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs.\n\nWrite a class `LSTM` with the following methods:\n\n- `__init__(self, input_size, hidden_size)`: Initializes the LSTM with random weights and zero biases.\n- `forward(self, x, initial_hidden_state, initial_cell_state)`: Processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\n\nThe LSTM should compute the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state.\n\n    ",
  "learn_section": "\n## Understanding Long Short-Term Memory Networks (LSTMs)\n\nLong Short-Term Memory Networks are a special type of RNN designed to capture long-term dependencies in sequential data by using a more complex hidden state structure.\n\n### LSTM Gates and Their Functions\n\nFor each time step $t$, the LSTM updates its cell state $c_t$ and hidden state $h_t$ using the current input $x_t$, the previous cell state $c_{t-1}$, and the previous hidden state $h_{t-1}$. The LSTM architecture consists of several gates that control the flow of information:\n\n#### Forget Gate $f_t$:\n\nThis gate decides what information to discard from the cell state. It looks at the previous hidden state $h_{t-1}$ and the current input $x_t$, and outputs a number between 0 and 1 for each number in the cell state. A 1 represents \"keep this\" while a 0 represents \"forget this\".\n\n$$\nf_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n$$\n\n#### Input Gate $i_t$:\n\nThis gate decides which new information will be stored in the cell state. It consists of two parts:\n- A sigmoid layer that decides which values we'll update.\n- A tanh layer that creates a vector of new candidate values $\\tilde{c}_t$ that could be added to the state.\n\n$$\ni_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n$$\n\n$$\n\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)\n$$\n\n#### Cell State Update $c_t$:\n\nThis step updates the old cell state $c_{t-1}$ into the new cell state $c_t$. It multiplies the old state by the forget gate output, then adds the product of the input gate and the new candidate values.\n\n$$\nc_t = f_t \\circ c_{t-1} + i_t \\circ \\tilde{c}_t\n$$\n\n#### Output Gate $o_t$:\n\nThis gate decides what parts of the cell state we're going to output. It uses a sigmoid function to determine which parts of the cell state to output, and then multiplies it by a tanh of the cell state to get the final output.\n\n$$\no_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n$$\n\n$$\nh_t = o_t \\circ \\tanh(c_t)\n$$\n\nWhere:\n- $(W_f, W_i, W_c, W_o)$ are weight matrices for the forget gate, input gate, cell state, and output gate respectively.\n- $(b_f, b_i, b_c, b_o)$ are bias vectors.\n- $\\sigma$ is the sigmoid activation function.\n- $\\circ$ denotes element-wise multiplication.\n\n### Implementation Steps\n\n1. **Initialization**: Start with the initial cell state $c_0$ and hidden state $h_0$.\n2. **Sequence Processing**: For each input $x_t$ in the sequence:\n   - Compute forget gate $f_t$, input gate $i_t$, candidate cell state $\\tilde{c}_t$, and output gate $o_t$.\n   - Update cell state $c_t$ and hidden state $h_t$.\n3. **Final Output**: After processing all inputs, the final hidden state $h_T$ (where $T$ is the length of the sequence) contains information from the entire sequence.\n\n### Example Calculation\n\nGiven:\n- Inputs: $x_1 = 1.0$, $x_2 = 2.0$, $x_3 = 3.0$\n- Initial states: $c_0 = 0.0$, $h_0 = 0.0$\n- Simplified weights (for demonstration): $W_f = W_i = W_c = W_o = 0.5$\n- All biases: $b_f = b_i = b_c = b_o = 0.1$\n\n#### Compute:\n\n**First time step $t = 1$:**\n\n$$\nf_1 = \\sigma(0.5 \\times 1.0 + 0.1) = 0.6487\n$$\n\n$$\ni_1 = \\sigma(0.5 \\times 1.0 + 0.1) = 0.6487\n$$\n\n$$\n\\tilde{c}_1 = \\tanh(0.5 \\times 1.0 + 0.1) = 0.5370\n$$\n\n$$\nc_1 = f_1 \\times 0.0 + i_1 \\times \\tilde{c}_1 = 0.6487 \\times 0.0 + 0.6487 \\times 0.5370 = 0.3484\n$$\n\n$$\no_1 = \\sigma(0.5 \\times 1.0 + 0.1) = 0.6487\n$$\n\n$$\nh_1 = o_1 \\times \\tanh(c_1) = 0.6487 \\times \\tanh(0.3484) = 0.2169\n$$\n\n**Second time step $t = 2$:**\n(Calculations omitted for brevity, but follow the same pattern using $x_2 = 2.0$ and the previous states)\n\n**Third time step $t = 3$:**\n(Calculations omitted for brevity, but follow the same pattern using $x_3 = 3.0$ and the previous states)\n\nThe final hidden state $h_3$ would be the result after these calculations.\n\n### Applications\n\nLSTMs are extensively used in various sequence modeling tasks, including machine translation, speech recognition, and time series forecasting, where capturing long-term dependencies is crucial.",
  "starter_code": "import numpy as np\n\nclass LSTM:\n\tdef __init__(self, input_size, hidden_size):\n\t\tself.input_size = input_size\n\t\tself.hidden_size = hidden_size\n\n\t\t# Initialize weights and biases\n\t\tself.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n\t\tself.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n\t\tself.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n\t\tself.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n\t\tself.bf = np.zeros((hidden_size, 1))\n\t\tself.bi = np.zeros((hidden_size, 1))\n\t\tself.bc = np.zeros((hidden_size, 1))\n\t\tself.bo = np.zeros((hidden_size, 1))\n\n\tdef forward(self, x, initial_hidden_state, initial_cell_state):\n\t\t\"\"\"\n\t\tProcesses a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n\t\t\"\"\"\n\t\tpass",
  "solution": "import numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        h = initial_hidden_state\n        c = initial_cell_state\n        outputs = []\n\n        for t in range(len(x)):\n            xt = x[t].reshape(-1, 1)\n            concat = np.vstack((h, xt))\n\n            # Forget gate\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n\n            # Input gate\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n\n            # Cell state update\n            c = ft * c + it * c_tilde\n\n            # Output gate\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n\n            # Hidden state update\n            h = ot * np.tanh(c)\n\n            outputs.append(h)\n\n        return np.array(outputs), h, c\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))",
  "example": {
    "input": "input_sequence = np.array([[1.0], [2.0], [3.0]])\ninitial_hidden_state = np.zeros((1, 1))\ninitial_cell_state = np.zeros((1, 1))\n\nlstm = LSTM(input_size=1, hidden_size=1)\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)",
    "output": "[[0.73698596]] (approximate)",
    "reasoning": "The LSTM processes the input sequence [1.0, 2.0, 3.0] and produces the final hidden state [0.73698596]."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\ninput_sequence = np.array([[1.0], [2.0], [3.0]])\ninitial_hidden_state = np.zeros((1, 1))\ninitial_cell_state = np.zeros((1, 1))\n\nlstm = LSTM(input_size=1, hidden_size=1)\n# Set weights and biases for reproducibility\nlstm.Wf = np.array([[0.5, 0.5]])\nlstm.Wi = np.array([[0.5, 0.5]])\nlstm.Wc = np.array([[0.3, 0.3]])\nlstm.Wo = np.array([[0.5, 0.5]])\nlstm.bf = np.array([[0.1]])\nlstm.bi = np.array([[0.1]])\nlstm.bc = np.array([[0.1]])\nlstm.bo = np.array([[0.1]])\n\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)",
      "expected_output": "[[0.73698596]]"
    },
    {
      "test": "import numpy as np\n\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]])\ninitial_hidden_state = np.zeros((2, 1))\ninitial_cell_state = np.zeros((2, 1))\n\nlstm = LSTM(input_size=2, hidden_size=2)\n# Set weights and biases for reproducibility\nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\nlstm.bf = np.array([[0.1], [0.2]])\nlstm.bi = np.array([[0.1], [0.2]])\nlstm.bc = np.array([[0.1], [0.2]])\nlstm.bo = np.array([[0.1], [0.2]])\n\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)",
      "expected_output": "[[0.16613133], [0.40299449]]"
    }
  ]
}