{
  "id": "85",
  "title": "Positional Encoding Calculator",
  "difficulty": "hard",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/dsaha21",
      "name": "Dripto Saha"
    }
  ],
  "description": "Write a Python function to implement the Positional Encoding layer for Transformers.\n      The function should calculate positional encodings for a sequence length (`position`) and model dimensionality (`d_model`) using sine and cosine functions as specified in the Transformer architecture.\n      The function should return -1 if `position` is 0, or if `d_model` is less than or equal to 0. The output should be a numpy array of type `float16`.",
  "learn_section": "## **The Positional Encoding Layer in Transformers**\n\nThe Positional Encoding layer in Transformers plays a critical role by providing necessary positional information to the model. \nThis is particularly important because the Transformer architecture, unlike RNNs or LSTMs, processes input sequences in parallel\nand lacks inherent mechanisms to account for the sequential order of tokens.\n\nThe mathematical intuition behind the Positional Encoding layer in Transformers is centered on enabling the model to incorporate\ninformation about the order of tokens in a sequence.\n\n---\n\n### **Function Parameters**\n\n- **`position`**: Total positions or length of the sequence.\n- **`d_model`**: Dimensionality of the model's output.\n\n---\n\n### **Generating the Base Matrix**\n\n- **`angle_rads`**: Creates a matrix where rows represent sequence positions and columns represent feature dimensions.\n Values are scaled by dividing each position index by:  \n  $10000^{\\frac{2 \\cdot i}{d_{model}}}$\n\n---\n\n### **Applying Sine and Cosine Functions**\n\n- For even indices: Apply the sine function to encode positions.  \n  $PE(\\text{pos}, 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{model}}}}\\right)$\n\n- For odd indices: Apply the cosine function for a phase-shifted encoding.  \n  $PE(\\text{pos}, 2i+1) = \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{model}}}}\\right)$\n\n---\n\n### **Creating the Positional Encoding Tensor**\n\n- The matrix is expanded to match input shape expectations of models like Transformers and cast to `float32`.\n\n---\n\n### **Output**\n\nReturns a TensorFlow tensor of shape $(1, \\text{position}, \\text{d\\_model})$, ready to be added to input embeddings\nto incorporate positional information.",
  "starter_code": "import numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n\t# Your code here\n\tpos_encoding = np.float16(pos_encoding)\n\treturn pos_encoding",
  "solution": "import numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Create position and dimension indices\n    pos = np.arange(position, dtype=np.float32).reshape(position, 1)\n    ind = np.arange(d_model, dtype=np.float32).reshape(1, d_model)\n\n    # Compute the angles\n    angle_rads = pos / np.power(10000, (2 * (ind // 2)) / d_model)\n\n    # Apply sine to even indices, cosine to odd indices\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Even indices (0, 2, 4...)\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Odd indices (1, 3, 5...)\n\n    # Convert to float16 as required\n    return angle_rads.astype(np.float16)",
  "example": {
    "input": "position = 2, d_model = 8",
    "reasoning": "The function computes the positional encoding by calculating sine values for even indices and cosine values for odd indices, ensuring that the encoding provides the required positional information.",
    "output": "[[[ 0.,0.,0.,0.,1.,1.,1.,1.,]\n  [ 0.8413,0.0998,0.01,0.001,0.5405,0.995,1.,1.]]]"
  },
  "test_cases": [
    {
      "test": "print(pos_encoding(2, 8))",
      "expected_output": "[[0.     , 1.     , 0.     , 1.     , 0.     , 1.     , 0.     ,\n        1.     ],\n       [0.8413 , 0.5405 , 0.09985, 0.995  , 0.01   , 1.     , 0.001  ,\n        1.     ]]"
    },
    {
      "test": "print(pos_encoding(5, 16))",
      "expected_output": "[[ 0.000e+00,  1.000e+00,  0.000e+00,  1.000e+00,  0.000e+00,\n         1.000e+00,  0.000e+00,  1.000e+00,  0.000e+00,  1.000e+00,\n         0.000e+00,  1.000e+00,  0.000e+00,  1.000e+00,  0.000e+00,\n         1.000e+00],\n       [ 8.413e-01,  5.405e-01,  3.110e-01,  9.502e-01,  9.985e-02,\n         9.951e-01,  3.162e-02,  9.995e-01,  1.000e-02,  1.000e+00,\n         3.162e-03,  1.000e+00,  1.000e-03,  1.000e+00,  3.161e-04,\n         1.000e+00],\n       [ 9.092e-01, -4.163e-01,  5.913e-01,  8.066e-01,  1.986e-01,\n         9.800e-01,  6.323e-02,  9.980e-01,  2.000e-02,  1.000e+00,\n         6.325e-03,  1.000e+00,  2.001e-03,  1.000e+00,  6.323e-04,\n         1.000e+00],\n       [ 1.411e-01, -9.902e-01,  8.125e-01,  5.825e-01,  2.954e-01,\n         9.556e-01,  9.473e-02,  9.956e-01,  3.000e-02,  9.995e-01,\n         9.483e-03,  1.000e+00,  3.000e-03,  1.000e+00,  9.489e-04,\n         1.000e+00],\n       [-7.568e-01, -6.538e-01,  9.536e-01,  3.010e-01,  3.894e-01,\n         9.209e-01,  1.261e-01,  9.922e-01,  3.998e-02,  9.990e-01,\n         1.265e-02,  1.000e+00,  4.002e-03,  1.000e+00,  1.265e-03,\n         1.000e+00]]"
    }
  ]
}