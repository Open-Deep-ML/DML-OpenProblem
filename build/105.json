{
  "id": "105",
  "title": "Train Softmax Regression with Gradient Descent",
  "difficulty": "hard",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/turkunov",
      "name": "turkunov"
    }
  ],
  "description": "Implement a gradient descent-based training algorithm for Softmax regression. Your task is to compute model parameters using Cross Entropy loss and return the optimized coefficients along with collected loss values over iterations. Make sure to round your solution to 4 decimal places",
  "learn_section": "## Overview\nSoftmax regression is a type of logistic regression that extends it to a multiclass problem by outputting a vector $P$ of probabilities for each distinct class and taking $argmax(P)$.\n\n## Connection to a regular logistic regression\nRecall that a standard logistic regression is aimed at approximating\n$$\np = \\frac{1}{e^{-X\\beta}+1} = \\\\\n= \\frac{e^{X\\beta}}{1+e^{X\\beta}},\n$$\n\nwhich actually alignes with the definition of the softmax function:\n$$\nsoftmax(z_i)=\\sigma(z_i)=\\frac{e^{z_i}}{\\sum_j^Ce^{z_j}},\n$$\n\nwhere $C$ is the number of classes and values of which sum up to $1$. Hence it simply extends the functionality of sigmoid to more than 2 classes and could be used for assigning probability values in a categorical distribution, i.e. softmax regression searches for the following vector-approximation:\n$$\np^{(i)}=\\frac{e^{x^{(i)}\\beta}}{\\sum_j^Ce^{x^{(i)}\\beta_j}_j}\n$$\n\n## Loss in softmax regression\n**tl;dr** key differences in the loss from logistic regression include replacing sigmoid with softmax and calculating several gradients for vectors $\\beta_j$ corresponding to a particular class $j\\in\\{1,...,C\\}$.\n\nRecall that we use MLE in logistic regression. It is the same case with softmax regression, although instead of Bernoulli-distributed random variable we have categorical distribution, which is an extension of Bernoulli to more than 2 labels. Its PMF is defined as:\n$$\nf(y|p)=\\prod_{i=1}^Kp_i^{[i=y]},\n$$\n\nHence, our log-likelihood looks like:\n$$\n\\sum_X \\sum_j^C [y_i=j] \\log \\left[p\\left(x_i\\right)\\right]\n$$\n\nWhere we replace our probability function with softmax:\n$$\n\\sum_X \\sum_j^C [y_i=j] \\log \\frac{e^{x_i\\beta_j}}{\\sum_j^Ce^{x_i\\beta_j}}\n$$\n\nwhere $[i=y]$ is a function, that returns $0$, if $i\\neq y$, and $1$ otherwise and $C$ - number of distinct classes (labels). You can see that since we are expecting a $1\\times C$ output of $y$, just like in the neuron backprop problem, we will be having separate vector $\\beta_j$ for every $j$ class out of $C$. \n\n## Optimization objective\nThe optimization objective is the same as with logistic regression. The function, which we are optimizing, is also commonly refered as **Cross Entropy** (CE):\n\n$$\nargmin_\\beta -[\\sum_X \\sum_j^C [y_i=j] \\log \\frac{e^{x_i\\beta_j}}{\\sum_j^Ce^{x_i\\beta_j}}] \\\\\n$$\n\nThen we are yet again using a chain rule for calculating partial derivative of $CE$ with respect to $\\beta$:\n\n\n$$\n\\frac{\\partial CE}{\\partial\\beta^{(j)}_i}=\\frac{\\partial CE}{\\partial\\sigma}\\frac{\\partial\\sigma}{\\partial[X\\beta^{(j)}]}\\frac{\\partial[X\\beta^{(j)}]}{\\beta^{(j)}_i}\n$$\n\nWhich is eventually reduced to a similar to logistic regression gradient matrix form:\n$$\nX^T(\\sigma(X\\beta^{(j)})-Y)\n$$\n\nThen we can finally use gradient descent in order to iteratively update our parameters with respect to a particular class:\n$$\n\\beta^{(j)}_{t+1}=\\beta^{(j)}_t - \\eta [X^T(\\sigma(X\\beta^{(j)}_t)-Y)]\n$$",
  "starter_code": "import numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n\t\"\"\"\n\tGradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\t\"\"\"\n\t# Your code here\n\tpass",
  "solution": "import numpy as np\n\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, \n                 learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    '''        \n    Gradient-descent training algorithm for softmax regression, that collects mean-reduced\n    CE losses, accuracies.\n    Returns\n    -------\n    B : list[float]\n        CxM updated parameter vector rounded to 4 floating points\n    losses : list[float]\n        collected values of a Cross Entropy rounded to 4 floating points\n    '''\n\n    def softmax(z):\n        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n\n    def accuracy(y_pred, y_true):\n        return (np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1)).sum() / len(y_true)\n\n    def ce_loss(y_pred, y_true):\n        true_labels_idx = np.argmax(y_true, axis=1)\n        return -np.sum(np.log(y_pred)[list(range(len(y_pred))),true_labels_idx])\n\n    y = y.astype(int)\n    C = y.max()+1 # we assume that classes start from 0\n    y = np.eye(C)[y]\n    X = np.hstack((np.ones((X.shape[0], 1)), X))\n    B = np.zeros((X.shape[1], C))\n    accuracies, losses = [], []\n\n    for epoch in range(iterations):\n        y_pred = softmax(X @ B)\n        B -= learning_rate * X.T @ (y_pred - y)\n        losses.append(round(ce_loss(y_pred, y), 4))\n        accuracies.append(round(accuracy(y_pred, y), 4))\n\n    return B.T.round(4).tolist(), losses",
  "example": {
    "input": "train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10)",
    "reasoning": "The function iteratively updates the Softmax regression parameters using gradient descent and collects loss values over iterations.",
    "output": "([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])"
  },
  "test_cases": [
    {
      "test": "print(train_softmaxreg(np.array([[2.5257, 2.3333, 1.7730, 0.4106, -1.6648], [1.5101, 1.3023, 1.3198, 1.3608, 0.4638], [-2.0969, -1.3596, -1.0403, -2.2548, -0.3235], [-0.9666, -0.6068, -0.7201, -1.7325, -1.1281], [-0.3809, -0.2485, 0.1878, 0.5235, 1.3072], [0.5482, 0.3315, 0.1067, 0.3069, -0.3755], [-3.0339, -2.0196, -0.6546, -0.9033, 2.8918], [0.2860, -0.1265, -0.5220, 0.2830, -0.5865], [-0.2626, 0.7601, 1.8409, -0.2324, 1.8071], [0.3028, -0.4023, -1.2955, -0.1422, -1.7812]]), np.array([2, 3, 0, 0, 1, 3, 0, 1, 2, 1]), 0.03, 10))",
      "expected_output": "([[-0.0841, -0.5693, -0.3651, -0.2423, -0.5344, 0.0339], [0.2566, 0.0535, -0.2103, -0.4004, 0.2709, -0.1461], [-0.1318, 0.211, 0.3998, 0.523, -0.1001, 0.0545], [-0.0407, 0.3049, 0.1757, 0.1197, 0.3637, 0.0576]], [13.8629, 10.7201, 9.3163, 8.4942, 7.9132, 7.4598, 7.0854, 6.7653, 6.4851, 6.2358])"
    },
    {
      "test": "print(train_softmaxreg(np.array([[0.5, -1.2], [-0.3, 1.1], [0.8, -0.6]]), np.array([0, 1, 2]), 0.01, 10))",
      "expected_output": "([[-0.0011, 0.0145, -0.0921], [0.002, -0.0598, 0.1263], [-0.0009, 0.0453, -0.0342]], [3.2958, 3.2611, 3.2272, 3.1941, 3.1618, 3.1302, 3.0993, 3.0692, 3.0398, 3.011])"
    }
  ]
}