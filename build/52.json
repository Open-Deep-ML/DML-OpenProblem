{
  "id": "52",
  "title": "Implement Recall Metric in Binary Classification",
  "difficulty": "easy",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/rafaelgreca",
      "name": "Rafael Greca"
    }
  ],
  "description": "## Task: Implement Recall in Binary Classification\n\nYour task is to implement the **recall** metric in a binary classification setting. Recall is a performance measure that evaluates how effectively a machine learning model identifies positive instances from all the actual positive cases in a dataset.\n\nYou need to write a function `recall(y_true, y_pred)` that calculates the recall metric. The function should accept two inputs:\n\n- `y_true`: A list of true binary labels (0 or 1) for the dataset.\n- `y_pred`: A list of predicted binary labels (0 or 1) from the model.\n\nYour function should return the recall value rounded to three decimal places. If the denominator (TP + FN) is zero, the recall should be 0.0 to avoid division by zero.\n\n    ",
  "learn_section": "\n## Understanding Recall in Classification\n\nRecall is a metric that measures how often a machine learning model correctly identifies positive instances, also known as true positives, from all the actual positive samples in the dataset.\n\n### Mathematical Definition\n\nRecall, also known as sensitivity, is the fraction of relevant instances that were retrieved. It is calculated using the following equation:\n$$\n\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n$$\n\nWhere:\n1. **True Positives (TP)**: The number of positive samples that are correctly identified as positive.\n2. **False Negatives (FN)**: The number of positive samples that are incorrectly identified as negative.\n\n### Task\n\nIn this problem, you will implement a function to calculate recall given the true labels and predicted labels of a binary classification task. The results should be rounded to three decimal places.",
  "starter_code": "import numpy as np\ndef recall(y_true, y_pred):\n    ",
  "solution": "import numpy as np\n\ndef recall(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    try:\n        return round(tp / (tp + fn), 3)\n    except ZeroDivisionError:\n        return 0.0",
  "example": {
    "input": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nprint(recall(y_true, y_pred))",
    "output": "# 0.75",
    "reasoning": "The recall value for the given true labels and predicted labels is 0.75. The model correctly identified 3 out of 4 positive instances in the dataset."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nprint(recall(y_true, y_pred))",
      "expected_output": "0.75"
    },
    {
      "test": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 0, 0, 0, 1])\nprint(recall(y_true, y_pred))",
      "expected_output": "0.333"
    },
    {
      "test": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 1, 0, 0])\nprint(recall(y_true, y_pred))",
      "expected_output": "1.0"
    },
    {
      "test": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([0, 0, 0, 1, 0, 1])\nprint(recall(y_true, y_pred))",
      "expected_output": "0.5"
    },
    {
      "test": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([0, 1, 0, 0, 1, 0])\nprint(recall(y_true, y_pred))",
      "expected_output": "0.0"
    }
  ]
}