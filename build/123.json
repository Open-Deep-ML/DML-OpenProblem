{
  "id": "123",
  "title": "Calculate Computational Efficiency of MoE",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "Calculate the computational cost savings of an MoE layer compared to a dense layer, as discussed in the paper 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.' Given the number of experts, sparsity (number of active experts), and input/output dimensions, compute the floating-point operations (FLOPs) for both and determine the savings percentage.",
  "learn_section": "\n## Understanding MoE Efficiency\n\nThe paper *\"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\"* introduces the idea of activating only a few expert networks per input to drastically reduce computation. This is known as **conditional computation**, and it allows models to scale to billions of parameters without significantly increasing cost.\n\n---\n\n### Key Idea\n\nIn a **dense layer**, every input goes through the full set of parameters.  \nIn a **Mixture-of-Experts (MoE)** layer, only $k$ out of $n$ experts are active for each input.\n\n---\n\n### FLOPs Formulas\n\nLet:\n- $d_{in}$ = input dimension  \n- $d_{out}$ = output dimension  \n- $n$ = total experts  \n- $k$ = active experts per input  \n\nThen:\n- **Dense layer FLOPs**:  \n  $$\n  \\text{FLOPs}_{\\text{dense}} = n \\cdot d_{in} \\cdot d_{out}\n  $$\n- **MoE layer FLOPs**:  \n  $$\n  \\text{FLOPs}_{\\text{moe}} = k \\cdot d_{in} \\cdot d_{out}\n  $$\n- **Efficiency gain**:\n  $$\n  \\text{Savings}(\\%) = \\left( \\frac{\\text{FLOPs}_{\\text{dense}} - \\text{FLOPs}_{\\text{moe}}}{\\text{FLOPs}_{\\text{dense}}} \\right) \\cdot 100\n  $$\n\n---\n\n### Example\n\nSuppose:\n- $n = 1000$, $k = 2$  \n- $d_{in} = d_{out} = 512$  \n\nThen:\n- MoE FLOPs = $2 \\cdot 512 \\cdot 512 = 524,\\!288$  \n- Full dense (all 1000 experts): $1000 \\cdot 512 \\cdot 512 = 262,\\!144,\\!000$  \n- Savings:\n  $$\n  \\left( \\frac{262,\\!144,\\!000 - 524,\\!288}{262,\\!144,\\!000} \\right) \\cdot 100 \\approx 99.8\\%\n  $$\n\nThis means the MoE layer uses just **0.2%** of the computation compared to a full dense version — an enormous gain in efficiency.\n\n---\n\n### Summary\n\nBy activating only a small number of experts per input, MoE layers reduce computation while maintaining high model capacity. This makes it feasible to train outrageously large models efficiently.",
  "starter_code": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate computational savings of MoE vs. dense layer.\n\n    Args:\n        n_experts: Total number of experts\n        k_active: Number of active experts (sparsity)\n        d_in: Input dimension\n        d_out: Output dimension\n\n    Returns:\n        Percentage savings in FLOPs\n    \"\"\"\n    pass",
  "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    dense_flops = n_experts * d_in * d_out\n    moe_flops = k_active * d_in * d_out\n    savings = (dense_flops - moe_flops) / dense_flops * 100\n    return round(savings, 1)",
  "example": {
    "input": "compute_efficiency(1000, 2, 512, 512)",
    "output": "99.8",
    "reasoning": "Dense layer FLOPs: 1000 * 512 * 512 = 262,144,000. MoE FLOPs: 2 * 512 * 512 = 524,288. Savings:  ((262,144,000 - 524,288) / 262,144,000) x 100 ≈ 99.8%."
  },
  "test_cases": [
    {
      "test": "print(round(compute_efficiency(1000, 2, 512, 512),1))",
      "expected_output": "99.8"
    },
    {
      "test": "print(round(compute_efficiency(10, 2, 256, 256),1))",
      "expected_output": "80.0"
    }
  ]
}