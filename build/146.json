{
  "id": "146",
  "title": "Momentum Optimizer",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/mavleo96",
      "name": "Vijayabharathi Murugan"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "Implement the momentum optimizer update step function. Your function should take the current parameter value, gradient, and velocity as inputs, and return the updated parameter value and new velocity. The function should also handle scalar and array inputs.",
  "learn_section": "# Implementing Momentum Optimizer\n\n## Introduction\nMomentum is a popular optimization technique that helps accelerate gradient descent in the relevant direction and dampens oscillations. It works by adding a fraction of the previous update vector to the current gradient.\n\n## Learning Objectives\n- Understand how momentum optimization works\n- Learn to implement momentum-based gradient updates\n- Understand the effect of momentum on optimization\n\n## Theory\nMomentum optimization uses a moving average of gradients to determine the direction of the update. The key equations are:\n\n$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J(\\theta)$ (Velocity update)\n\n$\\theta_t = \\theta_{t-1} - v_t$ (Parameter update)\n\nWhere:\n- $v_t$ is the velocity at time t\n- $\\gamma$ is the momentum coefficient (typically 0.9)\n- $\\eta$ is the learning rate\n- $\\nabla_\\theta J(\\theta)$ is the gradient of the loss function\n\nRead more at:\n\n1. Ruder, S. (2017). An overview of gradient descent optimization algorithms. [arXiv:1609.04747](https://arxiv.org/pdf/1609.04747)\n\n\n## Problem Statement\nImplement the momentum optimizer update step function. Your function should take the current parameter value, gradient, and velocity as inputs, and return the updated parameter value and new velocity.\n\n### Input Format\nThe function should accept:\n- parameter: Current parameter value\n- grad: Current gradient\n- velocity: Current velocity\n- learning_rate: Learning rate (default=0.01)\n- momentum: Momentum coefficient (default=0.9)\n\n### Output Format\nReturn tuple: (updated_parameter, updated_velocity)\n\n## Example\n```python\n# Example usage:\nparameter = 1.0\ngrad = 0.1\nvelocity = 0.1\n\nnew_param, new_velocity = momentum_optimizer(parameter, grad, velocity)\n```\n\n## Tips\n- Initialize velocity as zero\n- Use numpy for numerical operations\n- Test with both scalar and array inputs\n\n---",
  "starter_code": "import numpy as np\n\ndef momentum_optimizer(parameter, grad, velocity, learning_rate=0.01, momentum=0.9):\n    \"\"\"\n    Update parameters using the momentum optimizer.\n    Uses momentum to accelerate learning in relevant directions and dampen oscillations.\n\n    Args:\n        parameter: Current parameter value\n        grad: Current gradient\n        velocity: Current velocity/momentum term\n        learning_rate: Learning rate (default=0.01)\n        momentum: Momentum coefficient (default=0.9)\n\n    Returns:\n        tuple: (updated_parameter, updated_velocity)\n    \"\"\"\n    # Your code here\n    return np.round(parameter, 5), np.round(velocity, 5)",
  "solution": "import numpy as np\n\ndef momentum_optimizer(parameter, grad, velocity, learning_rate=0.01, momentum=0.9):\n    \"\"\"\n    Update parameters using the momentum optimizer.\n    Uses momentum to accelerate learning in relevant directions and dampen oscillations.\n\n    Args:\n        parameter: Current parameter value\n        grad: Current gradient\n        velocity: Current velocity/momentum term\n        learning_rate: Learning rate (default=0.01)\n        momentum: Momentum coefficient (default=0.9)\n\n    Returns:\n        tuple: (updated_parameter, updated_velocity)\n    \"\"\"\n    assert learning_rate > 0, \"Learning rate must be positive\"\n    assert 0 <= momentum < 1, \"Momentum must be between 0 and 1\"\n\n    # Update velocity\n    velocity = momentum * velocity + learning_rate * grad\n    \n    # Update parameters\n    parameter = parameter - velocity\n\n    return np.round(parameter, 5), np.round(velocity, 5)",
  "example": {
    "input": "parameter = 1.0, grad = 0.1, velocity = 0.1",
    "output": "(0.909, 0.091)",
    "reasoning": "The momentum optimizer computes updated values for the parameter and the velocity. With input values parameter=1.0, grad=0.1, and velocity=0.1, the updated parameter becomes 0.909 and the updated velocity becomes 0.091."
  },
  "test_cases": [
    {
      "test": "print(momentum_optimizer(1., 0.1, 0.5, 0.01, 0.9))",
      "expected_output": "(0.549, 0.451)"
    },
    {
      "test": "print(momentum_optimizer(np.array([1., 2.]), np.array([0.1, 0.2]), np.array([0.5, 1.0]), 0.01, 0.9))",
      "expected_output": "(array([0.549, 1.098]), array([0.451, 0.902]))"
    },
    {
      "test": "print(momentum_optimizer(np.array([1., 2.]), np.array([0.1, 0.2]), np.array([0.5, 1.0]), 0.01, 0.))",
      "expected_output": "(array([0.999, 1.998]), array([0.001, 0.002]))"
    },
    {
      "test": "print(momentum_optimizer(np.array([1., 2.]), np.array([0., 0.]), np.array([0.5, 0.5]), 0.01, 0.9))",
      "expected_output": "(array([0.55, 1.55]), array([0.45, 0.45]))"
    }
  ]
}