{
  "id": "46",
  "title": "Implement Precision Metric",
  "difficulty": "easy",
  "category": "Machine Learning",
  "video": "https://youtu.be/u99yBNF4vE0",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description": "Write a Python function `precision` that calculates the precision metric given two numpy arrays: `y_true` and `y_pred`. The `y_true` array contains the true binary labels, and the `y_pred` array contains the predicted binary labels. Precision is defined as the ratio of true positives to the sum of true positives and false positives.",
  "learn_section": "\n## Understanding Precision in Classification\n\nPrecision is a key metric used in the evaluation of classification models, particularly in binary classification. It provides insight into the accuracy of the positive predictions made by the model.\n\n### Mathematical Definition\nPrecision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP):\n$$\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n$$\n\nWhere:\n- **True Positives (TP)**: The number of positive samples that are correctly identified as positive.\n- **False Positives (FP)**: The number of negative samples that are incorrectly identified as positive.\n\n### Characteristics of Precision\n- **Range**: Precision ranges from 0 to 1, where 1 indicates perfect precision (no false positives) and 0 indicates no true positives.\n- **Interpretation**: High precision means that the model has a low false positive rate, meaning it rarely labels negative samples as positive.\n- **Use Case**: Precision is particularly useful when the cost of false positives is high, such as in medical diagnosis or fraud detection.\n\nIn this problem, you will implement a function to calculate precision given the true labels and predicted labels of a binary classification task.",
  "starter_code": "import numpy as np\ndef precision(y_true, y_pred):\n\t# Your code here\n\tpass",
  "solution": "import numpy as np\n\ndef precision(y_true, y_pred):\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0",
  "example": {
    "input": "import numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nresult = precision(y_true, y_pred)\nprint(result)",
    "output": "1.0",
    "reasoning": "- True Positives (TP) = 3\n- False Positives (FP) = 0\n- Precision = TP / (TP + FP) = 3 / (3 + 0) = 1.0"
  },
  "test_cases": [
    {
      "test": "import numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(result)",
      "expected_output": "1.0"
    },
    {
      "test": "import numpy as np\ny_true = np.array([1, 0, 1, 1, 0, 0])\ny_pred = np.array([1, 0, 0, 0, 0, 1])\nresult = precision(y_true, y_pred)\nprint(result)",
      "expected_output": "0.5"
    }
  ]
}