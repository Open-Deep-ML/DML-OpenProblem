{
  "id": "62",
  "title": "Implement a Simple RNN with Backpropagation Through Time (BPTT)",
  "difficulty": "hard",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/moe18",
      "name": "Moe Chabot"
    }
  ],
  "description": "## Task: Implement a Simple RNN with Backpropagation Through Time (BPTT)\n\nYour task is to implement a simple Recurrent Neural Network (RNN) and backpropagation through time (BPTT) to learn from sequential data. The RNN will process input sequences, update hidden states, and perform backpropagation to adjust weights based on the error gradient.\n\nWrite a class `SimpleRNN` with the following methods:\n\n- `__init__(self, input_size, hidden_size, output_size)`: Initializes the RNN with random weights and zero biases.\n- `forward(self, x)`: Processes a sequence of inputs and returns the hidden states and output.\n- `backward(self, x, y, learning_rate)`: Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n\nIn this task, the RNN will be trained on sequence prediction, where the network will learn to predict the next item in a sequence. You should use 1/2 * Mean Squared Error (MSE) as the loss function and make sure to aggregate the losses at each time step by summing.",
  "learn_section": "# Understanding Recurrent Neural Networks (RNNs) and Backpropagation Through Time (BPTT)\n\nRecurrent Neural Networks (RNNs) are designed to handle sequential data by maintaining a hidden state that captures information from previous inputs. They are particularly useful for tasks where context or sequential order is important, such as language modeling, time series forecasting, and sequence prediction.\n\n## RNN Architecture\n\nAn RNN processes inputs one at a time while maintaining a hidden state that gets updated at each time step. The core equations governing the forward pass of an RNN are:\n\n### 1) Hidden State Update\n\n$$\nh_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n$$\n\n### 2) Output Computation\n\n$$\ny_t = W_{hy} h_t + b_y\n$$\n\nWhere:\n\n1. $x_t$ is the input at time step $t$.\n2. $h_t$ is the hidden state at time step $t$.\n3. $W_{xh}$ is the weight matrix for input to hidden state.\n4. $W_{hh}$ is the weight matrix for hidden state to hidden state.\n5. $W_{hy}$ is the weight matrix for hidden state to output.\n6. $b_h$ and $b_y$ are the bias terms for the hidden state and output, respectively.\n7. $\\tanh$ is the hyperbolic tangent activation function applied element-wise.\n\n## Forward Pass Implementation\n\nIn the forward pass, we iterate over each element in the input sequence, updating the hidden state and computing the output:\n\n1. **Initialize the hidden state** $h_0$ to zeros.\n2. For each time step $t$:\n   - Compute $h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$.\n   - Compute $y_t = W_{hy} h_t + b_y$.\n   - Store $h_t$ and $y_t$ for use in backpropagation.\n\n## Loss Function\n\nThe loss function measures the discrepancy between the predicted outputs and the actual target values. For sequence prediction tasks, we often use the **Mean Squared Error (MSE)** loss:\n\n$$\n\\text{Loss} = \\frac{1}{T} \\sum_{t=1}^{T} (\\hat{y}_t - y_t)^2\n$$\n\nWhere $T$ is the length of the sequence, $\\hat{y}_t$ is the predicted output, and $y_t$ is the actual target at time step $t$.\n\n## Backpropagation Through Time (BPTT)\n\nBPTT is the process of training RNNs by unrolling them through time and applying backpropagation to compute gradients for each time step. The key steps in BPTT are:\n\n1. Compute the gradient of the loss with respect to the outputs:\n\n$$\n\\frac{dL}{dy_t} = \\hat{y}_t - y_t\n$$\n\n2. Compute the gradients for the output layer weights and biases:\n\n$$\ndW_{hy} += \\frac{dL}{dy_t} \\cdot h_t^T\n$$\n\n$$\ndb_y += \\frac{dL}{dy_t}\n$$\n\n3. Backpropagate the gradients through the hidden layers:\n\n$$\ndh_t = W_{hy}^T \\cdot \\frac{dL}{dy_t} + dh_{t+1}\n$$\n\n$$\ndh_{\\text{raw}} = dh_t \\circ (1 - h_t^2)\n$$\n\nHere, $\\circ$ denotes element-wise multiplication, and $(1 - h_t^2)$ is the derivative of the $\\tanh$ activation function.\n\n4. Compute the gradients for the hidden layer weights and biases:\n\n$$\ndW_{xh} += dh_{\\text{raw}} \\cdot x_t^T\n$$\n\n$$\ndW_{hh} += dh_{\\text{raw}} \\cdot h_{t-1}^T\n$$\n\n$$\ndb_h += dh_{\\text{raw}}\n$$\n\nWe repeat steps 1-4 for each time step $t$ in reverse order (from $T$ to 1), accumulating the gradients. The term $dh_{t+1}$ represents the gradient flowing from the next time step, initialized to zeros at the last time step.\n\n## Updating Weights\n\nAfter computing the gradients, we update the weights and biases using gradient descent:\n\n$$\nW_{xh} -= \\text{learning\\_rate} \\times dW_{xh}\n$$\n\n$$\nW_{hh} -= \\text{learning\\_rate} \\times dW_{hh}\n$$\n\n$$\nW_{hy} -= \\text{learning\\_rate} \\times dW_{hy}\n$$\n\n$$\nb_h -= \\text{learning\\_rate} \\times db_h\n$$\n\n$$\nb_y -= \\text{learning\\_rate} \\times db_y\n$$\n\n## Implementing the RNN\n\nTo implement the RNN with BPTT, follow these steps:\n\n1. **Initialization**: Initialize the weight matrices $W_{xh}, W_{hh}, W_{hy}$ with small random values and biases $b_h, b_y$ with zeros.\n2. **Forward Pass**: Implement the forward method to process the input sequence, updating the hidden states and computing the outputs at each time step. Store the inputs, hidden states, and outputs for use in backpropagation.\n3. **Backward Pass**: Implement the backward method to perform BPTT. Compute the gradients at each time step in reverse order, accumulate them, and update the weights and biases.\n4. **Training Loop**: Train the RNN over multiple epochs by repeatedly performing forward and backward passes and updating the weights.\n\n## Tips for Implementation\n\n1. **Gradient Clipping**: To prevent exploding gradients, consider applying gradient clipping, which scales down gradients if they exceed a certain threshold.\n2. **Learning Rate**: Choose an appropriate learning rate. If the learning rate is too high, the training may become unstable.\n3. **Debugging**: Check the dimensions of all matrices and vectors to ensure they align correctly during matrix multiplication.\n4. **Testing**: Start with small sequences and hidden sizes to test your implementation before scaling up.\n\n## Example Calculation\n\nSuppose we have an input sequence $x = [x_1, x_2]$ and target sequence $y = [y_1, y_2]$. Here's how you would compute the forward and backward passes:\n\n### 1) Forward Pass\n\n- At $t = 1$:\n  - Compute $h_1 = \\tanh(W_{xh} x_1 + W_{hh} h_0 + b_h)$.\n  - Compute $\\hat{y}_1 = W_{hy} h_1 + b_y$.\n\n- At $t = 2$:\n  - Compute $h_2 = \\tanh(W_{xh} x_2 + W_{hh} h_1 + b_h)$.\n  - Compute $\\hat{y}_2 = W_{hy} h_2 + b_y$.\n\n### 2) Compute Loss\n\n$$\nL = \\frac{1}{2} \\left[ (\\hat{y}_1 - y_1)^2 + (\\hat{y}_2 - y_2)^2 \\right]\n$$\n\n### 3) Backward Pass\n\nStarting from $t = 2$ to $t = 1$:\n\n- At $t = 2$:\n  - Compute $\\frac{dL}{d\\hat{y}_2} = \\hat{y}_2 - y_2$.\n  - Backpropagate to find $dh_2$, $dW_{hy}$, $db_y$.\n\n- At $t = 1$:\n  - Use the chain rule to compute gradients with respect to inputs and weights.\n\n## Conclusion\n\nRNNs with BPTT are powerful for modeling sequences, but they come with challenges such as vanishing and exploding gradients. Techniques like gradient clipping, long short-term memory (LSTM) cells, or gated recurrent units (GRUs) can help mitigate these issues. Implementing an RNN from scratch provides a deeper understanding of the underlying mechanisms and prepares you for working with more complex architectures.",
  "starter_code": "\nimport numpy as np\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n\t\t\"\"\"\n\t\tInitializes the RNN with random weights and zero biases.\n\t\t\"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n    def forward(self, x):\n\t\t\"\"\"\n\t\tForward pass through the RNN for a given sequence of inputs.\n\t\t\"\"\"\n\t\tpass\n\n\tdef backward(self, x, y, learning_rate):\n\t\t\"\"\"\n\t\tBackpropagation through time to adjust weights based on error gradient.\n\t\t\"\"\"\n\t\tpass",
  "solution": "import numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        h = np.zeros((self.hidden_size, 1))  # Initialize hidden state\n        outputs = []\n        self.last_inputs = []\n        self.last_hiddens = [h]\n        \n        for t in range(len(x)):\n            self.last_inputs.append(x[t].reshape(-1, 1))\n            h = np.tanh(np.dot(self.W_xh, self.last_inputs[t]) + np.dot(self.W_hh, h) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            self.last_hiddens.append(h)\n        \n        self.last_outputs = outputs\n        return np.array(outputs)\n\n    def backward(self, x, y, learning_rate):\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        dh_next = np.zeros((self.hidden_size, 1))\n\n        for t in reversed(range(len(x))):\n            dy = self.last_outputs[t] - y[t].reshape(-1, 1)  # (Predicted - Actual)\n            dW_hy += np.dot(dy, self.last_hiddens[t+1].T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - self.last_hiddens[t+1] ** 2) * dh  # Derivative of tanh\n\n            dW_xh += np.dot(dh_raw, self.last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, self.last_hiddens[t].T)\n            db_h += dh_raw\n\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y",
  "example": {
    "input": "import numpy as np\n    input_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\n    expected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n    # Initialize RNN\n    rnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n    \n    # Forward pass\n    output = rnn.forward(input_sequence)\n    \n    # Backward pass\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n    \n    print(output)\n    \n    # The output should show the RNN predictions for each step of the input sequence.",
    "output": "[[x1], [x2], [x3], [x4]]",
    "reasoning": "The RNN processes the input sequence [1.0, 2.0, 3.0, 4.0] and predicts the next item in the sequence at each step."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\nnp.random.seed(42) \n\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n\nrnn = SimpleRNN(input_size=1, hidden_size=5, output_size=1)\n\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\nprint(output)",
      "expected_output": "[[[2.24143915]],[[3.18450265]],[[4.04305928]],[[4.57419398]]]"
    },
    {
      "test": "import numpy as np\n\nnp.random.seed(42) \n\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n\nrnn = SimpleRNN(input_size=2, hidden_size=3, output_size=1)\n\n# Train the RNN over multiple epochs\nfor epoch in range(100):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\nprint(output)",
      "expected_output": "[[[2.42201379]],[[3.44167595]],[[3.6129965 ]],[[4.50660152]]]"
    },
    {
      "test": "import numpy as np\n\nnp.random.seed(42) \n\ninput_sequence = np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]])\nexpected_output = np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]])\n\nrnn = SimpleRNN(input_size=2, hidden_size=10, output_size=2)\n\n# Train the RNN over multiple epochs\nfor epoch in range(50):\n    output = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, learning_rate=0.01)\n\nprint(output)",
      "expected_output": "[[[3.28424506],[5.93532247]],[[3.60393582],[6.82013468]],[[3.52586543],[6.58278163]],[[3.61336207],[6.84916339]]]"
    }
  ]
}