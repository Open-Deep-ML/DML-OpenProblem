{
  "id": "58",
  "title": "Gaussian Elimination for Solving Linear Systems",
  "difficulty": "medium",
  "category": "Linear Algebra",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/paddywardle",
      "name": "paddywardle"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "description": "## Task: Implement the Gaussian Elimination Method\n\nYour task is to implement the Gaussian Elimination method, which transforms a system of linear equations into an upper triangular matrix. This method can then be used to solve for the variables using backward substitution.\n\nWrite a function `gaussian_elimination(A, b)` that performs Gaussian Elimination with partial pivoting to solve the system \\(Ax = b\\).\n\nThe function should return the solution vector \\(x\\).\n\n    ",
  "learn_section": "\n## Understanding Gaussian Elimination\n\nGaussian Elimination is used to replace matrix coefficients with a row-echelon form matrix, which can be more easily solved via backwards substitution.\n\n### Row-Echelon Form Criteria\n\n- **Non-zero rows** are above any rows of all zeros.\n- The **leading entry** of each non-zero row is to the right of the leading entry of the previous row.\n- The **leading entry** in any non-zero row is 1, and all entries below it in the same column are zeros.\n\n### Augmented Matrix\n\nFor a linear system $Ax = b$, an augmented matrix is a way of displaying all the numerical information in a linear system in a single matrix. This combines the coefficient matrix $A$ and vector source $b$ as follows:\n\n$$\n\\begin{pmatrix} \na_{11} & a_{21} & a_{31} & b_1\\\\ \na_{12} & a_{22} & a_{32} & b_2\\\\ \na_{31} & a_{32} & a_{33} & b_3 \n\\end{pmatrix}\n$$\n\n### Partial Pivoting\n\nIn linear algebra, diagonal elements of a matrix are referred to as the \"pivot\". To solve a linear system, the diagonal is used as a divisor for other elements within the matrix. This means that Gaussian Elimination will fail if there is a zero pivot.\n\nIn this case, pivoting is used to interchange rows, ensuring a non-zero pivot. Specifically, **partial pivoting** looks at all other rows in the current column to find the row with the highest absolute value. This row is then interchanged with the current row. This not only increases the numerical stability of the solution, but also reduces round-off errors caused by dividing by small entries.\n\n### Gaussian Elimination Mathematical Formulation\n\n**Gaussian Elimination:**\n\n- For $k = 1$ to $ \\text{number of rows} - 1$:\n  - Apply partial pivoting to the current row.\n  - For $i = k + 1$ to $ \\text{number of rows}$:\n    - $ m_{ik} = \\frac{a_{ik}}{a_{kk}} $\n    - For $j = k$ to $ \\text{number of columns}$:\n      - $ a_{ij} = a_{ij} - m_{ik} \\times a_{kj} $\n    - $ b_i = b_i - m_{ik} \\times b_k $\n\n**Backwards Substitution:**\n\n- For $k = \\text{number of rows}$ to $1$:\n  - For $i = \\text{number of columns} - 1$ to $1$:\n    - $ b_k = b_k - a_{ki} \\times b_i $\n  - $ b_k = \\frac{b_k}{a_{kk}} $\n\n### Example Calculation\n\nLetâ€™s solve the system of equations:\n\n$$\nA = \\begin{pmatrix} \n2 & 8 & 4\\\\ \n5 & 5 & 1 \\\\ \n4 & 10 & -1 \n\\end{pmatrix} \n\\quad \\text{and} \\quad \nb = \\begin{pmatrix} \n2 \\\\ 5 \\\\ 1 \n\\end{pmatrix} \n$$\n\n1. Apply **partial pivoting** to increase the magnitude of the pivot. For $A_{11}$, calculate the factor for the elimination of $A_{12}$: \n\n$$ \nm_{12} = \\frac{A_{12}}{A_{11}} = \\frac{2}{5} = 0.4 \n$$\n\n2. Apply this scaling to row 1 and subtract this from row 2, eliminating $A_{12}$:\n\n$$\nA = \\begin{pmatrix} \n5 & 5 & 1 \\\\ \n0 & 6 & 3.6 \\\\ \n4 & 10 & -1 \n\\end{pmatrix} \n\\quad \\text{and} \\quad \nb = \\begin{pmatrix} \n5 \\\\ 0 \\\\ 1 \n\\end{pmatrix} \n$$\n\nAfter the full **Gaussian Elimination** process has been applied to $A$ and $b$, we get the following:\n\n$$\nA = \\begin{pmatrix} \n5 & 5 & 1\\\\ \n0 & 6 & 3.6 \\\\ \n0 & 0 & -5.4 \n\\end{pmatrix} \n\\quad \\text{and} \\quad \nb = \\begin{pmatrix} \n5 \\\\ 0 \\\\ 3 \n\\end{pmatrix} \n$$\n\nTo calculate $x$, we apply **backward substitution** by substituting in the currently solved values and dividing by the pivot. This gives the following for the first iteration:\n\n$$ \nx_3 = \\frac{b_3}{A_{33}} = \\frac{3}{-5.4} = -0.56 \n$$\n\nThis process can be repeated iteratively for all rows to solve the linear system, substituting in the solved values for the rows below.\n\n### Applications\n\nGaussian Elimination and linear solvers have a wide range of real-world applications, including their use in:\n\n- Machine learning\n- Computational fluid dynamics\n- 3D graphics",
  "starter_code": "import numpy as np\n\ndef gaussian_elimination(A, b):\n\t\"\"\"\n\tSolves the system Ax = b using Gaussian Elimination with partial pivoting.\n    \n\t:param A: Coefficient matrix\n\t:param b: Right-hand side vector\n\t:return: Solution vector x\n\t\"\"\"\n\treturn np.zeros_like(b)",
  "solution": "import numpy as np\n\ndef partial_pivoting(A_aug, row_num, col_num):\n    rows, cols = A_aug.shape\n    max_row = row_num\n    max_val = abs(A_aug[row_num, col_num])\n    for i in range(row_num, rows):\n        current_val = abs(A_aug[i, col_num])\n        if current_val > max_val:\n            max_val = current_val\n            max_row = i\n    if max_row != row_num:\n        A_aug[[row_num, max_row]] = A_aug[[max_row, row_num]]\n    return A_aug\n\ndef gaussian_elimination(A, b):\n    rows, cols = A.shape\n    A_aug = np.hstack((A, b.reshape(-1, 1)))\n\n    for i in range(rows-1):\n        A_aug = partial_pivoting(A_aug, i, i)\n        for j in range(i+1, rows):\n            A_aug[j, i:] -= (A_aug[j, i] / A_aug[i, i]) * A_aug[i, i:]\n\n    x = np.zeros_like(b, dtype=float)\n    for i in range(rows-1, -1, -1):\n        x[i] = (A_aug[i, -1] - np.dot(A_aug[i, i+1:cols], x[i+1:])) / A_aug[i, i]\n    return x",
  "example": {
    "input": "A = np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float)\nb = np.array([2,5,1], dtype=float)\n\nprint(gaussian_elimination(A, b))",
    "output": "[11.0, -4.0, 3.0]",
    "reasoning": "The Gaussian Elimination method transforms the system of equations into an upper triangular matrix and then uses backward substitution to solve for the variables."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\nA = np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float)\nb = np.array([2,5,1], dtype=float)\nprint(gaussian_elimination(A, b))",
      "expected_output": "[11.0, -4.0, 3.0]"
    },
    {
      "test": "import numpy as np\n\nA = np.array([\n    [0, 2, 1, 0, 0, 0, 0],\n    [2, 6, 2, 1, 0, 0, 0],\n    [1, 2, 7, 2, 1, 0, 0],\n    [0, 1, 2, 8, 2, 1, 0],\n    [0, 0, 1, 2, 9, 2, 1],\n    [0, 0, 0, 1, 2, 10, 2],\n    [0, 0, 0, 0, 1, 2, 11]\n], dtype=float)\nb = np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)\nprint(gaussian_elimination(A, b))",
      "expected_output": "[-0.4894027,   0.36169985,  0.2766003,   0.25540569,  0.31898951,  0.40387497, 0.53393278]"
    },
    {
      "test": "import numpy as np\n\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)\nb = np.array([8, -11, -3], dtype=float)\nprint(gaussian_elimination(A, b))",
      "expected_output": "[2.0, 3.0, -1.0]"
    }
  ]
}