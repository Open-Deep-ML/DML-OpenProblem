{
  "id": "96",
  "title": "Implement the Hard Sigmoid Activation Function",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Haleshot",
      "name": "Haleshot"
    }
  ],
  "tinygrad_difficulty": null,
  "pytorch_difficulty": null,
  "marimo_link": "https://open-deep-ml.github.io/deepml-notebooks/96/",
  "description": "Implement the Hard Sigmoid activation function, a computationally efficient approximation of the standard sigmoid function. Your function should take a single input value and return the corresponding output based on the Hard Sigmoid definition.",
  "learn_section": "## Understanding the Hard Sigmoid Activation Function\n\nThe Hard Sigmoid is a piecewise linear approximation of the sigmoid activation function. It's computationally more efficient than the standard sigmoid function while maintaining similar characteristics. This function is particularly useful in neural networks where computational efficiency is crucial.\n\n### Mathematical Definition\n\nThe Hard Sigmoid function is mathematically defined as:\n\n$$\nHardSigmoid(x) = \\begin{cases} \n0 & \\text{if } x \\leq -2.5 \\\\ \n0.2x + 0.5 & \\text{if } -2.5 < x < 2.5 \\\\ \n1 & \\text{if } x \\geq 2.5 \n\\end{cases}\n$$\n\nWhere $x$ is the input to the function.\n\n### Characteristics\n\n- **Output Range:** The output is always bounded in the range $[0, 1]$\n- **Shape:** The function consists of three parts:\n  - A constant value of 0 for inputs <= -2.5\n  - A linear segment with slope 0.2 for inputs between -2.5 and 2.5\n  - A constant value of 1 for inputs >= 2.5\n- **Gradient:** The gradient is 0.2 in the linear region and 0 in the saturated regions\n\n### Advantages in Neural Networks\n\nThis function is particularly useful in neural networks as it provides:\n- Computational efficiency compared to standard sigmoid\n- Bounded output range similar to sigmoid\n- Simple gradient computation",
  "starter_code": "def hard_sigmoid(x: float) -> float:\n\t\"\"\"\n\tImplements the Hard Sigmoid activation function.\n\n\tArgs:\n\t\tx (float): Input value\n\n\tReturns:\n\t\tfloat: The Hard Sigmoid of the input\n\t\"\"\"\n\t# Your code here\n\tpass",
  "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5",
  "example": {
    "input": "hard_sigmoid(0.0)",
    "output": "0.5",
    "reasoning": "The input 0.0 falls in the linear region of the Hard Sigmoid function. Using the formula $HardSigmoid(x) = 0.2x + 0.5$, the output is $0.2 \\times 0.0 + 0.5 = 0.5$."
  },
  "test_cases": [
    {
      "test": "print(hard_sigmoid(.56))",
      "expected_output": "0.612"
    },
    {
      "test": "print(hard_sigmoid(3.0))",
      "expected_output": "1.0"
    },
    {
      "test": "print(hard_sigmoid(0.0))",
      "expected_output": "0.5"
    },
    {
      "test": "print(hard_sigmoid(1.0))",
      "expected_output": "0.7"
    },
    {
      "test": "print(hard_sigmoid(-1.0))",
      "expected_output": "0.3"
    },
    {
      "test": "print(hard_sigmoid(2.5))",
      "expected_output": "1.0"
    },
    {
      "test": "print(hard_sigmoid(-2.5))",
      "expected_output": "0.0"
    }
  ]
}