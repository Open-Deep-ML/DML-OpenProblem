{
  "id": "56",
  "title": "KL Divergence Between Two Normal Distributions",
  "difficulty": "easy",
  "category": "Deep Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/Hui-cd",
      "name": "Hui"
    }
  ],
  "description": "## Task: Implement KL Divergence Between Two Normal Distributions\n\nYour task is to compute the Kullback Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\n\nWrite a function kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q) that calculates the KL divergence between two normal distributions.\n\nThe function should return the KL divergence as a floating point number.\n\n    ",
  "learn_section": "## Understanding Kullback-Leibler Divergence (KL Divergence)\n\nThe **Kullback-Leibler (KL) divergence**, also known as relative entropy, measures the difference between two probability distributions. It quantifies how much information is lost when approximating one distribution with another.\n\n---\n\n### Definition of KL Divergence\n\nFor continuous variables, the KL divergence is defined as:\n\n$$\nKL(P \\parallel Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\, dx\n$$\n\nwhere:\n- $p(x)$ is the probability density function of the **reference** distribution $P$.\n- $q(x)$ is the probability density function of the **comparison** distribution $Q$.\n\n---\n\n### KL Divergence Between Two Normal Distributions\n\nConsider two normal distributions $P$ and $Q$:\n\n- $P \\sim N(\\mu_P, \\sigma_P^2)$  \n- $Q \\sim N(\\mu_Q, \\sigma_Q^2)$\n\nFor these, the KL divergence simplifies to:\n\n$$\nKL(P \\parallel Q) = \\int p(x) \\left[\n\\log \\frac{\\sigma_Q}{\\sigma_P}\n+ \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2}\n- \\frac{1}{2}\n\\right] dx\n$$\n\nSince $p(x)$ is the PDF of $x$ under $P$, the integral over $p(x)$ just multiplies by 1 for each constant term. Thus, the final closed form is:\n\n$$\nKL(P \\parallel Q) =\n\\log \\frac{\\sigma_Q}{\\sigma_P}\n+ \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2\\sigma_Q^2}\n- \\frac{1}{2}\n$$\n\n---\n\n### Interpretation\n\nThis expression quantifies how one normal distribution $P$ **diverges** from another normal distribution $Q$. A KL divergence of zero indicates the two distributions are identical. As the divergence grows, it signals that $Q$ is a poorer approximation of $P$.\n\nThe KL divergence is **asymmetric**:\n\n$$\nKL(P \\parallel Q) \\neq KL(Q \\parallel P)\n$$\n\nmaking it sensitive to the **direction** of comparison.",
  "starter_code": "import numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n\treturn 0.0",
  "solution": "import numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl_div = term1 + term2 - 0.5\n    return kl_div",
  "example": {
    "input": "mu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\n\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
    "output": "0.5",
    "reasoning": "The KL divergence between the normal distributions \\( P \\) and \\( Q \\) with parameters \\( \\mu_P = 0.0 \\), \\( \\sigma_P = 1.0 \\) and \\( \\mu_Q = 1.0 \\), \\( \\sigma_Q = 1.0 \\) is 0.5."
  },
  "test_cases": [
    {
      "test": "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 1.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
      "expected_output": "0.0"
    },
    {
      "test": "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
      "expected_output": "0.5"
    },
    {
      "test": "import numpy as np\n\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 2.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
      "expected_output": "0.3181471805599453"
    },
    {
      "test": "import numpy as np\n\nmu_p = 1.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 2.0\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))",
      "expected_output": "0.4431471805599453"
    }
  ]
}