{
  "id": "139",
  "title": "Elastic Net Regression via Gradient Descent",
  "difficulty": "medium",
  "category": "Machine Learning",
  "video": "",
  "likes": "0",
  "dislikes": "0",
  "contributor": [
    {
      "profile_link": "https://github.com/komaksym",
      "name": "komaksym"
    }
  ],
  "description": "Implement Elastic Net Regression using gradient descent, combining L1 and L2 penalties to handle multicollinearity and encourage sparsity in the feature weights.",
  "learn_section": "# Elastic Net Regression Using Gradient Descent\n\nElastic Net Regression combines both L1 (Lasso) and L2 (Ridge) regularization techniques to overcome the limitations of using either regularization method alone. It's particularly useful when dealing with datasets that have many correlated features.\n\n## What is Elastic Net?\n\nElastic Net addresses two main issues:\n- **Lasso's limitation**: When features are highly correlated, Lasso tends to select only one feature from a group of correlated features arbitrarily\n- **Ridge's limitation**: Ridge regression doesn't perform feature selection (coefficients approach zero but never become exactly zero)\n\nThe goal of Elastic Net is to minimize the objective function:\n\n$$J(w, b) = \\underbrace{\\frac{1}{2n} \\sum_{i=1}^n\\left( y_i - \\left(\\sum_{j=1}^pX_{ij}w_j+b\\right)\\right)^2}_{\\text{MSE Loss}} + \\underbrace{\\alpha_1 \\sum_{j=1}^p |w_j|}_{\\text{L1 Regularization}} + \\underbrace{\\alpha_2 \\sum_{j=1}^p w_j^2}_{\\text{L2 Regularization}}$$\n\nWhere:\n* The first term is the **Mean Squared Error (MSE) Loss**: $\\frac{1}{2n} \\sum_{i=1}^n\\left( y_i - \\left(\\sum_{j=1}^pX_{ij}w_j+b\\right)\\right)^2$\n* The second term is the **L1 Regularization** (Lasso penalty): $\\alpha_1 \\sum_{j=1}^p |w_j|$\n* The third term is the **L2 Regularization** (Ridge penalty): $\\alpha_2 \\sum_{j=1}^p w_j^2$\n* $\\alpha_1$ controls the strength of L1 regularization\n* $\\alpha_2$ controls the strength of L2 regularization\n\n## Step-by-Step Implementation Guide\n\n### 1. Initialize weights $w_j$ and bias $b$ to 0\n\n### 2. Make Predictions\nAt each iteration, calculate predictions using:\n$$\\hat{y}_i = \\sum_{j=1}^pX_{ij}w_j + b$$\n\nWhere:\n- $\\hat{y}_i$ is the predicted value for the $i$-th sample\n- $X_{ij}$ is the value of the $i$-th sample's $j$-th feature\n- $w_j$ is the weight associated with the $j$-th feature\n\n### 3. Calculate Residuals\nFind the difference between actual and predicted values: $error_i = \\hat{y}_i - y_i$\n\n### 4. Update Weights and Bias Using Gradients\n\n**Gradient with respect to weights:**\n$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^nX_{ij}(\\hat{y}_i - y_i) + \\alpha_1 \\cdot \\text{sign}(w_j) + 2\\alpha_2 \\cdot w_j$$\n\n**Gradient with respect to bias:**\n$$\\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n(\\hat{y}_i - y_i)$$\n\n**Update rules:**\n$$w_j = w_j - \\eta \\cdot \\frac{\\partial J}{\\partial w_j}$$\n$$b = b - \\eta \\cdot \\frac{\\partial J}{\\partial b}$$\n\nWhere $\\eta$ is the learning rate.\n\n### 5. Check for Convergence\nRepeat steps 2-4 until convergence. Convergence is determined by evaluating the L1 norm of the weight gradients:\n\n$$||\\nabla w||_1 = \\sum_{j=1}^p \\left|\\frac{\\partial J}{\\partial w_j}\\right|$$\n\nIf $||\\nabla w||_1 < \\text{tolerance}$, stop the algorithm.\n\n### 6. Return the Final Weights and Bias\n\n## Key Parameters\n\n- **alpha1**: L1 regularization strength (promotes sparsity)\n- **alpha2**: L2 regularization strength (handles correlated features)\n- **learning_rate**: Step size for gradient descent\n- **max_iter**: Maximum number of iterations\n- **tol**: Convergence tolerance\nPath\n## Key Differences from Lasso and Ridge\n\n1. **Lasso (L1 only)**: Tends to select one feature from correlated groups, can be unstable with small sample sizes\n2. **Ridge (L2 only)**: Keeps all features but shrinks coefficients, doesn't perform feature selection\n3. **Elastic Net (L1 + L2)**: Combines benefits of both - performs feature selection while handling correlated features better than Lasso alone\n\nThe balance between L1 and L2 regularization is controlled by the `alpha1` and `alpha2` parameters, allowing you to tune the model for your specific dataset characteristics.",
  "starter_code": "import numpy as np\n\ndef elastic_net_gradient_descent(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha1: float = 0.1,\n    alpha2: float = 0.1,\n    learning_rate: float = 0.01,\n    max_iter: int = 1000,\n    tol: float = 1e-4,\n) -> tuple:\n    # Implement Elastic Net regression here\n    pass",
  "solution": "import numpy as np\n\ndef elastic_net_gradient_descent(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha1: float = 0.1,\n    alpha2: float = 0.1,\n    learning_rate: float = 0.01,\n    max_iter: int = 1000,\n    tol: float = 1e-4,\n) -> tuple:\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        error = y_pred - y\n        grad_w = (1 / n_samples) * np.dot(X.T, error) + alpha1 * np.sign(weights) + 2 * alpha2 * weights\n        grad_b = (1 / n_samples) * np.sum(error)\n        weights -= learning_rate * grad_w\n        bias -= learning_rate * grad_b\n        if np.linalg.norm(grad_w, ord=1) < tol:\n            break\n\n    return weights, bias",
  "example": {
    "input": "X = np.array([[0, 0], [1, 1], [2, 2]]); y = np.array([0, 1, 2])",
    "output": "(array([0.37, 0.37]), 0.25)",
    "reasoning": "The model learns a nearly perfect linear relationship with regularization controlling weight magnitude. The weights converge around 0.37 with a bias around 0.25."
  },
  "test_cases": [
    {
      "test": "X = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\nw, b = elastic_net_gradient_descent(X, y, alpha1=0.1, alpha2=0.1, learning_rate=0.01, max_iter=1000)\nprint(np.round(w,2), round(b,2))",
      "expected_output": "[0.37, 0.37],0.25"
    },
    {
      "test": "X = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 2, 3, 4, 5])\nw, b = elastic_net_gradient_descent(X, y, alpha1=0.1, alpha2=0.1, learning_rate=0.01, max_iter=2000)\nprint(np.round(w,2),round(b,2))",
      "expected_output": "[0.43, 0.48], 0.69"
    }
  ]
}