# Learning Goals

1. Understand how **softmax** converts logits to probabilities.
2. Learn why **log-softmax** is preferred for numerical stability.
3. Implement **cross-entropy loss** manually.
4. Apply batch computations in PyTorch.
5. Understand the connection between logits, probabilities, and loss functions.
