<h2>Performance Metrics</h2>

<p>Performance metrics such as accuracy, F1 score, specificity, negative value, precision and recall are vital to understand how a model is performing</p>

<p>How many observations are correctly labeled? Are we mislabelling one category more than the other? Performance metrics can answer those questions and provide an idea of where to focus in order to improve a model's performance</p>

<p>For this problem, starting with the confusion matrix is a helpful first step, as all of the elements of the confusion matrix can help with calculating the other performance metrics</p>

<p>For a binary classification problem of a dataset with \(n\) observations, the confusion matrix is a \(2 \times 2\) matrix with the following structure:</p>

\[
M = \begin{pmatrix} 
TP & FN \\
FP & TN
\end{pmatrix}
\]

Where:

<ul>
    <li>TP: True positives, the number of observations from the positive label that were correctly labeled as positive</li>
    <li>FN: False negatives, the number of observations from the positive label that were incorrectly labeled as negative    </li>
    <li>FP: False positives, the number of observations from the negative label that were incorrectly labeled as positive    </li>
    <li>TN: True negatives, the number of observations from the negative label that were correctly labeled as negative    </li>
</ul>

<p>From here, we can define each performance metric in order to answer a specific question.</p>

<p><b>How many observations are we labelling as the actual category they belong to?</b> For this question, the <b>accuracy</b> is the metric you would like to focus on. The accuracy is defined by:</p>

\[
\frac{TP+TN}{TP+TN+FP+FN}
\]

<p>Accuracy is a great starting point, but it has a crucial flaw in unbalanced datasets. Suppose for example that 99% of your data is in one category. If your model always predicts the majority category, then it will get a 99%, but it is arguably not a very good model, as it is unable to identify elements from the minority class. Other metrics can help with this issue.</p>

<p><b>How many elements that were labeled as positive are actually positive?</b> The metric to answer this question is the <b>precision</b>. This metric is helpful in narrowing down to one specific class and properly understanding it. It is defined by:</p>

\[
\frac{TP}{TP+FP}
\]

<p>This metric provides more insight into the labelling of the positive class, but if very few observations are actually labeled as positive, then it might be misleading.</p>

<p>There is an analogous for the negative class which is the <b>negative predictive value</b>. Defined by:</p>

\[
\frac{TN}{TN+FN}
\]

<p><b>Out of all of the positive elements, how many were correctly labeled?</b> The metric to answer this question is the <b>recall</b>. This metric is helpful in narrowing down to one specific class as well. It is defined by:</p>

\[
\frac{TP}{TP+FN}
\]

<p>Note that this metric introduces some tension, if you wanted to encourage your model to label more observations as positive, you risk mislabelling more negative observations as positive</p>

<p>Similarly, the <b>specificity</b> provides the same insight for the negative class. It is defined by:</p>

\[
\frac{TN}{TN+FP}
\]

<p><b>How to account for the trade-off of false negatives and positives?</b> The metric for this is the <b>F1 score</b>. This is the harmonic mean of the precision and recall, which provides insight into the balance of false positives and false negatives. It is defined by:</p>

\[
2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]