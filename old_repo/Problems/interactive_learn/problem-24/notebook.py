# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "numpy==2.2.2",
#     "pandas==2.2.3",
#     "plotly==6.0.0",
# ]
# ///

import marimo

__generated_with = "0.10.17"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        # Understanding Single Neuron with Sigmoid Activation

        The single neuron is the fundamental building block of neural networks. It's a simple yet powerful computational unit that can perform binary classification by processing multidimensional input features through a sigmoid activation function. Let's explore this interactively!
        """
    ).center()
    return


@app.cell(hide_code=True)
def _(mo):
    # math definition accordion
    definition = mo.md(r"""
    A single neuron processes input features through these key steps:

    1. **Weighted Sum Calculation:**

    \[
    z = \sum_{i=1}^{n} (w_i \times feature_i) + bias
    \]

    2. **Sigmoid Activation Function:**

    \[
    \sigma(z) = \frac{1}{1 + e^{-z}}
    \]

    3. **Mean Squared Error:**

    \[
    MSE = \frac{1}{n}\sum_{i=1}^{n} (predicted_i - true_i)^2
    \]

    where $w_i$ are weights, $feature_i$ are input features, and $n$ is the number of features.
    """)

    mo.accordion({"### Mathematical Formulation": definition})
    return (definition,)


@app.cell
def _(insights):
    insights
    return


@app.cell(hide_code=True)
def _(mo):
    # insights accordion
    insights = mo.accordion(
        {
            "üß† Understanding the Components": mo.md("""
        **Key Concepts:**

        1. **Input Features**: Multiple numerical values representing different aspects of the data
        2. **Weights**: Learned parameters that determine feature importance
        3. **Bias**: Offset term that helps the model fit the data better
        4. **Sigmoid Function**: Squashes input to probability between 0 and 1
        """),
            "üìä Model Behavior": mo.md("""
        The neuron's behavior is influenced by:

        1. **Feature Values**: Input data characteristics
        2. **Weight Magnitudes**: Control feature influence
        3. **Bias Term**: Shifts the decision boundary
        4. **Sigmoid Activation**: Creates non-linear output
        """),
        }
    )
    return (insights,)


@app.cell
def _(feature_inputs):
    feature_inputs
    return


@app.cell(hide_code=True)
def _(mo):
    # input controls: features
    features_1 = mo.ui.array(
        [
            mo.ui.number(value=0.5, label="Feature 1", step=0.1),
            mo.ui.number(value=1.0, label="Feature 2", step=0.1),
        ]
    )

    features_2 = mo.ui.array(
        [
            mo.ui.number(value=-1.5, label="Feature 1", step=0.1),
            mo.ui.number(value=-2.0, label="Feature 2", step=0.1),
        ]
    )

    features_3 = mo.ui.array(
        [
            mo.ui.number(value=2.0, label="Feature 1", step=0.1),
            mo.ui.number(value=1.5, label="Feature 2", step=0.1),
        ]
    )

    feature_inputs = mo.vstack(
        [
            mo.md("### Input Features"),
            mo.hstack(
                [
                    mo.vstack([mo.md("**Example 1**"), features_1]),
                    mo.vstack([mo.md("**Example 2**"), features_2]),
                    mo.vstack([mo.md("**Example 3**"), features_3]),
                ]
            ),
        ]
    )
    return feature_inputs, features_1, features_2, features_3


@app.cell
def _(weight_controls):
    weight_controls
    return


@app.cell(hide_code=True)
def _(mo):
    # Weight and bias (wandb) controls
    w1 = mo.ui.number(value=0.7, label="Weight 1", step=0.1)

    w2 = mo.ui.number(value=-0.4, label="Weight 2", step=0.1)

    bias = mo.ui.number(value=-0.1, label="Bias", step=0.1)

    weight_controls = mo.vstack(
        [
            mo.md("### Model Parameters"),
            mo.hstack([w1, w2, bias]),
            mo.callout(
                mo.md("""
            Adjust weights and bias to see how they affect the neuron's output:

            - Positive weights increase output for positive features

            - Negative weights increase output for negative features

            - Bias shifts the decision boundary
            """),
                kind="info",
            ),
        ]
    )
    return bias, w1, w2, weight_controls


@app.cell
def _(calculate_button):
    calculate_button
    return


@app.cell
def _(mo):
    calculate_button = mo.ui.run_button(label="Calculate Neuron Output")
    return (calculate_button,)


@app.cell
def _(results_display):
    results_display
    return


@app.cell(hide_code=True)
def _(
    bias,
    calculate_button,
    features_1,
    features_2,
    features_3,
    mo,
    np,
    w1,
    w2,
):
    def sigmoid(z):
        return 1 / (1 + np.exp(-z))

    def compute_neuron_output(features, weights, bias):
        z = np.dot(features, weights) + bias
        return sigmoid(z)

    results = []
    mse = 0

    if calculate_button.value:
        # Convert inputs to numpy arrays
        weights = np.array([w1.value, w2.value])
        true_labels = np.array([0, 1, 0])

        # Process each feature vector
        feature_vectors = [
            [f.value for f in features_1.elements],
            [f.value for f in features_2.elements],
            [f.value for f in features_3.elements],
        ]

        predictions = []
        for features in feature_vectors:
            pred = compute_neuron_output(features, weights, bias.value)
            predictions.append(round(float(pred), 4))

        # Calculate MSE
        mse = round(np.mean((np.array(predictions) - true_labels) ** 2), 4)
        results = predictions

    results_display = mo.vstack(
        [
            mo.md("### Results"),
            mo.md(f"**Predictions:** {results}"),
            mo.md(f"**Mean Squared Error:** {mse}"),
            mo.accordion(
                {
                    "üîç Understanding the Results": mo.md("""
            - Predictions close to 0 indicate class 0
            - Predictions close to 1 indicate class 1
            - Lower MSE means better model performance
            """)
                }
            ),
        ]
    )
    return (
        compute_neuron_output,
        feature_vectors,
        features,
        mse,
        pred,
        predictions,
        results,
        results_display,
        sigmoid,
        true_labels,
        weights,
    )


@app.cell
def _(visualize_button):
    visualize_button
    return


@app.cell(hide_code=True)
def _(mo):
    visualize_button = mo.ui.run_button(label="Visualize Decision Boundary")
    return (visualize_button,)


@app.cell(hide_code=True)
def _(bias, features_1, features_2, features_3, np, pd, px, w1, w2):
    # plotting function
    def plot_decision_boundary():
        # meshgrid for visualization
        x = np.linspace(-3, 3, 100)
        y = np.linspace(-3, 3, 100)
        X, Y = np.meshgrid(x, y)

        # Z values calculated using the neuron model
        Z = 1 / (1 + np.exp(-(w1.value * X + w2.value * Y + bias.value)))

        df = pd.DataFrame({"x": X.flatten(), "y": Y.flatten(), "z": Z.flatten()})

        # Plot decision boundary using density contour
        fig = px.density_contour(
            df,
            x="x",
            y="y",
            z="z",
            title="Decision Boundary Visualization",
            labels={"x": "Feature 1", "y": "Feature 2", "z": "Probability"},
        )

        # Add scatter points for input examples
        feature_points = pd.DataFrame(
            {
                "Feature 1": [
                    features_1.elements[0].value,
                    features_2.elements[0].value,
                    features_3.elements[0].value,
                ],
                "Feature 2": [
                    features_1.elements[1].value,
                    features_2.elements[1].value,
                    features_3.elements[1].value,
                ],
                "Label": ["Example 1", "Example 2", "Example 3"],
            }
        )

        scatter = px.scatter(feature_points, x="Feature 1", y="Feature 2", text="Label")

        for trace in scatter.data:
            fig.add_trace(trace)

        fig.update_layout(width=800, height=600)
        return fig

    return (plot_decision_boundary,)


@app.cell(hide_code=True)
def _(plot_decision_boundary, visualize_button):
    decision_boundary_plot = None
    if visualize_button.value:
        decision_boundary_plot = plot_decision_boundary()
    decision_boundary_plot
    return (decision_boundary_plot,)


@app.cell
def _(conclusion):
    conclusion
    return


@app.cell(hide_code=True)
def _(mo):
    # Conclusion and next steps
    conclusion = mo.vstack(
        [
            mo.callout(
                mo.md("""
                **Congratulations!** 
                You've explored the single neuron model with sigmoid activation. You've learned:

                - How weights and bias affect the decision boundary
                - How sigmoid activation converts linear combinations to probabilities
                - How to evaluate performance using MSE
            """),
                kind="success",
            ),
            mo.accordion(
                {
                    "üéØ Applications": mo.md("""
                - Binary classification tasks
                - Feature importance analysis
                - Basic pattern recognition
                - Building blocks for larger neural networks
            """),
                    "üöÄ Next Steps": mo.md("""
                1. Implement the neuron in a real project
                2. Explore other activation functions
                3. Add more features to handle complex patterns
                4. Learn about gradient descent for training
            """),
                }
            ),
        ]
    )
    return (conclusion,)


@app.cell
def _():
    import marimo as mo

    return (mo,)


@app.cell
def _():
    import numpy as np
    import pandas as pd
    import plotly.express as px

    return np, pd, px


if __name__ == "__main__":
    app.run()
