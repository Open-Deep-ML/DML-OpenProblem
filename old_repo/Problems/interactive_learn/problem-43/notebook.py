# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "marimo",
#     "numpy==2.2.1",
#     "matplotlib==3.10.0",
#     "plotly==5.24.1",
#     "pandas==2.2.3",
# ]
# ///

import marimo

__generated_with = "0.11.31"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        # Understanding Ridge Regression Loss Function

        [Ridge Regression](https://en.wikipedia.org/wiki/Ridge_regression) is a powerful regularized version of linear regression that helps prevent overfitting. Let's explore its loss function interactively!
        """
    ).center()
    return


@app.cell(hide_code=True)
def _(mo):
    # Mathematical definition accordion
    definition = mo.md(r"""
    The Ridge Regression loss function combines Mean Squared Error (MSE) with L2 regularization:

    \[
    L(\beta) = \underbrace{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}_{\text{MSE}} + \underbrace{\lambda \sum_{j=1}^p \beta_j^2}_{\text{L2 Regularization}}
    \]

    where:

    - $n$ is the number of samples

    - $y_i$ is the true value

    - $\hat{y}_i$ is the predicted value

    - $\lambda$ (alpha) is the regularization parameter

    - $\beta_j$ are the model coefficients
    """)

    mo.accordion({"### Mathematical Formulation": definition})
    return (definition,)


@app.cell(hide_code=True)
def _(mo):
    insights = mo.accordion(
        {
            "üîç Key Components": mo.md("""
        **1. Mean Squared Error (MSE)**
        - Measures prediction accuracy
        - Penalizes larger errors more heavily
        - Always non-negative

        **2. L2 Regularization Term**
        - Controls model complexity
        - Prevents coefficient values from becoming too large
        - Helps prevent overfitting
        """),
            "‚öôÔ∏è Role of Alpha (Œª)": mo.md("""
        The regularization parameter Œ± controls:

        1. Œ± = 0: Equivalent to standard linear regression
        2. Small Œ±: Slight regularization effect
        3. Large Œ±: Strong regularization, coefficients approach zero
        """),
            "üßÆ Coefficient Shrinkage": mo.md("""
        Ridge regression shrinks coefficients by adding a penalty proportional to their squared magnitude:

        - Larger coefficients incur higher penalties
        - The penalty applies to all coefficients equally
        - Unlike Lasso, Ridge typically keeps all features but with reduced magnitudes
        - Mathematically, Ridge finds the minimum of: ||y - XŒ≤||¬≤ + Œª||Œ≤||¬≤
        """),
        }
    )
    return (insights,)


@app.cell(hide_code=True)
def _(mo):
    # controls for sample data
    sample_size = mo.ui.slider(start=4, stop=20, value=10, step=1, label="Sample Size")

    alpha = mo.ui.number(
        value=0.1, start=0, stop=10, step=0.1, label="Regularization Parameter (Œ±)"
    )

    controls = mo.hstack(
        [mo.vstack([mo.md("### Data Parameters"), sample_size, alpha])]
    )
    return alpha, controls, sample_size


@app.cell(hide_code=True)
def _(controls):
    controls
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""### Model Coefficients""")
    return


@app.cell
def _(mo):
    coefficient_inputs = mo.ui.array(
        [
            mo.ui.number(value=0.2, label="Coefficient 1", step=0.1),
            mo.ui.number(value=2.0, label="Coefficient 2", step=0.1),
        ],
        label="Model Coefficients",
    )

    coefficient_section = mo.hstack(
        [
            coefficient_inputs,
            mo.callout(
                mo.md("Adjust coefficients to see how they affect the loss value."),
                kind="warn",
            ),
        ]
    )
    return coefficient_inputs, coefficient_section


@app.cell(hide_code=True)
def _(coefficient_section):
    coefficient_section
    return


@app.cell
def _(np):
    def ridge_loss(X, w, y_true, alpha):
        """Calculate Ridge Regression loss.

        Args:
            X (np.ndarray): Feature matrix (n_samples, n_features)
            w (np.ndarray): Coefficient vector
            y_true (np.ndarray): True target values
            alpha (float): Regularization parameter

        Returns:
            float: Ridge loss value
        """
        X.shape[0]

        # Check dimension compatibility
        if X.shape[1] != len(w):
            raise ValueError(
                f"Coefficient count ({len(w)}) must match feature count ({X.shape[1]})"
            )

        y_pred = X @ w
        mse = np.mean((y_true - y_pred) ** 2)
        regularization = alpha * np.sum(w**2)
        return mse + regularization

    return (ridge_loss,)


@app.cell(hide_code=True)
def _(mo):
    visualize_button = mo.ui.run_button(label="Visualize Predictions")
    return (visualize_button,)


@app.cell
def _(alpha, coefficient_inputs, mo, np, ridge_loss, sample_size):
    # Generate sample data
    X = np.column_stack(
        [np.arange(1, sample_size.value + 1), np.ones(sample_size.value)]
    )
    y_true = np.arange(2, sample_size.value + 2)
    w = np.array(coefficient_inputs.value)

    try:
        # Calculate loss
        current_loss = ridge_loss(X, w, y_true, alpha.value)

        # Calculate components for display
        mse_component = np.mean((y_true - X @ w) ** 2)
        reg_component = alpha.value * np.sum(w**2)

        # Display results
        result_display = mo.vstack(
            [
                mo.md("### Current Loss Value"),
                mo.callout(
                    mo.md(
                        f"Ridge Loss: **{current_loss:.4f}**\n\n"
                        f"- MSE Component: {mse_component:.4f}\n"
                        f"- Regularization Component: {reg_component:.4f}"
                    ),
                    kind="info",
                ),
            ]
        )
    except Exception as e:
        result_display = mo.vstack(
            [mo.md("### Error"), mo.callout(mo.md(f"Error: {str(e)}"), kind="danger")]
        )
        current_loss = None
    return (
        X,
        current_loss,
        mse_component,
        reg_component,
        result_display,
        w,
        y_true,
    )


@app.cell(hide_code=True)
def _(result_display):
    result_display
    return


@app.cell
def _(pd, px):
    def plot_predictions(X, w, y_true):
        y_pred = X @ w

        # Use pandas DataFrame for better compatibility with plotly
        df = pd.DataFrame({"x": X[:, 0], "True Values": y_true, "Predictions": y_pred})

        # Prepare data for plotting
        plot_df = pd.melt(
            df,
            id_vars=["x"],
            value_vars=["True Values", "Predictions"],
            var_name="Type",
            value_name="Value",
        )

        fig = px.scatter(
            plot_df,
            x="x",
            y="Value",
            color="Type",
            title="True Values vs Predictions",
            labels={"Value": "Value", "x": "Sample Index"},
        )

        # Add lines connecting points
        for series_name in ["True Values", "Predictions"]:
            series_data = df[["x", series_name]].sort_values("x")
            fig.add_scatter(
                x=series_data["x"],
                y=series_data[series_name],
                mode="lines",
                name=f"{series_name} (line)",
                line=dict(dash="dash"),
            )

        return fig

    return (plot_predictions,)


@app.cell(hide_code=True)
def _(visualize_button):
    visualize_button
    return


@app.cell
def _(X, mo, plot_predictions, visualize_button, w, y_true):
    plot_results = None
    if visualize_button.value:
        try:
            plot_results = plot_predictions(X, w, y_true)
        except Exception as e:
            plot_results = mo.md(f"Error generating plot: {str(e)}").callout(
                kind="danger"
            )
    plot_results
    return (plot_results,)


@app.cell(hide_code=True)
def _(mo):
    conclusion = mo.vstack(
        [
            mo.callout(
                mo.md("""
                **Congratulations!** 
                You've explored the Ridge Regression loss function interactively. Key takeaways:

                - Understanding the balance between MSE and regularization
                - Impact of the regularization parameter (Œ±)
                - How coefficients affect predictions and loss
                - How Ridge regression shrinks coefficients toward zero
            """),
                kind="success",
            ),
            mo.accordion(
                {
                    "üéØ Applications": mo.md("""
                - High-dimensional data analysis
                - Feature selection
                - Preventing overfitting in linear models
                - Multicollinearity handling
            """),
                    "üöÄ Next Steps": mo.md("""
                1. Implement gradient descent optimization
                2. Compare with Lasso regression
                3. Explore cross-validation for Œ± selection
                4. Apply to real-world datasets
            """),
                }
            ),
        ]
    )
    return (conclusion,)


@app.cell(hide_code=True)
def _(conclusion):
    conclusion
    return


@app.cell
def _():
    import marimo as mo

    return (mo,)


@app.cell(hide_code=True)
def _():
    import numpy as np
    import plotly.express as px
    import pandas as pd

    return np, pd, px


if __name__ == "__main__":
    app.run()
